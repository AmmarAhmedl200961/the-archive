{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MedQuAD (Medical Question Answering Dataset)\n",
    "======\n",
    "\n",
    "The MedQuad dataset provides a comprehensive source of medical questions and answers for natural\n",
    "language processing. With over 43,000 patient inquiries from real-life situations categorized into 31\n",
    "distinct types of questions, the dataset offers an invaluable opportunity to research correlations between\n",
    "treatments, chronic diseases, medical protocols and more. Answers provided in this database come not\n",
    "only from doctors but also other healthcare professionals such as nurses and pharmacists, providing a\n",
    "more complete array of responses to help researchers unlock deeper insights within the realm of\n",
    "healthcare. This incredible trove of knowledge is just waiting to be mined - so grab your data mining\n",
    "equipment and get exploring!\n",
    "## How to use the dataset\n",
    "In order to make the most out of this dataset, start by having a look at the column names and\n",
    "understanding what information they offer: qtype (the type of medical question), Question (the question\n",
    "in itself), and Answer (the expert response). The qtype column will help you categorize the dataset\n",
    "according to your desired question topics. Once you have filtered down your criteria as much as possible\n",
    "using qtype, it is time to analyze the data. Start by asking yourself questions such as “What treatments do\n",
    "most patients search for?” or “Are there any correlations between chronic conditions and protocols?”\n",
    "Then use simple queries such as SELECT Answer FROM MedQuad WHERE qtype='Treatment' AND\n",
    "Question LIKE '%pain%' to get closer to answering those questions.\n",
    "\n",
    "Once you have obtained new insights about healthcare based on the answers provided in this dynmaic\n",
    "data set - now it’s time for action! Use all that newfound understanding about patient needs in order\n",
    "develop educational materials and implement any suggested changes necessary. If more criteria are\n",
    "needed for querying this data set see if MedQuad offers additional columns; sometimes extra columns\n",
    "may be added periodically that could further enhance analysis capabilities.\n",
    "Link: https://www.kaggle.com/datasets/thedevastator/comprehensive-medical-q-a-dataset/data\n",
    "\n",
    "---\n",
    "### TASK) Questioning Answering using Transformer based model\n",
    "Implement following transformer based variants for the Question Answering task.\n",
    "1. BERT\n",
    "2. MobileBERT\n",
    "3. RoBERTa\n",
    "   \n",
    "Link: https://simpletransformers.ai/docs/qa-specifics/\n",
    "\n",
    "From the link given above you can get information about the model you need to fine-tune.\n",
    "Moreover you can find guideline on how input is tailored to pass to Transformer based models.\n",
    "\n",
    "Use 75% for training and 25% for testing.\n",
    "\n",
    "For each of these models, try different hyper parameters and report the best results with\n",
    "parameter values. Like changing number of Encoder Layers etc.\n",
    "Dropout rate, 0.3 or 0.7\n",
    "Set n_best_size = 5 and for few questions show models top 5 predicted answers along with\n",
    "actual.\n",
    "\n",
    "Use “wandb” to record training visualization.\n",
    "\n",
    "Calculate BLUE Score and Rouge for both the models and report the results in table.\n",
    "\n",
    "Also report parameter values which were used to get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opendatasets \n",
    "import opendatasets as od\n",
    "import os\n",
    "\n",
    "# Download the dataset\n",
    "od.download(\"https://www.kaggle.com/thedevastator/comprehensive-medical-q-a-dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating st environment\n",
    "- https://simpletransformers.ai/docs/installation/\n",
    "```bash\n",
    "conda create -n st python pandas tqdm\n",
    "conda activate st\n",
    "conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "pip install simpletransformers\n",
    "pip install wandb\n",
    "pip install --upgrade simpletransformers\n",
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtype</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>susceptibility</td>\n",
       "      <td>Who is at risk for Lymphocytic Choriomeningiti...</td>\n",
       "      <td>LCMV infections can occur after exposure to fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>symptoms</td>\n",
       "      <td>What are the symptoms of Lymphocytic Choriomen...</td>\n",
       "      <td>LCMV is most commonly recognized as causing ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>susceptibility</td>\n",
       "      <td>Who is at risk for Lymphocytic Choriomeningiti...</td>\n",
       "      <td>Individuals of all ages who come into contact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exams and tests</td>\n",
       "      <td>How to diagnose Lymphocytic Choriomeningitis (...</td>\n",
       "      <td>During the first phase of the disease, the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>treatment</td>\n",
       "      <td>What are the treatments for Lymphocytic Chorio...</td>\n",
       "      <td>Aseptic meningitis, encephalitis, or meningoen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16402</th>\n",
       "      <td>symptoms</td>\n",
       "      <td>What are the symptoms of Familial visceral myo...</td>\n",
       "      <td>What are the signs and symptoms of Familial vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16403</th>\n",
       "      <td>information</td>\n",
       "      <td>What is (are) Pseudopelade of Brocq ?</td>\n",
       "      <td>Pseudopelade of Brocq (PBB) is a slowly progre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16404</th>\n",
       "      <td>symptoms</td>\n",
       "      <td>What are the symptoms of Pseudopelade of Brocq ?</td>\n",
       "      <td>What are the signs and symptoms of Pseudopelad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>treatment</td>\n",
       "      <td>What are the treatments for Pseudopelade of Br...</td>\n",
       "      <td>Is there treatment or a cure for pseudopelade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16406</th>\n",
       "      <td>information</td>\n",
       "      <td>What is (are) Desmoplastic small round cell tu...</td>\n",
       "      <td>Desmoplastic small round cell tumors (DSRCT), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16407 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 qtype                                           Question  \\\n",
       "0       susceptibility  Who is at risk for Lymphocytic Choriomeningiti...   \n",
       "1             symptoms  What are the symptoms of Lymphocytic Choriomen...   \n",
       "2       susceptibility  Who is at risk for Lymphocytic Choriomeningiti...   \n",
       "3      exams and tests  How to diagnose Lymphocytic Choriomeningitis (...   \n",
       "4            treatment  What are the treatments for Lymphocytic Chorio...   \n",
       "...                ...                                                ...   \n",
       "16402         symptoms  What are the symptoms of Familial visceral myo...   \n",
       "16403      information              What is (are) Pseudopelade of Brocq ?   \n",
       "16404         symptoms   What are the symptoms of Pseudopelade of Brocq ?   \n",
       "16405        treatment  What are the treatments for Pseudopelade of Br...   \n",
       "16406      information  What is (are) Desmoplastic small round cell tu...   \n",
       "\n",
       "                                                  Answer  \n",
       "0      LCMV infections can occur after exposure to fr...  \n",
       "1      LCMV is most commonly recognized as causing ne...  \n",
       "2      Individuals of all ages who come into contact ...  \n",
       "3      During the first phase of the disease, the mos...  \n",
       "4      Aseptic meningitis, encephalitis, or meningoen...  \n",
       "...                                                  ...  \n",
       "16402  What are the signs and symptoms of Familial vi...  \n",
       "16403  Pseudopelade of Brocq (PBB) is a slowly progre...  \n",
       "16404  What are the signs and symptoms of Pseudopelad...  \n",
       "16405  Is there treatment or a cure for pseudopelade ...  \n",
       "16406  Desmoplastic small round cell tumors (DSRCT), ...  \n",
       "\n",
       "[16407 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('comprehensive-medical-q-a-dataset/train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "converted_data = []\n",
    "for index, row in data.iterrows():\n",
    "    context = row['Question'] + \" \" + row['Answer']\n",
    "    answer_start = len(row['Question']) + 1  # +1 for the space\n",
    "    converted_data.append({\n",
    "        'qas': [\n",
    "            {\n",
    "                'id': str(index),\n",
    "                'question': row['Question'],\n",
    "                'answers': [\n",
    "                    {\n",
    "                        'text': row['Answer'],\n",
    "                        'answer_start': answer_start\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        'context': context\n",
    "    })\n",
    "\n",
    "train_data, test_data = train_test_split(converted_data, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'id': '6723',\n",
       "   'question': 'What is (are) Muckle-Wells syndrome ?',\n",
       "   'answers': [{'text': 'Muckle-Wells syndrome is a disorder characterized by periodic episodes of skin rash, fever, and joint pain. Progressive hearing loss and kidney damage also occur in this disorder.  People with Muckle-Wells syndrome have recurrent \"flare-ups\" that begin during infancy or early childhood. These episodes may appear to arise spontaneously or be triggered by cold, heat, fatigue, or other stresses. Affected individuals typically develop a non-itchy rash, mild to moderate fever, painful and swollen joints, and in some cases redness in the whites of the eyes (conjunctivitis).  Hearing loss caused by progressive nerve damage (sensorineural deafness) typically becomes apparent during the teenage years. Abnormal deposits of a protein called amyloid (amyloidosis) cause progressive kidney damage in about one-third of people with Muckle-Wells syndrome; these deposits may also damage other organs. In addition, pigmented skin lesions may occur in affected individuals.',\n",
       "     'answer_start': 38}]}],\n",
       " 'context': 'What is (are) Muckle-Wells syndrome ? Muckle-Wells syndrome is a disorder characterized by periodic episodes of skin rash, fever, and joint pain. Progressive hearing loss and kidney damage also occur in this disorder.  People with Muckle-Wells syndrome have recurrent \"flare-ups\" that begin during infancy or early childhood. These episodes may appear to arise spontaneously or be triggered by cold, heat, fatigue, or other stresses. Affected individuals typically develop a non-itchy rash, mild to moderate fever, painful and swollen joints, and in some cases redness in the whites of the eyes (conjunctivitis).  Hearing loss caused by progressive nerve damage (sensorineural deafness) typically becomes apparent during the teenage years. Abnormal deposits of a protein called amyloid (amyloidosis) cause progressive kidney damage in about one-third of people with Muckle-Wells syndrome; these deposits may also damage other organs. In addition, pigmented skin lesions may occur in affected individuals.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'id': '15104',\n",
       "   'question': 'What are the treatments for 21-hydroxylase deficiency ?',\n",
       "   'answers': [{'text': 'What is the goal for treating 21-hydroxylase-deficient congenital adrenal hyperplasia? The objectives for treating 21-hydroxylase deficiency differ with age. In childhood, the overall goal is to replace cortisol. Obtaining hormonal balance is important and patients growth velocity and bone age is monitored. Routine analysis of blood, urine, and/or saliva may also be necessary. Corrective surgery is frequently required for females born with abnormal genitalia. In late childhood and adolescence, maintaining hormonal balance is equally important. Overtreatment may result in obesity and delayed menarche/puberty, whereas under-replacement will result in sexual precocity. Also, it is important that teens and young adults with 21-hydroxylase deficiency be successfully transitioned to adult care facilities. Follow-up of adult patients should involve multidisciplinary clinics. Problems in adult women include fertility concerns, excessive hair growth, and menstrual irregularity; obesity and impact of short stature; sexual dysfunction and psychological problems. Counseling may be helpful. Adult males may develop enlargement of the testes and if so, should work with an endocrinologist familiar with the management of patients with this deficiency.',\n",
       "     'answer_start': 56}]}],\n",
       " 'context': 'What are the treatments for 21-hydroxylase deficiency ? What is the goal for treating 21-hydroxylase-deficient congenital adrenal hyperplasia? The objectives for treating 21-hydroxylase deficiency differ with age. In childhood, the overall goal is to replace cortisol. Obtaining hormonal balance is important and patients growth velocity and bone age is monitored. Routine analysis of blood, urine, and/or saliva may also be necessary. Corrective surgery is frequently required for females born with abnormal genitalia. In late childhood and adolescence, maintaining hormonal balance is equally important. Overtreatment may result in obesity and delayed menarche/puberty, whereas under-replacement will result in sexual precocity. Also, it is important that teens and young adults with 21-hydroxylase deficiency be successfully transitioned to adult care facilities. Follow-up of adult patients should involve multidisciplinary clinics. Problems in adult women include fertility concerns, excessive hair growth, and menstrual irregularity; obesity and impact of short stature; sexual dysfunction and psychological problems. Counseling may be helpful. Adult males may develop enlargement of the testes and if so, should work with an endocrinologist familiar with the management of patients with this deficiency.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q simpletransformers rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: ammar-90123 (ammar-90). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_predictions(predictions):\n",
    "    predicted_answers = []\n",
    "    for prediction in predictions:\n",
    "        predicted_answers.append(prediction['answer'])\n",
    "    return predicted_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answers(data):\n",
    "    actual_answers = []\n",
    "    for data_item in data:  # Iterate over each dictionary in the list\n",
    "        for item in data_item['qas']:\n",
    "            for answer in item['answers']:\n",
    "                actual_answers.append(answer['text'])\n",
    "    return actual_answers\n",
    "# To get the actual answers from the test data\n",
    "actual_answers = extract_answers(test_data)\n",
    "\n",
    "To get the predicted answers from the model\n",
    "The model.predict() function expects a list of contexts and questions\n",
    "If test_data is a list of dictionaries\n",
    "contexts = [data['context'] for data in test_data]\n",
    "questions = [[qas['question'] for qas in data['qas']] for data in test_data]\n",
    "\n",
    "# Combine contexts and questions\n",
    "to_predict = [{'context': context, 'qas': [{'question': question, 'id': str(i)} for i, question in enumerate(questions_list)]} for context, questions_list in zip(contexts, questions)]\n",
    "# Predict answers\n",
    "predictions = model.predict(to_predict)\n",
    "\n",
    "# The predictions are a list of two lists. The first list contains dictionaries with 'id' and 'answer' keys.\n",
    "predicted_answers = [pred['answer'][0] for pred in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores)  \u001b[38;5;66;03m# Return average BLEU score\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_answers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage BLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bleu_score)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Function to calculate ROUGE scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 54\u001b[0m, in \u001b[0;36mcalculate_bleu\u001b[1;34m(actual_answers, predicted_answers)\u001b[0m\n\u001b[0;32m     52\u001b[0m     reference \u001b[38;5;241m=\u001b[39m actual\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Actual answer tokens\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Predicted answer tokens\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores)\n",
      "File \u001b[1;32mc:\\Users\\ammar\\anaconda3\\envs\\st\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_bleu\u001b[39m(\n\u001b[0;32m     21\u001b[0m     references,\n\u001b[0;32m     22\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m ):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    :rtype: float / list(float)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reweigh\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ammar\\anaconda3\\envs\\st\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\ammar\\anaconda3\\envs\\st\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[0;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import logging\n",
    "# import wandb\n",
    "# import os\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from rouge_score import rouge_scorer\n",
    "# from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "# import numpy as np\n",
    "\n",
    "# os.environ[\"WANDB_HTTP_TIMEOUT\"] = \"180\"\n",
    "# wandb.init(project=\"MedQuad\", entity=\"ammar-90\",\n",
    "#            config={\"batch_size\": 12, \"epochs\": 3, \"learning_rate\": 3e-5, \"train_size\": len(train_data), \"eval_size\": len(test_data) })\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# model_args = {\n",
    "#     'reprocess_input_data': True,\n",
    "#     'overwrite_output_dir': True,\n",
    "#     'num_train_epochs': 3,\n",
    "#     'learning_rate': 3e-5,\n",
    "#     'n_best_size': 5,\n",
    "#     'max_seq_length': 384,\n",
    "#     'doc_stride': 128,\n",
    "#     'train_batch_size': 12,\n",
    "#     'gradient_accumulation_steps': 8,\n",
    "#      'wandb_project': 'MedQuad',\n",
    "#      \"use_multiprocessing_for_evaluation\": True,\n",
    "# \"multiprocessing_chunksize\": 5\n",
    "# }\n",
    "\n",
    "# model = QuestionAnsweringModel(\n",
    "#     \"bert\", \"bert-base-uncased\", args=model_args, use_cuda=True\n",
    "# )\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# model.train_model(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "# eval_results = model.eval_model(test_data)\n",
    "\n",
    "# print(eval_results)\n",
    "\n",
    "# actual_answers = extract_answers(test_data)\n",
    "# The predictions are a list of two lists. The first list contains dictionaries with 'id' and 'answer' keys.\n",
    "predicted_answers = [pred['answer'][0] for pred in predictions[0]]\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(actual_answers, predicted_answers):\n",
    "    scores = []\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        reference = actual.split()  # Actual answer tokens\n",
    "        candidate = predicted.split()  # Predicted answer tokens\n",
    "        score = sentence_bleu([reference], candidate)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)  # Return average BLEU score\n",
    "\n",
    "bleu_score = calculate_bleu(actual_answers, predicted_answers)\n",
    "print(\"Average BLEU Score:\", bleu_score)\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(actual_answers, predicted_answers):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        score = scorer.score(actual, predicted)\n",
    "        for key in scores:\n",
    "            scores[key].append(score[key].fmeasure)  # We are using the F1 measure here\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_scores = {key: np.mean(value) for key, value in scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "rouge_scores = calculate_rouge(actual_answers, predicted_answers)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MobileBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"WANDB_HTTP_TIMEOUT\"] = \"180\"\n",
    "wandb.init(project=\"MedQuad\", entity=\"ammar-90\",\n",
    "           config={\"batch_size\": 12, \"epochs\": 3, \"learning_rate\": 3e-5, \"train_size\": len(train_data), \"eval_size\": len(test_data) })\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = {\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 3e-5,\n",
    "    'n_best_size': 5,\n",
    "    'max_seq_length': 384,\n",
    "    'doc_stride': 128,\n",
    "    'train_batch_size': 12,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "     'wandb_project': 'MedQuad',\n",
    "     \"use_multiprocessing_for_evaluation\": True,\n",
    "\"multiprocessing_chunksize\": 5\n",
    "}\n",
    "\n",
    "model = QuestionAnsweringModel(\n",
    "    \"mobilebert\", \"google/mobilebert-uncased\", args=model_args, use_cuda=True\n",
    ")\n",
    "\n",
    "model.train_model(train_data)\n",
    "\n",
    "results, model_outputs, wrong_predictions = model.eval_model(test_data)\n",
    "\n",
    "print(results)\n",
    "\n",
    "actual_answers = [x['answers'][0]['text'] for x in test_data]\n",
    "predicted_answers = model.predict([x['context'] for x in test_data])\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(actual_answers, predicted_answers):\n",
    "    scores = []\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        reference = actual.split()  # Actual answer tokens\n",
    "        candidate = predicted.split()  # Predicted answer tokens\n",
    "        score = sentence_bleu([reference], candidate)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)  # Return average BLEU score\n",
    "\n",
    "bleu_score = calculate_bleu(actual_answers, predicted_answers)\n",
    "print(\"Average BLEU Score:\", bleu_score)\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(actual_answers, predicted_answers):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        score = scorer.score(actual, predicted)\n",
    "        for key in scores:\n",
    "            scores[key].append(score[key].fmeasure)  # We are using the F1 measure here\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_scores = {key: np.mean(value) for key, value in scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "rouge_scores = calculate_rouge(actual_answers, predicted_answers)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"WANDB_HTTP_TIMEOUT\"] = \"180\"\n",
    "wandb.init(project=\"MedQuad\", entity=\"ammar-90\",\n",
    "           config={\"batch_size\": 12, \"epochs\": 3, \"learning_rate\": 3e-5, \"train_size\": len(train_data), \"eval_size\": len(test_data) })\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = {\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 3e-5,\n",
    "    'n_best_size': 5,\n",
    "    'max_seq_length': 384,\n",
    "    'doc_stride': 128,\n",
    "    'train_batch_size': 12,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "     'wandb_project': 'MedQuad',\n",
    "     \"use_multiprocessing_for_evaluation\": True,\n",
    "\"multiprocessing_chunksize\": 5\n",
    "}\n",
    "\n",
    "model = QuestionAnsweringModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=True\n",
    ")\n",
    "\n",
    "model.train_model(train_data)\n",
    "\n",
    "results, model_outputs, wrong_predictions = model.eval_model(test_data)\n",
    "\n",
    "print(results)\n",
    "\n",
    "actual_answers = [x['answers'][0]['text'] for x in test_data]\n",
    "predicted_answers = model.predict([x['context'] for x in test_data])\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(actual_answers, predicted_answers):\n",
    "    scores = []\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        reference = actual.split()  # Actual answer tokens\n",
    "        candidate = predicted.split()  # Predicted answer tokens\n",
    "        score = sentence_bleu([reference], candidate)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)  # Return average BLEU score\n",
    "\n",
    "bleu_score = calculate_bleu(actual_answers, predicted_answers)\n",
    "print(\"Average BLEU Score:\", bleu_score)\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(actual_answers, predicted_answers):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "    for actual, predicted in zip(actual_answers, predicted_answers):\n",
    "        score = scorer.score(actual, predicted)\n",
    "        for key in scores:\n",
    "            scores[key].append(score[key].fmeasure)  # We are using the F1 measure here\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_scores = {key: np.mean(value) for key, value in scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "rouge_scores = calculate_rouge(actual_answers, predicted_answers)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
