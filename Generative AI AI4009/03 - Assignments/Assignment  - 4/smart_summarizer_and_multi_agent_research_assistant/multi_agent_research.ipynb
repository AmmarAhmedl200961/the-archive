{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Autonomous Research Assistant\n",
    "\n",
    "This notebook implements a multi-agent autonomous research system using LangGraph/LangChain. The system automates the academic research process by:\n",
    "\n",
    "1. Accepting user-provided research keywords\n",
    "2. Retrieving relevant academic papers from multiple sources\n",
    "3. Ranking and selecting the most significant papers\n",
    "4. Summarizing their contents using a fine-tuned model\n",
    "5. Comparing insights to identify common themes, contradictions, and research gaps\n",
    "\n",
    "The system consists of five specialized agents orchestrated using LangGraph:\n",
    "\n",
    "1. **KeywordAgent**: Enhances the user's input by generating expanded and related keywords\n",
    "2. **SearchAgent**: Interfaces with academic search APIs to retrieve papers\n",
    "3. **RankAgent**: Scores and ranks papers using a multi-criteria strategy\n",
    "4. **SummaryAgent**: Processes selected papers using a fine-tuned summarization model\n",
    "5. **CompareAgent**: Performs comparative analysis on the summarized content\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Agent Implementation](#2-agent-implementation)\n",
    "3. [Agent Orchestration with LangGraph](#3-agent-orchestration-with-langgraph)\n",
    "4. [Testing and Evaluation](#4-testing-and-evaluation)\n",
    "5. [Research Report Generation](#5-research-report-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install all the required libraries and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install langchain langchain_openai \"langgraph>=0.0.19\" pydantic langchain_community langchain_core bs4 scholarly arxiv feedparser tqdm nltk matplotlib pandas numpy requests torch transformers rich lxml streamlit together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union, Callable\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import arxiv\n",
    "import feedparser\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import LangChain and LangGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Import LangGraph components\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages, get_messages_by_id, MessageGraph\n",
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "# Import LLM components\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Import LangChain community components\n",
    "from langchain_community.tools import BaseTool\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "# Import Transformers for the fine-tuned model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set up API keys\n",
    "# Replace with your actual API keys\n",
    "api_keys = {\n",
    "    \"TOGETHER_API_KEY\": \"YOUR_TOGETHER_API_KEY\",\n",
    "    \"OPENAI_API_KEY\": \"YOUR_OPENAI_API_KEY\",\n",
    "    \"SEMANTIC_SCHOLAR_API_KEY\": \"YOUR_SEMANTIC_SCHOLAR_API_KEY\" # Optional\n",
    "}\n",
    "\n",
    "# Set environment variables\n",
    "for key, value in api_keys.items():\n",
    "    os.environ[key] = value\n",
    "\n",
    "# Initialize Together.ai API\n",
    "import together\n",
    "together.api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # You can replace with a different model\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# Alternative: Use Together.ai API\n",
    "# from langchain_community.llms import Together\n",
    "# llm = Together(\n",
    "#     model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "#     temperature=0.3,\n",
    "#     max_tokens=1024\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Fine-Tuned Summarization Model\n",
    "\n",
    "Load the fine-tuned model from Part A for paper summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "def load_fine_tuned_model(model_path=\"./fine_tuned_model\"):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model for paper summarization.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model\n",
    "        \n",
    "    Returns:\n",
    "        model, tokenizer: The loaded model and tokenizer\n",
    "    \"\"\"\n",
    "    # Check if model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model path {model_path} does not exist.\")\n",
    "        print(\"Using fallback method with Together.ai API for summarization.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load the base model\n",
    "        base_model_name = \"mistralai/Mistral-7B-v0.1\"  # Use the same base model as in Part A\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Check if CUDA is available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Load the base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_4bit=True if torch.cuda.is_available() else False,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load the fine-tuned model\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        print(\"Fine-tuned model loaded successfully.\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model: {e}\")\n",
    "        print(\"Using fallback method with Together.ai API for summarization.\")\n",
    "        return None, None\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model, tokenizer = load_fine_tuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Data Models\n",
    "\n",
    "Define Pydantic models for the data structures used in the multi-agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data models\n",
    "class Keyword(BaseModel):\n",
    "    \"\"\"A keyword or phrase for academic search.\"\"\"\n",
    "    keyword: str = Field(description=\"The keyword or phrase\")\n",
    "    relevance: Optional[float] = Field(default=1.0, description=\"Relevance score from 0.0 to 1.0\")\n",
    "    \n",
    "class KeywordSet(BaseModel):\n",
    "    \"\"\"A set of keywords for academic search.\"\"\"\n",
    "    original_query: str = Field(description=\"Original user query\")\n",
    "    expanded_keywords: List[Keyword] = Field(description=\"List of expanded keywords\")\n",
    "    \n",
    "class Author(BaseModel):\n",
    "    \"\"\"An author of an academic paper.\"\"\"\n",
    "    name: str = Field(description=\"Author's name\")\n",
    "    affiliation: Optional[str] = Field(default=None, description=\"Author's affiliation\")\n",
    "    \n",
    "class Paper(BaseModel):\n",
    "    \"\"\"An academic paper.\"\"\"\n",
    "    id: str = Field(description=\"Unique identifier for the paper\")\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    authors: List[Author] = Field(description=\"List of authors\")\n",
    "    abstract: str = Field(description=\"Abstract of the paper\")\n",
    "    year: Optional[int] = Field(default=None, description=\"Year of publication\")\n",
    "    url: Optional[str] = Field(default=None, description=\"URL to the paper\")\n",
    "    pdf_url: Optional[str] = Field(default=None, description=\"URL to the PDF\")\n",
    "    citation_count: Optional[int] = Field(default=None, description=\"Number of citations\")\n",
    "    source: str = Field(description=\"Source of the paper (e.g., 'arxiv', 'semantic_scholar')\")\n",
    "    keywords: Optional[List[str]] = Field(default=None, description=\"Keywords associated with the paper\")\n",
    "    full_text: Optional[str] = Field(default=None, description=\"Full text of the paper if available\")\n",
    "    \n",
    "class RankedPaper(Paper):\n",
    "    \"\"\"An academic paper with ranking information.\"\"\"\n",
    "    rank_score: float = Field(description=\"Overall ranking score\")\n",
    "    relevance_score: float = Field(description=\"Relevance to the query\")\n",
    "    citation_score: float = Field(description=\"Score based on citations\")\n",
    "    recency_score: float = Field(description=\"Score based on publication recency\")\n",
    "    \n",
    "class PaperSummary(BaseModel):\n",
    "    \"\"\"A summary of an academic paper.\"\"\"\n",
    "    paper_id: str = Field(description=\"ID of the summarized paper\")\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    authors: List[str] = Field(description=\"List of author names\")\n",
    "    summary: str = Field(description=\"Generated summary of the paper\")\n",
    "    methodology: Optional[str] = Field(default=None, description=\"Summary of the methodology used\")\n",
    "    contributions: Optional[str] = Field(default=None, description=\"Key contributions of the paper\")\n",
    "    limitations: Optional[str] = Field(default=None, description=\"Limitations or gaps identified\")\n",
    "    \n",
    "class ComparativeAnalysis(BaseModel):\n",
    "    \"\"\"Comparative analysis of multiple paper summaries.\"\"\"\n",
    "    common_findings: List[str] = Field(description=\"Common findings across papers\")\n",
    "    contradictions: List[str] = Field(description=\"Contradictory findings between papers\")\n",
    "    research_gaps: List[str] = Field(description=\"Identified research gaps\")\n",
    "    future_directions: List[str] = Field(description=\"Suggested future research directions\")\n",
    "    \n",
    "class ResearchReport(BaseModel):\n",
    "    \"\"\"Complete research report.\"\"\"\n",
    "    topic: str = Field(description=\"Research topic\")\n",
    "    topic_summary: str = Field(description=\"Brief overview of the topic\")\n",
    "    expanded_keywords: List[str] = Field(description=\"Expanded keywords used for search\")\n",
    "    paper_summaries: List[PaperSummary] = Field(description=\"Summaries of top papers\")\n",
    "    comparative_analysis: ComparativeAnalysis = Field(description=\"Comparative analysis of papers\")\n",
    "    timestamp: str = Field(description=\"Timestamp of the report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent Implementation\n",
    "\n",
    "Implement each of the five specialized agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 KeywordAgent\n",
    "\n",
    "Enhances the user's input by generating expanded and related keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordAgent:\n",
    "    \"\"\"Agent that expands user-provided keywords into a comprehensive set of search terms.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define the prompt template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"You are an expert research assistant specialized in expanding search queries into comprehensive sets of keywords for academic research.\n",
    "\n",
    "Given the following research query, generate an expanded list of 8-12 related keywords and phrases that would be useful for finding relevant academic papers. \n",
    "Include both broader and more specific terms. Consider synonyms, related concepts, and any important subtopics.\n",
    "\n",
    "Research query: {query}\n",
    "\n",
    "Please respond with a JSON object in the following format:\n",
    "{{\"original_query\": \"the original query\",\n",
    "\"expanded_keywords\": [\n",
    "    {{\"keyword\": \"first keyword\", \"relevance\": 0.9}},\n",
    "    {{\"keyword\": \"second keyword\", \"relevance\": 0.8}},\n",
    "    ... and so on\n",
    "]}}\n",
    "\n",
    "Relevance should be a float between 0.0 and 1.0, where 1.0 indicates highest relevance to the original query.\n",
    "Order the keywords by relevance, with the most relevant ones first.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Set up the output parser\n",
    "        self.output_parser = JsonOutputParser()\n",
    "        \n",
    "        # Create the chain\n",
    "        self.chain = self.prompt_template | self.llm | self.output_parser\n",
    "    \n",
    "    def process(self, query: str) -> KeywordSet:\n",
    "        \"\"\"Expand the user's query into a set of related keywords.\"\"\"\n",
    "        try:\n",
    "            # Generate expanded keywords using the LLM\n",
    "            result = self.chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Convert the result to a KeywordSet object\n",
    "            expanded_keywords = [Keyword(keyword=item[\"keyword\"], relevance=item[\"relevance\"]) \n",
    "                                for item in result[\"expanded_keywords\"]]\n",
    "            \n",
    "            return KeywordSet(\n",
    "                original_query=result[\"original_query\"],\n",
    "                expanded_keywords=expanded_keywords\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in KeywordAgent: {e}\")\n",
    "            # Fallback: return the original query as a keyword\n",
    "            return KeywordSet(\n",
    "                original_query=query,\n",
    "                expanded_keywords=[Keyword(keyword=query, relevance=1.0)]\n",
    "            )\n",
    "    \n",
    "    def __call__(self, query: str) -> KeywordSet:\n",
    "        \"\"\"Process the query and return a KeywordSet.\"\"\"\n",
    "        return self.process(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SearchAgent\n",
    "\n",
    "Interfaces with academic search APIs to retrieve relevant papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAgent:\n",
    "    \"\"\"Agent that searches for academic papers across multiple sources.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.MAX_RESULTS_PER_SOURCE = 5  # Max results to retrieve from each source\n",
    "        self.semantic_scholar_api_key = os.environ.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    \n",
    "    def search_arxiv(self, query, max_results=5):\n",
    "        \"\"\"Search arXiv for papers matching the query.\"\"\"\n",
    "        try:\n",
    "            # Use the arXiv API via the arxiv library\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            \n",
    "            papers = []\n",
    "            for result in search.results():\n",
    "                # Extract the year from the published date\n",
    "                year = result.published.year if hasattr(result, 'published') else None\n",
    "                \n",
    "                paper = Paper(\n",
    "                    id=result.entry_id.split('/')[-1],\n",
    "                    title=result.title,\n",
    "                    authors=[Author(name=author.name) for author in result.authors],\n",
    "                    abstract=result.summary,\n",
    "                    year=year,\n",
    "                    url=result.entry_id,\n",
    "                    pdf_url=result.pdf_url,\n",
    "                    citation_count=None,  # arXiv doesn't provide citation count\n",
    "                    source=\"arxiv\",\n",
    "                    keywords=None,  # arXiv doesn't provide keywords\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching arXiv: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_arxiv_api(self, query, max_results=5):\n",
    "        \"\"\"Search arXiv using the official API.\"\"\"\n",
    "        try:\n",
    "            # Format the query for the arXiv API\n",
    "            encoded_query = quote(query)\n",
    "            url = f\"http://export.arxiv.org/api/query?search_query=all:{encoded_query}&start=0&max_results={max_results}\"\n",
    "            \n",
    "            # Send the request\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the response using feedparser\n",
    "            feed = feedparser.parse(response.content)\n",
    "            \n",
    "            papers = []\n",
    "            for entry in feed.entries:\n",
    "                # Extract the year from the published date\n",
    "                published = entry.get('published', '')\n",
    "                year = int(published[:4]) if published and len(published) >= 4 else None\n",
    "                \n",
    "                # Extract authors\n",
    "                authors = []\n",
    "                for author in entry.get('authors', []):\n",
    "                    name = author.get('name', '')\n",
    "                    if name:\n",
    "                        authors.append(Author(name=name))\n",
    "                \n",
    "                # Create the paper object\n",
    "                paper = Paper(\n",
    "                    id=entry.get('id', '').split('/')[-1],\n",
    "                    title=entry.get('title', ''),\n",
    "                    authors=authors,\n",
    "                    abstract=entry.get('summary', ''),\n",
    "                    year=year,\n",
    "                    url=entry.get('link', ''),\n",
    "                    pdf_url=next((link.href for link in entry.get('links', []) if link.get('title') == 'pdf'), None),\n",
    "                    citation_count=None,\n",
    "                    source=\"arxiv\",\n",
    "                    keywords=None,\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching arXiv API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_semantic_scholar(self, query, max_results=5):\n",
    "        \"\"\"Search Semantic Scholar for papers matching the query.\"\"\"\n",
    "        try:\n",
    "            # Use the Semantic Scholar API\n",
    "            url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"limit\": max_results,\n",
    "                \"fields\": \"title,authors,abstract,year,url,citationCount,influentialCitationCount\"\n",
    "            }\n",
    "            \n",
    "            headers = {}\n",
    "            if self.semantic_scholar_api_key:\n",
    "                headers[\"x-api-key\"] = self.semantic_scholar_api_key\n",
    "            \n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            papers = []\n",
    "            for item in data.get('data', []):\n",
    "                # Extract authors\n",
    "                authors = []\n",
    "                for author in item.get('authors', []):\n",
    "                    name = author.get('name', '')\n",
    "                    if name:\n",
    "                        authors.append(Author(name=name))\n",
    "                \n",
    "                # Create the paper object\n",
    "                paper = Paper(\n",
    "                    id=item.get('paperId', ''),\n",
    "                    title=item.get('title', ''),\n",
    "                    authors=authors,\n",
    "                    abstract=item.get('abstract', ''),\n",
    "                    year=item.get('year'),\n",
    "                    url=item.get('url'),\n",
    "                    pdf_url=None,  # Semantic Scholar API doesn't directly provide PDF URL\n",
    "                    citation_count=item.get('citationCount'),\n",
    "                    source=\"semantic_scholar\",\n",
    "                    keywords=None,\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching Semantic Scholar: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_pubmed(self, query, max_results=5):\n",
    "        \"\"\"Search PubMed for papers matching the query.\"\"\"\n",
    "        try:\n",
    "            # Format the query for the PubMed eUtils API\n",
    "            encoded_query = quote(query)\n",
    "            \n",
    "            # First get the IDs of matching articles\n",
    "            esearch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={encoded_query}&retmode=json&retmax={max_results}\"\n",
    "            esearch_response = requests.get(esearch_url)\n",
    "            esearch_response.raise_for_status()\n",
    "            esearch_data = esearch_response.json()\n",
    "            \n",
    "            # Get the list of IDs\n",
    "            id_list = esearch_data.get('esearchresult', {}).get('idlist', [])\n",
    "            if not id_list:\n",
    "                return []\n",
    "            \n",
    "            # Then fetch details for these IDs\n",
    "            ids_string = \",\".join(id_list)\n",
    "            efetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={ids_string}&retmode=xml\"\n",
    "            efetch_response = requests.get(efetch_url)\n",
    "            efetch_response.raise_for_status()\n",
    "            \n",
    "            # Parse the XML response\n",
    "            root = ET.fromstring(efetch_response.content)\n",
    "            papers = []\n",
    "            \n",
    "            for article in root.findall(\".//PubmedArticle\"):\n",
    "                try:\n",
    "                    # Extract PubMed ID\n",
    "                    pmid = article.find(\".//PMID\").text if article.find(\".//PMID\") is not None else \"unknown\"\n",
    "                    \n",
    "                    # Extract article title\n",
    "                    title_element = article.find(\".//ArticleTitle\")\n",
    "                    title = title_element.text if title_element is not None else \"Unknown Title\"\n",
    "                    \n",
    "                    # Extract abstract\n",
    "                    abstract_elements = article.findall(\".//AbstractText\")\n",
    "                    abstract = \" \".join([elem.text for elem in abstract_elements if elem.text is not None]) if abstract_elements else \"\"\n",
    "                    \n",
    "                    # Extract year\n",
    "                    year_element = article.find(\".//PubDate/Year\")\n",
    "                    year = int(year_element.text) if year_element is not None and year_element.text.isdigit() else None\n",
    "                    \n",
    "                    # Extract authors\n",
    "                    authors = []\n",
    "                    author_elements = article.findall(\".//Author\")\n",
    "                    for author_elem in author_elements:\n",
    "                        last_name = author_elem.find(\"LastName\")\n",
    "                        fore_name = author_elem.find(\"ForeName\")\n",
    "                        name = \"\"\n",
    "                        if last_name is not None and last_name.text is not None:\n",
    "                            name += last_name.text\n",
    "                            if fore_name is not None and fore_name.text is not None:\n",
    "                                name = f\"{fore_name.text} {name}\"\n",
    "                        if name:\n",
    "                            authors.append(Author(name=name))\n",
    "                    \n",
    "                    # Create URL to the PubMed article\n",
    "                    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "                    \n",
    "                    # Create paper object\n",
    "                    paper = Paper(\n",
    "                        id=pmid,\n",
    "                        title=title,\n",
    "                        authors=authors,\n",
    "                        abstract=abstract,\n",
    "                        year=year,\n",
    "                        url=url,\n",
    "                        pdf_url=None,  # PubMed doesn't directly provide PDF URLs\n",
    "                        citation_count=None,  # PubMed doesn't provide citation counts\n",
    "                        source=\"pubmed\",\n",
    "                        keywords=None,  # We could extract keywords if needed\n",
    "                    )\n",
    "                    papers.append(paper)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing PubMed article: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process(self, keyword_set: KeywordSet) -> List[Paper]:\n",
    "        \"\"\"Search for papers using the expanded keywords.\"\"\"\n",
    "        all_papers = []\n",
    "        \n",
    "        # Use the top 5 most relevant keywords for searching\n",
    "        top_keywords = sorted(keyword_set.expanded_keywords, key=lambda k: k.relevance, reverse=True)[:5]\n",
    "        \n",
    "        # Search using the original query first\n",
    "        print(f\"Searching for papers with query: {keyword_set.original_query}\")\n",
    "        \n",
    "        # Search arXiv\n",
    "        arxiv_papers = self.search_arxiv_api(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "        if arxiv_papers:\n",
    "            print(f\"Found {len(arxiv_papers)} papers from arXiv\")\n",
    "            all_papers.extend(arxiv_papers)\n",
    "        \n",
    "        # Search PubMed\n",
    "        pubmed_papers = self.search_pubmed(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "        if pubmed_papers:\n",
    "            print(f\"Found {len(pubmed_papers)} papers from PubMed\")\n",
    "            all_papers.extend(pubmed_papers)\n",
    "        \n",
    "        # Search Semantic Scholar if API key is available\n",
    "        if self.semantic_scholar_api_key:\n",
    "            ss_papers = self.search_semantic_scholar(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "            if ss_papers:\n",
    "                print(f\"Found {len(ss_papers)} papers from Semantic Scholar\")\n",
    "                all_papers.extend(ss_papers)\n",
    "        \n",
    "        # Search using each of the top keywords\n",
    "        for keyword in top_keywords:\n",
    "            print(f\"Searching for papers with keyword: {keyword.keyword}\")\n",
    "            \n",
    "            # Search arXiv\n",
    "            arxiv_papers = self.search_arxiv_api(keyword.keyword, self.MAX_RESULTS_PER_SOURCE)\n",
    "            if arxiv_papers:\n",
    "                print(f\"Found {len(arxiv_papers)} papers from arXiv for keyword: {keyword.keyword}\")\n",
    "                all_papers.extend(arxiv_papers)\n",
    "            \n",
    "            # Search PubMed\n",
    "            pubmed_papers = self.search_pubmed(keyword.keyword, self.MAX_RESULTS_PER_SOURCE)\n",
    "            if pubmed_papers:\n",
    "                print(f\"Found {len(pubmed_papers)} papers from PubMed for keyword: {keyword.keyword}\")\n",
    "                all_papers.extend(pubmed_papers)\n",
    "            \n",
    "            # Search Semantic Scholar if API key is available\n",
    "            if self.semantic_scholar_api_key:\n",
    "                ss_papers = self.search_semantic_scholar(keyword.keyword, self.MAX_RESULTS_PER_SOURCE)\n",
    "                if ss_papers:\n",
    "                    print(f\"Found {len(ss_papers)} papers from Semantic Scholar for keyword: {keyword.keyword}\")\n",
    "                    all_papers.extend(ss_papers)\n",
    "        \n",
    "        # Remove duplicates based on paper ID\n",
    "        unique_papers = []\n",
    "        paper_ids = set()\n",
    "        \n",
    "        for paper in all_papers:\n",
    "            if paper.id not in paper_ids:\n",
    "                unique_papers.append(paper)\n",
    "                paper_ids.add(paper.id)\n",
    "        \n",
    "        print(f\"Total unique papers found: {len(unique_papers)}\")\n",
    "        return unique_papers\n",
    "    \n",
    "    def __call__(self, keyword_set: KeywordSet) -> List[Paper]:\n",
    "        \"\"\"Process the keyword set and return a list of papers.\"\"\"\n",
    "        return self.process(keyword_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RankAgent\n",
    "\n",
    "Scores and ranks papers using a multi-criteria strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankAgent:\n",
    "    \"\"\"Agent that ranks academic papers based on multiple criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.current_year = datetime.now().year\n",
    "        \n",
    "        # Define the prompt template for relevance scoring\n",
    "        self.relevance_prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\", \"title\", \"abstract\"],\n",
    "            template=\"\"\"You are an expert academic researcher evaluating the relevance of a paper to a research query.\n",
    "\n",
    "Research query: {query}\n",
    "\n",
    "Paper title: {title}\n",
    "\n",
    "Paper abstract: {abstract}\n",
    "\n",
    "On a scale of 0.0 to 1.0, how relevant is this paper to the research query?\n",
    "- 0.0: Not relevant at all\n",
    "- 0.3: Somewhat relevant but only tangentially related\n",
    "- 0.5: Moderately relevant\n",
    "- 0.7: Highly relevant but not perfect match\n",
    "- 1.0: Extremely relevant, perfect match\n",
    "\n",
    "First explain your reasoning in 2-3 sentences, then provide a single number between 0.0 and 1.0 as your final relevance score.\n",
    "Your response should end with a line containing only the number as a floating point value.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Set up the chain for relevance scoring\n",
    "        self.relevance_chain = self.relevance_prompt_template | self.llm | StrOutputParser()\n",
    "    \n",
    "    def calculate_citation_score(self, citation_count):\n",
    "        \"\"\"Calculate a score based on citation count.\"\"\"\n",
    "        if citation_count is None:\n",
    "            return 0.5  # Default score when citation count is unknown\n",
    "        \n",
    "        # Log-based scoring to handle papers with very high citation counts\n",
    "        if citation_count == 0:\n",
    "            return 0.1\n",
    "        return min(1.0, 0.1 + 0.3 * np.log10(citation_count + 1))\n",
    "    \n",
    "    def calculate_recency_score(self, year):\n",
    "        \"\"\"Calculate a score based on publication recency.\"\"\"\n",
    "        if year is None:\n",
    "            return 0.5  # Default score when year is unknown\n",
    "        \n",
    "        # Linear decay from current year (1.0) to 10 years ago (0.1)\n",
    "        years_ago = self.current_year - year\n",
    "        if years_ago <= 0:\n",
    "            return 1.0  # Current or future publication (unlikely, but possible)\n",
    "        elif years_ago >= 10:\n",
    "            return 0.1  # 10+ years old\n",
    "        else:\n",
    "            return 1.0 - (0.9 * years_ago / 10.0)\n",
    "    \n",
    "    def calculate_relevance_score(self, paper, query):\n",
    "        \"\"\"Calculate a relevance score using the LLM.\"\"\"\n",
    "        try:\n",
    "            # Generate relevance analysis using the LLM\n",
    "            result = self.relevance_chain.invoke({\n",
    "                \"query\": query,\n",
    "                \"title\": paper.title,\n",
    "                \"abstract\": paper.abstract\n",
    "            })\n",
    "            \n",
    "            # Extract the relevance score (last line of the result)\n",
    "            lines = result.strip().split('\\n')\n",
    "            score_text = lines[-1].strip()\n",
    "            \n",
    "            # Extract the floating point value\n",
    "            score_match = re.search(r'\\d+\\.\\d+|\\d+', score_text)\n",
    "            if score_match:\n",
    "                score = float(score_match.group())\n",
    "                return min(1.0, max(0.0, score))  # Clamp between 0.0 and 1.0\n",
    "            \n",
    "            # Fallback: use a default score\n",
    "            return 0.5\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating relevance score: {e}\")\n",
    "            return 0.5  # Default score on error\n",
    "    \n",
    "    def calculate_rank_score(self, relevance_score, citation_score, recency_score):\n",
    "        \"\"\"Calculate an overall rank score by combining the individual scores.\"\"\"\n",
    "        # Weights for each score (can be adjusted)\n",
    "        weights = {\n",
    "            \"relevance\": 0.6,  # Relevance is most important\n",
    "            \"citation\": 0.25,  # Citations indicate impact\n",
    "            \"recency\": 0.15    # Recency ensures up-to-date information\n",
    "        }\n",
    "        \n",
    "        return (\n",
    "            weights[\"relevance\"] * relevance_score +\n",
    "            weights[\"citation\"] * citation_score +\n",
    "            weights[\"recency\"] * recency_score\n",
    "        )\n",
    "    \n",
    "    def process(self, papers: List[Paper], keyword_set: KeywordSet) -> List[RankedPaper]:\n",
    "        \"\"\"Rank the papers based on multiple criteria.\"\"\"\n",
    "        ranked_papers = []\n",
    "        \n",
    "        print(\"Ranking papers...\")\n",
    "        for i, paper in enumerate(papers):\n",
    "            print(f\"Ranking paper {i+1}/{len(papers)}: {paper.title}\")\n",
    "            \n",
    "            # Calculate individual scores\n",
    "            relevance_score = self.calculate_relevance_score(paper, keyword_set.original_query)\n",
    "            citation_score = self.calculate_citation_score(paper.citation_count)\n",
    "            recency_score = self.calculate_recency_score(paper.year)\n",
    "            \n",
    "            # Calculate overall rank score\n",
    "            rank_score = self.calculate_rank_score(relevance_score, citation_score, recency_score)\n",
    "            \n",
    "            # Create a RankedPaper object\n",
    "            ranked_paper = RankedPaper(\n",
    "                **paper.dict(),\n",
    "                rank_score=rank_score,\n",
    "                relevance_score=relevance_score,\n",
    "                citation_score=citation_score,\n",
    "                recency_score=recency_score\n",
    "            )\n",
    "            \n",
    "            ranked_papers.append(ranked_paper)\n",
    "        \n",
    "        # Sort papers by rank score in descending order\n",
    "        ranked_papers.sort(key=lambda p: p.rank_score, reverse=True)\n",
    "        \n",
    "        return ranked_papers\n",
    "    \n",
    "    def __call__(self, papers: List[Paper], keyword_set: KeywordSet) -> List[RankedPaper]:\n",
    "        \"\"\"Process the papers and return a list of ranked papers.\"\"\"\n",
    "        return self.process(papers, keyword_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 SummaryAgent\n",
    "\n",
    "Processes selected top-ranked papers and generates structured summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryAgent:\n",
    "    \"\"\"Agent that generates summaries of academic papers.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, tokenizer=None, llm=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define the prompt template for structure extraction\n",
    "        self.structure_prompt_template = PromptTemplate(\n",
    "            input_variables=[\"summary\"],\n",
    "            template=\"\"\"You are an expert academic researcher and summarizer. Extract and structure the information from the following paper summary into specific sections.\n",
    "\n",
    "Paper summary: {summary}\n",
    "\n",
    "Extract the following components from the summary:\n",
    "1. Methodology: What research methods or techniques were used?\n",
    "2. Key Contributions: What are the main findings or contributions of the paper?\n",
    "3. Limitations or Gaps: What limitations, open questions, or research gaps were identified?\n",
    "\n",
    "Respond with a JSON object in the following format:\n",
    "{{\"methodology\": \"description of methodology\",\n",
    "\"contributions\": \"key contributions and findings\",\n",
    "\"limitations\": \"limitations and research gaps\"}}\n",
    "\n",
    "Include only the JSON in your response, with no additional text.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Set up the output parser for structured information\n",
    "        self.structure_parser = JsonOutputParser()\n",
    "        \n",
    "        # Create the chain for structure extraction\n",
    "        self.structure_chain = self.structure_prompt_template | self.llm | self.structure_parser\n",
    "        \n",
    "        # Define the prompt template for paper summarization with Together.ai (fallback method)\n",
    "        self.together_prompt_template = \"\"\"Summarize the following academic paper with a focus on the main problem, methods, and key findings.\n",
    "        \n",
    "Title: {title}\n",
    "Authors: {authors}\n",
    "Abstract: {abstract}\n",
    "        \n",
    "Provide a concise but comprehensive summary of the paper that covers the research problem, methodology, and major contributions.\n",
    "Summary:\n",
    "\"\"\"\n",
    "    \n",
    "    def summarize_with_fine_tuned_model(self, paper):\n",
    "        \"\"\"Generate a summary using the fine-tuned model from Part A.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Create the prompt\n",
    "            prompt = f\"\"\"Summarize the following academic paper:\n",
    "            \n",
    "Article: {paper.abstract}\n",
    "            \n",
    "Summary: \"\"\"\n",
    "            \n",
    "            # Tokenize the input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            \n",
    "            # Generate the summary\n",
    "            output = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode the output\n",
    "            summary = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract just the summary part (after \"Summary:\")\n",
    "            try:\n",
    "                summary = summary.split(\"Summary:\")[1].strip()\n",
    "            except IndexError:\n",
    "                pass  # If \"Summary:\" is not in the output, use the whole output\n",
    "            \n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary with fine-tuned model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def summarize_with_together(self, paper):\n",
    "        \"\"\"Generate a summary using the Together.ai API (fallback method).\"\"\"\n",
    "        try:\n",
    "            # Prepare the author string\n",
    "            authors_str = \", \".join([author.name for author in paper.authors])\n",
    "            \n",
    "            # Create the prompt\n",
    "            prompt = self.together_prompt_template.format(\n",
    "                title=paper.title,\n",
    "                authors=authors_str,\n",
    "                abstract=paper.abstract\n",
    "            )\n",
    "            \n",
    "            # Call the Together.ai API\n",
    "            response = together.Complete.create(\n",
    "                prompt=prompt,\n",
    "                model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "                max_tokens=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                top_k=60,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "            \n",
    "            # Extract the summary\n",
    "            summary = response['output']['choices'][0]['text']\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary with Together.ai: {e}\")\n",
    "            return f\"Unable to generate summary due to an error: {e}\"\n",
    "    \n",
    "    def extract_structured_information(self, summary):\n",
    "        \"\"\"Extract structured information from the summary.\"\"\"\n",
    "        try:\n",
    "            result = self.structure_chain.invoke({\"summary\": summary})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting structured information: {e}\")\n",
    "            return {\n",
    "                \"methodology\": \"Not available\",\n",
    "                \"contributions\": \"Not available\",\n",
    "                \"limitations\": \"Not available\"\n",
    "            }\n",
    "    \n",
    "    def process(self, ranked_papers: List[RankedPaper], top_n=3) -> List[PaperSummary]:\n",
    "        \"\"\"Generate summaries for the top-ranked papers.\"\"\"\n",
    "        paper_summaries = []\n",
    "        \n",
    "        # Select the top N papers\n",
    "        top_papers = ranked_papers[:top_n]\n",
    "        \n",
    "        print(f\"Generating summaries for top {len(top_papers)} papers...\")\n",
    "        for i, paper in enumerate(top_papers):\n",
    "            print(f\"Summarizing paper {i+1}/{len(top_papers)}: {paper.title}\")\n",
    "            \n",
    "            # First try to summarize with the fine-tuned model\n",
    "            summary = self.summarize_with_fine_tuned_model(paper)\n",
    "            \n",
    "            # If that fails, use Together.ai as a fallback\n",
    "            if summary is None:\n",
    "                print(\"Using Together.ai API for summarization (fallback method)\")\n",
    "                summary = self.summarize_with_together(paper)\n",
    "            \n",
    "            # Extract structured information from the summary\n",
    "            structured_info = self.extract_structured_information(summary)\n",
    "            \n",
    "            # Create a PaperSummary object\n",
    "            paper_summary = PaperSummary(\n",
    "                paper_id=paper.id,\n",
    "                title=paper.title,\n",
    "                authors=[author.name for author in paper.authors],\n",
    "                summary=summary,\n",
    "                methodology=structured_info.get(\"methodology\"),\n",
    "                contributions=structured_info.get(\"contributions\"),\n",
    "                limitations=structured_info.get(\"limitations\")\n",
    "            )\n",
    "            \n",
    "            paper_summaries.append(paper_summary)\n",
    "        \n",
    "        return paper_summaries\n",
    "    \n",
    "    def __call__(self, ranked_papers: List[RankedPaper], top_n=3) -> List[PaperSummary]:\n",
    "        \"\"\"Process the ranked papers and return a list of paper summaries.\"\"\"\n",
    "        return self.process(ranked_papers, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 CompareAgent\n",
    "\n",
    "Performs comparative analysis on the summarized content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareAgent:\n",
    "    \"\"\"Agent that performs comparative analysis on multiple paper summaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define the prompt template for comparative analysis\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"summaries\"],\n",
    "            template=\"\"\"You are an expert academic researcher analyzing multiple papers on a related topic. Perform a comparative analysis of the following paper summaries to identify common themes, contradictions, research gaps, and future directions.\n",
    "\n",
    "Paper Summaries:\n",
    "{summaries}\n",
    "\n",
    "Based on these summaries, provide a comprehensive comparative analysis in the following JSON format:\n",
    "{{\"common_findings\": [\n",
    "    \"Common finding 1\",\n",
    "    \"Common finding 2\",\n",
    "    ... (at least 3-5 items)\n",
    "],\n",
    "\"contradictions\": [\n",
    "    \"Contradiction 1\",\n",
    "    \"Contradiction 2\",\n",
    "    ... (identify any contradictions between papers)\n",
    "],\n",
    "\"research_gaps\": [\n",
    "    \"Research gap 1\",\n",
    "    \"Research gap 2\",\n",
    "    ... (identify areas that need further research, at least 3-4 items)\n",
    "],\n",
    "\"future_directions\": [\n",
    "    \"Future direction 1\",\n",
    "    \"Future direction 2\",\n",
    "    ... (suggest future research directions, at least 3-4 items)\n",
    "]}}\n",
    "\n",
    "Include only the JSON in your response, with no additional text.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Set up the output parser\n",
    "        self.output_parser = JsonOutputParser()\n",
    "        \n",
    "        # Create the chain\n",
    "        self.chain = self.prompt_template | self.llm | self.output_parser\n",
    "    \n",
    "    def format_paper_summaries(self, paper_summaries: List[PaperSummary]) -> str:\n",
    "        \"\"\"Format the paper summaries for the prompt.\"\"\"\n",
    "        formatted_summaries = \"\"\n",
    "        \n",
    "        for i, summary in enumerate(paper_summaries):\n",
    "            formatted_summaries += f\"Paper {i+1}: {summary.title}\\n\"\n",
    "            formatted_summaries += f\"Authors: {', '.join(summary.authors)}\\n\"\n",
    "            formatted_summaries += f\"Summary: {summary.summary}\\n\"\n",
    "            formatted_summaries += f\"Methodology: {summary.methodology}\\n\"\n",
    "            formatted_summaries += f\"Key Contributions: {summary.contributions}\\n\"\n",
    "            formatted_summaries += f\"Limitations/Gaps: {summary.limitations}\\n\\n\"\n",
    "        \n",
    "        return formatted_summaries\n",
    "    \n",
    "    def process(self, paper_summaries: List[PaperSummary]) -> ComparativeAnalysis:\n",
    "        \"\"\"Perform comparative analysis on the paper summaries.\"\"\"\n",
    "        # Format the paper summaries for the prompt\n",
    "        formatted_summaries = self.format_paper_summaries(paper_summaries)\n",
    "        \n",
    "        print(\"Performing comparative analysis...\")\n",
    "        try:\n",
    "            # Generate comparative analysis using the LLM\n",
    "            result = self.chain.invoke({\"summaries\": formatted_summaries})\n",
    "            \n",
    "            # Create a ComparativeAnalysis object\n",
    "            comparative_analysis = ComparativeAnalysis(\n",
    "                common_findings=result.get(\"common_findings\", []),\n",
    "                contradictions=result.get(\"contradictions\", []),\n",
    "                research_gaps=result.get(\"research_gaps\", []),\n",
    "                future_directions=result.get(\"future_directions\", [])\n",
    "            )\n",
    "            \n",
    "            return comparative_analysis\n",
    "        except Exception as e:\n",
    "            print(f\"Error in comparative analysis: {e}\")\n",
    "            # Return a default ComparativeAnalysis object\n",
    "            return ComparativeAnalysis(\n",
    "                common_findings=[\"Error generating common findings\"],\n",
    "                contradictions=[\"Error generating contradictions\"],\n",
    "                research_gaps=[\"Error generating research gaps\"],\n",
    "                future_directions=[\"Error generating future directions\"]\n",
    "            )\n",
    "    \n",
    "    def __call__(self, paper_summaries: List[PaperSummary]) -> ComparativeAnalysis:\n",
    "        \"\"\"Process the paper summaries and return a comparative analysis.\"\"\"\n",
    "        return self.process(paper_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Orchestration with LangGraph\n",
    "\n",
    "Orchestrate the agents using LangGraph to create a complete research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Define the state for the LangGraph workflow\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    top_n: int\n",
    "    keyword_set: Optional[KeywordSet]\n",
    "    papers: Optional[List[Paper]]\n",
    "    ranked_papers: Optional[List[RankedPaper]]\n",
    "    paper_summaries: Optional[List[PaperSummary]]\n",
    "    comparative_analysis: Optional[ComparativeAnalysis]\n",
    "    research_report: Optional[ResearchReport]\n",
    "    status_message: str # To track progress\n",
    "\n",
    "# Initialize agents\n",
    "keyword_agent = KeywordAgent(llm)\n",
    "search_agent = SearchAgent()\n",
    "rank_agent = RankAgent(llm)\n",
    "summary_agent = SummaryAgent(model=model, tokenizer=tokenizer, llm=llm)\n",
    "compare_agent = CompareAgent(llm)\n",
    "\n",
    "# Define node functions for the graph\n",
    "def run_keyword_expansion(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Expand keywords from the query.\"\"\"\n",
    "    print(\"\\nExpanding keywords...\")\n",
    "    state[\"status_message\"] = \"Expanding keywords...\"\n",
    "    keyword_set = keyword_agent.process(state[\"query\"])\n",
    "    state[\"keyword_set\"] = keyword_set\n",
    "    print(f\"Expanded keywords: {[k.keyword for k in keyword_set.expanded_keywords]}\")\n",
    "    return state\n",
    "\n",
    "def run_literature_search(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Search for papers using the expanded keywords.\"\"\"\n",
    "    print(\"\\nSearching for papers...\")\n",
    "    state[\"status_message\"] = \"Searching for papers...\"\n",
    "    papers = search_agent.process(state[\"keyword_set\"])\n",
    "    state[\"papers\"] = papers\n",
    "    print(f\"Found {len(papers)} papers\")\n",
    "    return state\n",
    "\n",
    "def run_paper_ranking(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Rank the papers based on multiple criteria.\"\"\"\n",
    "    print(\"\\nRanking papers...\")\n",
    "    state[\"status_message\"] = \"Ranking papers...\"\n",
    "    ranked_papers = rank_agent.process(state[\"papers\"], state[\"keyword_set\"])\n",
    "    state[\"ranked_papers\"] = ranked_papers\n",
    "    print(\"\\nTop 5 ranked papers:\")\n",
    "    for i, paper in enumerate(ranked_papers[:5]):\n",
    "        print(f\"{i+1}. {paper.title} (Score: {paper.rank_score:.2f})\")\n",
    "    return state\n",
    "\n",
    "def run_paper_summarization(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Generate summaries for the top-ranked papers.\"\"\"\n",
    "    print(\"\\nSummarizing papers...\")\n",
    "    state[\"status_message\"] = \"Summarizing papers...\"\n",
    "    paper_summaries = summary_agent.process(state[\"ranked_papers\"], state[\"top_n\"])\n",
    "    state[\"paper_summaries\"] = paper_summaries\n",
    "    return state\n",
    "\n",
    "def run_comparative_analysis(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Perform comparative analysis on the paper summaries.\"\"\"\n",
    "    print(\"\\nPerforming comparative analysis...\")\n",
    "    state[\"status_message\"] = \"Performing comparative analysis...\"\n",
    "    comparative_analysis = compare_agent.process(state[\"paper_summaries\"])\n",
    "    state[\"comparative_analysis\"] = comparative_analysis\n",
    "    return state\n",
    "\n",
    "# Define topic summary generation (for the final report)\n",
    "topic_summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"keywords\"],\n",
    "    template=\"\"\"You are an expert academic researcher writing a brief overview of a research topic.\n",
    "\n",
    "Research topic: {query}\n",
    "\n",
    "Related keywords: {keywords}\n",
    "\n",
    "Write a concise (150-200 words) but comprehensive overview of this research topic that explains its significance, key aspects, and relevance in its field.\n",
    "\"\"\"\n",
    ")\n",
    "topic_summary_chain = topic_summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "def generate_research_report(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Generate the final research report.\"\"\"\n",
    "    print(\"\\nGenerating research report...\")\n",
    "    state[\"status_message\"] = \"Generating research report...\"\n",
    "    \n",
    "    # Generate topic summary\n",
    "    keywords_str = \", \".join([k.keyword for k in state[\"keyword_set\"].expanded_keywords])\n",
    "    topic_summary = topic_summary_chain.invoke({\"query\": state[\"query\"], \"keywords\": keywords_str})\n",
    "    \n",
    "    # Create the research report\n",
    "    research_report = ResearchReport(\n",
    "        topic=state[\"query\"],\n",
    "        topic_summary=topic_summary,\n",
    "        expanded_keywords=[k.keyword for k in state[\"keyword_set\"].expanded_keywords],\n",
    "        paper_summaries=state[\"paper_summaries\"],\n",
    "        comparative_analysis=state[\"comparative_analysis\"],\n",
    "        timestamp=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    \n",
    "    state[\"research_report\"] = research_report\n",
    "    state[\"status_message\"] = \"Research complete!\"\n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "def build_research_graph():\n",
    "    \"\"\"Build and return a compiled StateGraph for the research workflow.\"\"\"\n",
    "    # Create a new StateGraph with the ResearchState schema\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"expand_keywords\", run_keyword_expansion)\n",
    "    workflow.add_node(\"search_papers\", run_literature_search)\n",
    "    workflow.add_node(\"rank_papers\", run_paper_ranking)\n",
    "    workflow.add_node(\"summarize_papers\", run_paper_summarization)\n",
    "    workflow.add_node(\"compare_papers\", run_comparative_analysis)\n",
    "    workflow.add_node(\"generate_report\", generate_research_report)\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.set_entry_point(\"expand_keywords\")\n",
    "    workflow.add_edge(\"expand_keywords\", \"search_papers\")\n",
    "    workflow.add_edge(\"search_papers\", \"rank_papers\")\n",
    "    workflow.add_edge(\"rank_papers\", \"summarize_papers\")\n",
    "    workflow.add_edge(\"summarize_papers\", \"compare_papers\")\n",
    "    workflow.add_edge(\"compare_papers\", \"generate_report\")\n",
    "    workflow.add_edge(\"generate_report\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    compiled_graph = workflow.compile()\n",
    "    \n",
    "    return compiled_graph\n",
    "\n",
    "# Create the compiled research graph\n",
    "research_graph = build_research_graph()\n",
    "\n",
    "# Helper function to run the research workflow\n",
    "def run_research(query: str, top_n: int = 3):\n",
    "    \"\"\"Run the complete research workflow with the graph.\"\"\"\n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"top_n\": top_n,\n",
    "        \"status_message\": \"Starting research...\"\n",
    "    }\n",
    "    \n",
    "    # Invoke the graph\n",
    "    print(f\"Starting research workflow for query: {query}\")\n",
    "    final_state = research_graph.invoke(initial_state)\n",
    "    \n",
    "    # Return the research report\n",
    "    return final_state[\"research_report\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing and Evaluation\n",
    "\n",
    "Test the multi-agent research assistant with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow with a sample query\n",
    "sample_query = \"Large Language Models in healthcare\"\n",
    "research_report = run_research(sample_query, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the research report\n",
    "print(f\"Research Report: {research_report.topic}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTopic Summary:\")\n",
    "print(research_report.topic_summary)\n",
    "\n",
    "print(\"\\nExpanded Keywords:\")\n",
    "for keyword in research_report.expanded_keywords:\n",
    "    print(f\"- {keyword}\")\n",
    "\n",
    "print(\"\\nPaper Summaries:\")\n",
    "for i, summary in enumerate(research_report.paper_summaries):\n",
    "    print(f\"\\nPaper {i+1}: {summary.title}\")\n",
    "    print(f\"Authors: {', '.join(summary.authors)}\")\n",
    "    print(f\"Summary: {summary.summary}\")\n",
    "    print(f\"Methodology: {summary.methodology}\")\n",
    "    print(f\"Key Contributions: {summary.contributions}\")\n",
    "    print(f\"Limitations/Gaps: {summary.limitations}\")\n",
    "\n",
    "print(\"\\nComparative Analysis:\")\n",
    "print(\"\\nCommon Findings:\")\n",
    "for finding in research_report.comparative_analysis.common_findings:\n",
    "    print(f\"- {finding}\")\n",
    "\n",
    "print(\"\\nContradictions:\")\n",
    "for contradiction in research_report.comparative_analysis.contradictions:\n",
    "    print(f\"- {contradiction}\")\n",
    "\n",
    "print(\"\\nResearch Gaps:\")\n",
    "for gap in research_report.comparative_analysis.research_gaps:\n",
    "    print(f\"- {gap}\")\n",
    "\n",
    "print(\"\\nFuture Directions:\")\n",
    "for direction in research_report.comparative_analysis.future_directions:\n",
    "    print(f\"- {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Research Report Generation\n",
    "\n",
    "Create functions to export the research report in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_report_to_markdown(research_report, output_file=\"research_report.md\"):\n",
    "    \"\"\"Export the research report to a markdown file.\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(f\"# Research Report: {research_report.topic}\\n\\n\")\n",
    "        f.write(f\"*Generated on: {research_report.timestamp}*\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Topic Summary\\n\\n\")\n",
    "        f.write(f\"{research_report.topic_summary}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Expanded Keywords\\n\\n\")\n",
    "        for keyword in research_report.expanded_keywords:\n",
    "            f.write(f\"- {keyword}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"## Paper Summaries\\n\\n\")\n",
    "        for i, summary in enumerate(research_report.paper_summaries):\n",
    "            f.write(f\"### Paper {i+1}: {summary.title}\\n\\n\")\n",
    "            f.write(f\"**Authors:** {', '.join(summary.authors)}\\n\\n\")\n",
    "            f.write(f\"**Summary:**\\n{summary.summary}\\n\\n\")\n",
    "            f.write(f\"**Methodology:**\\n{summary.methodology}\\n\\n\")\n",
    "            f.write(f\"**Key Contributions:**\\n{summary.contributions}\\n\\n\")\n",
    "            f.write(f\"**Limitations/Gaps:**\\n{summary.limitations}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Comparative Analysis\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Common Findings\\n\\n\")\n",
    "        for finding in research_report.comparative_analysis.common_findings:\n",
    "            f.write(f\"- {finding}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Contradictions\\n\\n\")\n",
    "        for contradiction in research_report.comparative_analysis.contradictions:\n",
    "            f.write(f\"- {contradiction}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Research Gaps\\n\\n\")\n",
    "        for gap in research_report.comparative_analysis.research_gaps:\n",
    "            f.write(f\"- {gap}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Future Directions\\n\\n\")\n",
    "        for direction in research_report.comparative_analysis.future_directions:\n",
    "            f.write(f\"- {direction}\\n\")\n",
    "    \n",
    "    print(f\"Research report exported to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def export_report_to_json(research_report, output_file=\"research_report.json\"):\n",
    "    \"\"\"Export the research report to a JSON file.\"\"\"\n",
    "    # Convert the Pydantic model to a dictionary\n",
    "    report_dict = research_report.dict()\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(report_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"Research report exported to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the research report to markdown and JSON\n",
    "markdown_file = export_report_to_markdown(research_report)\n",
    "json_file = export_report_to_json(research_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Streamlit Interface\n",
    "\n",
    "Create a Streamlit interface for the multi-agent research assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_research_assistant.py\n",
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import together\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import xml.etree.ElementTree as ET\n",
    "import feedparser\n",
    "import arxiv\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Import the agent classes and utility functions\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# Data models (copied from notebook to ensure consistency)\n",
    "class Keyword(BaseModel):\n",
    "    \"\"\"A keyword or phrase for academic search.\"\"\"\n",
    "    keyword: str = Field(description=\"The keyword or phrase\")\n",
    "    relevance: Optional[float] = Field(default=1.0, description=\"Relevance score from 0.0 to 1.0\")\n",
    "    \n",
    "class KeywordSet(BaseModel):\n",
    "    \"\"\"A set of keywords for academic search.\"\"\"\n",
    "    original_query: str = Field(description=\"Original user query\")\n",
    "    expanded_keywords: List[Keyword] = Field(description=\"List of expanded keywords\")\n",
    "    \n",
    "class Author(BaseModel):\n",
    "    \"\"\"An author of an academic paper.\"\"\"\n",
    "    name: str = Field(description=\"Author's name\")\n",
    "    affiliation: Optional[str] = Field(default=None, description=\"Author's affiliation\")\n",
    "    \n",
    "class Paper(BaseModel):\n",
    "    \"\"\"An academic paper.\"\"\"\n",
    "    id: str = Field(description=\"Unique identifier for the paper\")\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    authors: List[Author] = Field(description=\"List of authors\")\n",
    "    abstract: str = Field(description=\"Abstract of the paper\")\n",
    "    year: Optional[int] = Field(default=None, description=\"Year of publication\")\n",
    "    url: Optional[str] = Field(default=None, description=\"URL to the paper\")\n",
    "    pdf_url: Optional[str] = Field(default=None, description=\"URL to the PDF\")\n",
    "    citation_count: Optional[int] = Field(default=None, description=\"Number of citations\")\n",
    "    source: str = Field(description=\"Source of the paper (e.g., 'arxiv', 'semantic_scholar')\")\n",
    "    keywords: Optional[List[str]] = Field(default=None, description=\"Keywords associated with the paper\")\n",
    "    full_text: Optional[str] = Field(default=None, description=\"Full text of the paper if available\")\n",
    "    \n",
    "class RankedPaper(Paper):\n",
    "    \"\"\"An academic paper with ranking information.\"\"\"\n",
    "    rank_score: float = Field(description=\"Overall ranking score\")\n",
    "    relevance_score: float = Field(description=\"Relevance to the query\")\n",
    "    citation_score: float = Field(description=\"Score based on citations\")\n",
    "    recency_score: float = Field(description=\"Score based on publication recency\")\n",
    "    \n",
    "class PaperSummary(BaseModel):\n",
    "    \"\"\"A summary of an academic paper.\"\"\"\n",
    "    paper_id: str = Field(description=\"ID of the summarized paper\")\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    authors: List[str] = Field(description=\"List of author names\")\n",
    "    summary: str = Field(description=\"Generated summary of the paper\")\n",
    "    methodology: Optional[str] = Field(default=None, description=\"Summary of the methodology used\")\n",
    "    contributions: Optional[str] = Field(default=None, description=\"Key contributions of the paper\")\n",
    "    limitations: Optional[str] = Field(default=None, description=\"Limitations or gaps identified\")\n",
    "    \n",
    "class ComparativeAnalysis(BaseModel):\n",
    "    \"\"\"Comparative analysis of multiple paper summaries.\"\"\"\n",
    "    common_findings: List[str] = Field(description=\"Common findings across papers\")\n",
    "    contradictions: List[str] = Field(description=\"Contradictory findings between papers\")\n",
    "    research_gaps: List[str] = Field(description=\"Identified research gaps\")\n",
    "    future_directions: List[str] = Field(description=\"Suggested future research directions\")\n",
    "    \n",
    "class ResearchReport(BaseModel):\n",
    "    \"\"\"Complete research report.\"\"\"\n",
    "    topic: str = Field(description=\"Research topic\")\n",
    "    topic_summary: str = Field(description=\"Brief overview of the topic\")\n",
    "    expanded_keywords: List[str] = Field(description=\"Expanded keywords used for search\")\n",
    "    paper_summaries: List[PaperSummary] = Field(description=\"Summaries of top papers\")\n",
    "    comparative_analysis: ComparativeAnalysis = Field(description=\"Comparative analysis of papers\")\n",
    "    timestamp: str = Field(description=\"Timestamp of the report generation\")\n",
    "\n",
    "# Define ResearchState for the LangGraph workflow\n",
    "class ResearchState(dict):\n",
    "    \"\"\"Dictionary-based state for the research workflow.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Agent implementations\n",
    "class KeywordAgent:\n",
    "    \"\"\"Agent that expands user-provided keywords into a comprehensive set of search terms.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define the prompt template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"You are an expert research assistant specialized in expanding search queries into comprehensive sets of keywords for academic research.\n",
    "\n",
    "Given the following research query, generate an expanded list of 8-12 related keywords and phrases that would be useful for finding relevant academic papers. \n",
    "Include both broader and more specific terms. Consider synonyms, related concepts, and any important subtopics.\n",
    "\n",
    "Research query: {query}\n",
    "\n",
    "Please respond with a JSON object in the following format:\n",
    "{{\"original_query\": \"the original query\",\n",
    "\"expanded_keywords\": [\n",
    "    {{\"keyword\": \"first keyword\", \"relevance\": 0.9}},\n",
    "    {{\"keyword\": \"second keyword\", \"relevance\": 0.8}},\n",
    "    ... and so on\n",
    "]}}\n",
    "\n",
    "Relevance should be a float between 0.0 and 1.0, where 1.0 indicates highest relevance to the original query.\n",
    "Order the keywords by relevance, with the most relevant ones first.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Set up the output parser\n",
    "        self.output_parser = JsonOutputParser()\n",
    "        \n",
    "        # Create the chain\n",
    "        self.chain = self.prompt_template | self.llm | self.output_parser\n",
    "    \n",
    "    def process(self, query: str) -> KeywordSet:\n",
    "        \"\"\"Expand the user's query into a set of related keywords.\"\"\"\n",
    "        try:\n",
    "            # Generate expanded keywords using the LLM\n",
    "            result = self.chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Convert the result to a KeywordSet object\n",
    "            expanded_keywords = [Keyword(keyword=item[\"keyword\"], relevance=item[\"relevance\"]) \n",
    "                                for item in result[\"expanded_keywords\"]]\n",
    "            \n",
    "            return KeywordSet(\n",
    "                original_query=result[\"original_query\"],\n",
    "                expanded_keywords=expanded_keywords\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in KeywordAgent: {e}\")\n",
    "            # Fallback: return the original query as a keyword\n",
    "            return KeywordSet(\n",
    "                original_query=query,\n",
    "                expanded_keywords=[Keyword(keyword=query, relevance=1.0)]\n",
    "            )\n",
    "\n",
    "class SearchAgent:\n",
    "    \"\"\"Agent that searches for academic papers across multiple sources.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.MAX_RESULTS_PER_SOURCE = 5  # Max results to retrieve from each source\n",
    "        self.semantic_scholar_api_key = os.environ.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    \n",
    "    def search_arxiv_api(self, query, max_results=5):\n",
    "        \"\"\"Search arXiv using the official API.\"\"\"\n",
    "        try:\n",
    "            # Format the query for the arXiv API\n",
    "            encoded_query = quote(query)\n",
    "            url = f\"http://export.arxiv.org/api/query?search_query=all:{encoded_query}&start=0&max_results={max_results}\"\n",
    "            \n",
    "            # Send the request\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the response using feedparser\n",
    "            feed = feedparser.parse(response.content)\n",
    "            \n",
    "            papers = []\n",
    "            for entry in feed.entries:\n",
    "                # Extract the year from the published date\n",
    "                published = entry.get('published', '')\n",
    "                year = int(published[:4]) if published and len(published) >= 4 else None\n",
    "                \n",
    "                # Extract authors\n",
    "                authors = []\n",
    "                for author in entry.get('authors', []):\n",
    "                    name = author.get('name', '')\n",
    "                    if name:\n",
    "                        authors.append(Author(name=name))\n",
    "                \n",
    "                # Create the paper object\n",
    "                paper = Paper(\n",
    "                    id=entry.get('id', '').split('/')[-1],\n",
    "                    title=entry.get('title', ''),\n",
    "                    authors=authors,\n",
    "                    abstract=entry.get('summary', ''),\n",
    "                    year=year,\n",
    "                    url=entry.get('link', ''),\n",
    "                    pdf_url=next((link.href for link in entry.get('links', []) if link.get('title') == 'pdf'), None),\n",
    "                    citation_count=None,\n",
    "                    source=\"arxiv\",\n",
    "                    keywords=None,\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching arXiv API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_semantic_scholar(self, query, max_results=5):\n",
    "        \"\"\"Search Semantic Scholar for papers matching the query.\"\"\"\n",
    "        try:\n",
    "            # Use the Semantic Scholar API\n",
    "            url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"limit\": max_results,\n",
    "                \"fields\": \"title,authors,abstract,year,url,citationCount,influentialCitationCount\"\n",
    "            }\n",
    "            \n",
    "            headers = {}\n",
    "            if self.semantic_scholar_api_key:\n",
    "                headers[\"x-api-key\"] = self.semantic_scholar_api_key\n",
    "            \n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            papers = []\n",
    "            for item in data.get('data', []):\n",
    "                # Extract authors\n",
    "                authors = []\n",
    "                for author in item.get('authors', []):\n",
    "                    name = author.get('name', '')\n",
    "                    if name:\n",
    "                        authors.append(Author(name=name))\n",
    "                \n",
    "                # Create the paper object\n",
    "                paper = Paper(\n",
    "                    id=item.get('paperId', ''),\n",
    "                    title=item.get('title', ''),\n",
    "                    authors=authors,\n",
    "                    abstract=item.get('abstract', ''),\n",
    "                    year=item.get('year'),\n",
    "                    url=item.get('url'),\n",
    "                    pdf_url=None,  # Semantic Scholar API doesn't directly provide PDF URL\n",
    "                    citation_count=item.get('citationCount'),\n",
    "                    source=\"semantic_scholar\",\n",
    "                    keywords=None,\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching Semantic Scholar: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_pubmed(self, query, max_results=5):\n",
    "        \"\"\"Search PubMed for papers matching the query.\"\"\"\n",
    "        try:\n",
    "            # Format the query for the PubMed eUtils API\n",
    "            encoded_query = quote(query)\n",
    "            \n",
    "            # First get the IDs of matching articles\n",
    "            esearch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={encoded_query}&retmode=json&retmax={max_results}\"\n",
    "            esearch_response = requests.get(esearch_url)\n",
    "            esearch_response.raise_for_status()\n",
    "            esearch_data = esearch_response.json()\n",
    "            \n",
    "            # Get the list of IDs\n",
    "            id_list = esearch_data.get('esearchresult', {}).get('idlist', [])\n",
    "            if not id_list:\n",
    "                return []\n",
    "            \n",
    "            # Then fetch details for these IDs\n",
    "            ids_string = \",\".join(id_list)\n",
    "            efetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={ids_string}&retmode=xml\"\n",
    "            efetch_response = requests.get(efetch_url)\n",
    "            efetch_response.raise_for_status()\n",
    "            \n",
    "            # Parse the XML response\n",
    "            try:\n",
    "                root = ET.fromstring(efetch_response.content)\n",
    "                papers = []\n",
    "                \n",
    "                for article in root.findall(\".//PubmedArticle\"):\n",
    "                    try:\n",
    "                        # Extract PubMed ID\n",
    "                        pmid = article.find(\".//PMID\").text if article.find(\".//PMID\") is not None else \"unknown\"\n",
    "                        \n",
    "                        # Extract article title\n",
    "                        title_element = article.find(\".//ArticleTitle\")\n",
    "                        title = title_element.text if title_element is not None else \"Unknown Title\"\n",
    "                        \n",
    "                        # Extract abstract\n",
    "                        abstract_elements = article.findall(\".//AbstractText\")\n",
    "                        abstract = \" \".join([elem.text for elem in abstract_elements if elem.text is not None]) if abstract_elements else \"\"\n",
    "                        \n",
    "                        # Extract year\n",
    "                        year_element = article.find(\".//PubDate/Year\")\n",
    "                        year = int(year_element.text) if year_element is not None and year_element.text.isdigit() else None\n",
    "                        \n",
    "                        # Extract authors\n",
    "                        authors = []\n",
    "                        author_elements = article.findall(\".//Author\")\n",
    "                        for author_elem in author_elements:\n",
    "                            last_name = author_elem.find(\"LastName\")\n",
    "                            fore_name = author_elem.find(\"ForeName\")\n",
    "                            name = \"\"\n",
    "                            if last_name is not None and last_name.text is not None:\n",
    "                                name += last_name.text\n",
    "                                if fore_name is not None and fore_name.text is not None:\n",
    "                                    name = f\"{fore_name.text} {name}\"\n",
    "                            if name:\n",
    "                                authors.append(Author(name=name))\n",
    "                        \n",
    "                        # Create URL to the PubMed article\n",
    "                        url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "                        \n",
    "                        # Create paper object\n",
    "                        paper = Paper(\n",
    "                            id=pmid,\n",
    "                            title=title,\n",
    "                            authors=authors,\n",
    "                            abstract=abstract,\n",
    "                            year=year,\n",
    "                            url=url,\n",
    "                            pdf_url=None,  # PubMed doesn't directly provide PDF URLs\n",
    "                            citation_count=None,  # PubMed doesn't provide citation counts\n",
    "                            source=\"pubmed\",\n",
    "                            keywords=None,  # We could extract keywords if needed\n",
    "                        )\n",
    "                        papers.append(paper)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing PubMed article: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                return papers\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing PubMed XML: {e}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process(self, keyword_set: KeywordSet) -> List[Paper]:\n",
    "        \"\"\"Search for papers using the expanded keywords.\"\"\"\n",
    "        all_papers = []\n",
    "        \n",
    "        # Use the top 3 most relevant keywords for searching\n",
    "        top_keywords = sorted(keyword_set.expanded_keywords, key=lambda k: k.relevance, reverse=True)[:3]\n",
    "        \n",
    "        # Search using the original query first\n",
    "        print(f\"Searching for papers with query: {keyword_set.original_query}\")\n",
    "        \n",
    "        # Search arXiv\n",
    "        arxiv_papers = self.search_arxiv_api(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "        if arxiv_papers:\n",
    "            all_papers.extend(arxiv_papers)\n",
    "        \n",
    "        # Search PubMed\n",
    "        pubmed_papers = self.search_pubmed(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "        if pubmed_papers:\n",
    "            all_papers.extend(pubmed_papers)\n",
    "        \n",
    "        # Search Semantic Scholar if API key is available\n",
    "        if self.semantic_scholar_api_key:\n",
    "            ss_papers = self.search_semantic_scholar(keyword_set.original_query, self.MAX_RESULTS_PER_SOURCE)\n",
    "            if ss_papers:\n",
    "                all_papers.extend(ss_papers)\n",
    "        \n",
    "        # Search using each of the top keywords\n",
    "        for keyword in top_keywords:\n",
    "            # Search arXiv with each keyword\n",
    "            arxiv_papers = self.search_arxiv_api(keyword.keyword, self.MAX_RESULTS_PER_SOURCE)\n",
    "            if arxiv_papers:\n",
    "                all_papers.extend(arxiv_papers)\n",
    "            \n",
    "            # Keep the app responsive by not searching too many sources\n",
    "            # In a production app, you might want to add more comprehensive searching\n",
    "        \n",
    "        # Remove duplicates based on paper ID\n",
    "        unique_papers = []\n",
    "        paper_ids = set()\n",
    "        \n",
    "        for paper in all_papers:\n",
    "            if paper.id not in paper_ids:\n",
    "                unique_papers.append(paper)\n",
    "                paper_ids.add(paper.id)\n",
    "        \n",
    "        return unique_papers\n",
    "\n",
    "def load_fine_tuned_model(model_path=\"./fine_tuned_model\"):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model for paper summarization.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model\n",
    "        \n",
    "    Returns:\n",
    "        model, tokenizer: The loaded model and tokenizer\n",
    "    \"\"\"\n",
    "    # Check if model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model path {model_path} does not exist.\")\n",
    "        print(\"Using fallback method with Together.ai API for summarization.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load the base model\n",
    "        base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Check if CUDA is available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load the base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_4bit=True if torch.cuda.is_available() else False,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load the fine-tuned model\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        print(\"Fine-tuned model loaded successfully.\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model: {e}\")\n",
    "        print(\"Using fallback method with Together.ai API for summarization.\")\n",
    "        return None, None\n",
    "\n",
    "# Load LLM\n",
    "def get_llm():\n",
    "    if \"OPENAI_API_KEY\" in os.environ and os.environ[\"OPENAI_API_KEY\"]:\n",
    "        return ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            temperature=0.3, \n",
    "            max_tokens=1024\n",
    "        )\n",
    "    elif \"TOGETHER_API_KEY\" in os.environ and os.environ[\"TOGETHER_API_KEY\"]:\n",
    "        from langchain_community.llms import Together\n",
    "        return Together(\n",
    "            model=\"meta-llama/Llama-3-8B-Instruct\", \n",
    "            temperature=0.3, \n",
    "            max_tokens=1024\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define a function to build and return the research graph\n",
    "def build_research_graph(model, tokenizer, llm):\n",
    "    from typing_extensions import TypedDict\n",
    "    \n",
    "    # Import required agents\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    \n",
    "    # Define the state for the LangGraph workflow\n",
    "    class ResearchState(TypedDict):\n",
    "        query: str\n",
    "        top_n: int\n",
    "        keyword_set: Optional[KeywordSet]\n",
    "        papers: Optional[List[Paper]]\n",
    "        ranked_papers: Optional[List[RankedPaper]]\n",
    "        paper_summaries: Optional[List[PaperSummary]]\n",
    "        comparative_analysis: Optional[ComparativeAnalysis]\n",
    "        research_report: Optional[ResearchReport]\n",
    "        status_message: str # To track progress\n",
    "    \n",
    "    # Initialize agents\n",
    "    keyword_agent = KeywordAgent(llm)\n",
    "    search_agent = SearchAgent()\n",
    "    \n",
    "    # Define node functions for the graph\n",
    "    def run_keyword_expansion(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Expand keywords from the query.\"\"\"\n",
    "        state[\"status_message\"] = \"Expanding keywords...\"\n",
    "        keyword_set = keyword_agent.process(state[\"query\"])\n",
    "        state[\"keyword_set\"] = keyword_set\n",
    "        return state\n",
    "\n",
    "    def run_literature_search(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Search for papers using the expanded keywords.\"\"\"\n",
    "        state[\"status_message\"] = \"Searching for papers...\"\n",
    "        papers = search_agent.process(state[\"keyword_set\"])\n",
    "        state[\"papers\"] = papers\n",
    "        return state\n",
    "\n",
    "    def run_paper_ranking(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Rank the papers based on multiple criteria.\"\"\"\n",
    "        state[\"status_message\"] = \"Ranking papers...\"\n",
    "        # Simple ranking based on recency\n",
    "        from datetime import datetime\n",
    "        current_year = datetime.now().year\n",
    "        \n",
    "        ranked_papers = []\n",
    "        for paper in state[\"papers\"]:\n",
    "            # Simplified ranking mechanism for the app\n",
    "            if paper.year is not None:\n",
    "                recency_score = max(0.1, min(1.0, 1.0 - 0.1 * (current_year - paper.year)))\n",
    "            else:\n",
    "                recency_score = 0.5\n",
    "                \n",
    "            relevance_score = 0.8  # Assume high relevance since we searched for these terms\n",
    "            citation_score = 0.6    # Placeholder without actual citation data\n",
    "            \n",
    "            # Calculate overall rank score\n",
    "            rank_score = 0.6 * relevance_score + 0.25 * citation_score + 0.15 * recency_score\n",
    "            \n",
    "            ranked_paper = RankedPaper(\n",
    "                **paper.dict(),\n",
    "                rank_score=rank_score,\n",
    "                relevance_score=relevance_score,\n",
    "                citation_score=citation_score,\n",
    "                recency_score=recency_score\n",
    "            )\n",
    "            ranked_papers.append(ranked_paper)\n",
    "        \n",
    "        # Sort papers by rank score in descending order\n",
    "        ranked_papers.sort(key=lambda p: p.rank_score, reverse=True)\n",
    "        state[\"ranked_papers\"] = ranked_papers\n",
    "        return state\n",
    "\n",
    "    def run_paper_summarization(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Generate summaries for the top-ranked papers.\"\"\"\n",
    "        state[\"status_message\"] = \"Summarizing papers...\"\n",
    "        # Select the top N papers\n",
    "        top_papers = state[\"ranked_papers\"][:state[\"top_n\"]]\n",
    "        \n",
    "        paper_summaries = []\n",
    "        for paper in top_papers:\n",
    "            # For the app, use Together API for summarization\n",
    "            try:\n",
    "                # Prepare the author string\n",
    "                authors_str = \", \".join([author.name for author in paper.authors])\n",
    "                \n",
    "                # Create the prompt\n",
    "                prompt = f\"\"\"Summarize the following academic paper with a focus on the main problem, methods, and key findings.\n",
    "                \n",
    "Title: {paper.title}\n",
    "Authors: {authors_str}\n",
    "Abstract: {paper.abstract}\n",
    "                \n",
    "Provide a concise but comprehensive summary of the paper that covers the research problem, methodology, and major contributions.\n",
    "Summary:\"\"\"\n",
    "                \n",
    "                # Call the Together.ai API\n",
    "                response = together.Complete.create(\n",
    "                    prompt=prompt,\n",
    "                    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "                    max_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    top_k=60,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "                \n",
    "                # Extract the summary\n",
    "                summary = response['output']['choices'][0]['text']\n",
    "                \n",
    "                # Process structured information\n",
    "                structure_prompt = f\"\"\"You are an expert academic researcher and summarizer. Extract and structure the information from the following paper summary into specific sections.\n",
    "\n",
    "Paper summary: {summary}\n",
    "\n",
    "Extract the following components from the summary:\n",
    "1. Methodology: What research methods or techniques were used?\n",
    "2. Key Contributions: What are the main findings or contributions of the paper?\n",
    "3. Limitations or Gaps: What limitations, open questions, or research gaps were identified?\n",
    "\n",
    "Respond with a JSON object in the following format:\n",
    "{{\"methodology\": \"description of methodology\",\n",
    "\"contributions\": \"key contributions and findings\",\n",
    "\"limitations\": \"limitations and research gaps\"}}\n",
    "\n",
    "Include only the JSON in your response, with no additional text.\"\"\"\n",
    "                \n",
    "                structure_response = together.Complete.create(\n",
    "                    prompt=structure_prompt,\n",
    "                    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "                    max_tokens=512,\n",
    "                    temperature=0.3,\n",
    "                    top_p=0.9,\n",
    "                    top_k=60,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "                \n",
    "                # Extract and parse the structure\n",
    "                structure_text = structure_response['output']['choices'][0]['text']\n",
    "                try:\n",
    "                    import json\n",
    "                    # Clean up the JSON text (remove markdown code markers if present)\n",
    "                    structure_text = structure_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                    structured_info = json.loads(structure_text)\n",
    "                except:\n",
    "                    structured_info = {\n",
    "                        \"methodology\": \"Not available\",\n",
    "                        \"contributions\": \"Not available\",\n",
    "                        \"limitations\": \"Not available\"\n",
    "                    }\n",
    "                \n",
    "                # Create PaperSummary\n",
    "                paper_summary = PaperSummary(\n",
    "                    paper_id=paper.id,\n",
    "                    title=paper.title,\n",
    "                    authors=[author.name for author in paper.authors],\n",
    "                    summary=summary,\n",
    "                    methodology=structured_info.get(\"methodology\", \"Not available\"),\n",
    "                    contributions=structured_info.get(\"contributions\", \"Not available\"),\n",
    "                    limitations=structured_info.get(\"limitations\", \"Not available\")\n",
    "                )\n",
    "                paper_summaries.append(paper_summary)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Fallback for errors\n",
    "                print(f\"Error summarizing paper: {e}\")\n",
    "                paper_summary = PaperSummary(\n",
    "                    paper_id=paper.id,\n",
    "                    title=paper.title,\n",
    "                    authors=[author.name for author in paper.authors],\n",
    "                    summary=f\"Summary generation failed: {str(e)}\",\n",
    "                    methodology=\"Not available due to error\",\n",
    "                    contributions=\"Not available due to error\",\n",
    "                    limitations=\"Not available due to error\"\n",
    "                )\n",
    "                paper_summaries.append(paper_summary)\n",
    "        \n",
    "        state[\"paper_summaries\"] = paper_summaries\n",
    "        return state\n",
    "\n",
    "    def run_comparative_analysis(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Perform comparative analysis on the paper summaries.\"\"\"\n",
    "        state[\"status_message\"] = \"Performing comparative analysis...\"\n",
    "        try:\n",
    "            # Format the paper summaries for the prompt\n",
    "            formatted_summaries = \"\"\n",
    "            for i, summary in enumerate(state[\"paper_summaries\"]):\n",
    "                formatted_summaries += f\"Paper {i+1}: {summary.title}\\n\"\n",
    "                formatted_summaries += f\"Authors: {', '.join(summary.authors)}\\n\"\n",
    "                formatted_summaries += f\"Summary: {summary.summary}\\n\"\n",
    "                formatted_summaries += f\"Methodology: {summary.methodology}\\n\"\n",
    "                formatted_summaries += f\"Key Contributions: {summary.contributions}\\n\"\n",
    "                formatted_summaries += f\"Limitations/Gaps: {summary.limitations}\\n\\n\"\n",
    "            \n",
    "            prompt = f\"\"\"You are an expert academic researcher analyzing multiple papers on a related topic. Perform a comparative analysis of the following paper summaries to identify common themes, contradictions, research gaps, and future directions.\n",
    "\n",
    "Paper Summaries:\n",
    "{formatted_summaries}\n",
    "\n",
    "Based on these summaries, provide a comprehensive comparative analysis in the following JSON format:\n",
    "{{\"common_findings\": [\n",
    "    \"Common finding 1\",\n",
    "    \"Common finding 2\",\n",
    "    ... (at least 3-5 items)\n",
    "],\n",
    "\"contradictions\": [\n",
    "    \"Contradiction 1\",\n",
    "    \"Contradiction 2\",\n",
    "    ... (identify any contradictions between papers)\n",
    "],\n",
    "\"research_gaps\": [\n",
    "    \"Research gap 1\",\n",
    "    \"Research gap 2\",\n",
    "    ... (identify areas that need further research, at least 3-4 items)\n",
    "],\n",
    "\"future_directions\": [\n",
    "    \"Future direction 1\",\n",
    "    \"Future direction 2\",\n",
    "    ... (suggest future research directions, at least 3-4 items)\n",
    "]}}\n",
    "\n",
    "Include only the JSON in your response, with no additional text.\"\"\"\n",
    "\n",
    "            # Get comparative analysis\n",
    "            analysis_response = together.Complete.create(\n",
    "                prompt=prompt,\n",
    "                model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "                max_tokens=1024,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                top_k=60,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "            \n",
    "            analysis_text = analysis_response['output']['choices'][0]['text']\n",
    "            \n",
    "            # Parse JSON response\n",
    "            import json\n",
    "            # Clean up the JSON text (remove markdown code markers if present)\n",
    "            analysis_text = analysis_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            analysis_data = json.loads(analysis_text)\n",
    "            \n",
    "            comparative_analysis = ComparativeAnalysis(\n",
    "                common_findings=analysis_data.get(\"common_findings\", [\"No common findings identified\"]),\n",
    "                contradictions=analysis_data.get(\"contradictions\", [\"No contradictions identified\"]),\n",
    "                research_gaps=analysis_data.get(\"research_gaps\", [\"No research gaps identified\"]),\n",
    "                future_directions=analysis_data.get(\"future_directions\", [\"No future directions identified\"])\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in comparative analysis: {e}\")\n",
    "            # Return a default ComparativeAnalysis object on error\n",
    "            comparative_analysis = ComparativeAnalysis(\n",
    "                common_findings=[\"Error generating common findings\"],\n",
    "                contradictions=[\"Error generating contradictions\"],\n",
    "                research_gaps=[\"Error generating research gaps\"],\n",
    "                future_directions=[\"Error generating future directions\"]\n",
    "            )\n",
    "        \n",
    "        state[\"comparative_analysis\"] = comparative_analysis\n",
    "        return state\n",
    "\n",
    "    def generate_research_report(state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Generate the final research report.\"\"\"\n",
    "        state[\"status_message\"] = \"Generating research report...\"\n",
    "        try:\n",
    "            # Generate topic summary\n",
    "            keywords_str = \", \".join([k.keyword for k in state[\"keyword_set\"].expanded_keywords])\n",
    "            \n",
    "            prompt = f\"\"\"You are an expert academic researcher writing a brief overview of a research topic.\n",
    "\n",
    "Research topic: {state[\"query\"]}\n",
    "\n",
    "Related keywords: {keywords_str}\n",
    "\n",
    "Write a concise (150-200 words) but comprehensive overview of this research topic that explains its significance, key aspects, and relevance in its field.\"\"\"\n",
    "            \n",
    "            summary_response = together.Complete.create(\n",
    "                prompt=prompt,\n",
    "                model=\"meta-llama/Llama-3-8B-Instruct\",\n",
    "                max_tokens=400,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "            topic_summary = summary_response['output']['choices'][0]['text']\n",
    "            \n",
    "            # Create the research report\n",
    "            research_report = ResearchReport(\n",
    "                topic=state[\"query\"],\n",
    "                topic_summary=topic_summary,\n",
    "                expanded_keywords=[k.keyword for k in state[\"keyword_set\"].expanded_keywords],\n",
    "                paper_summaries=state[\"paper_summaries\"],\n",
    "                comparative_analysis=state[\"comparative_analysis\"],\n",
    "                timestamp=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            \n",
    "            state[\"research_report\"] = research_report\n",
    "            state[\"status_message\"] = \"Research complete!\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating research report: {e}\")\n",
    "            state[\"status_message\"] = f\"Error generating report: {str(e)}\"\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    # Build the LangGraph workflow\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"expand_keywords\", run_keyword_expansion)\n",
    "    workflow.add_node(\"search_papers\", run_literature_search)\n",
    "    workflow.add_node(\"rank_papers\", run_paper_ranking)\n",
    "    workflow.add_node(\"summarize_papers\", run_paper_summarization)\n",
    "    workflow.add_node(\"compare_papers\", run_comparative_analysis)\n",
    "    workflow.add_node(\"generate_report\", generate_research_report)\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.set_entry_point(\"expand_keywords\")\n",
    "    workflow.add_edge(\"expand_keywords\", \"search_papers\")\n",
    "    workflow.add_edge(\"search_papers\", \"rank_papers\")\n",
    "    workflow.add_edge(\"rank_papers\", \"summarize_papers\")\n",
    "    workflow.add_edge(\"summarize_papers\", \"compare_papers\")\n",
    "    workflow.add_edge(\"compare_papers\", \"generate_report\")\n",
    "    workflow.add_edge(\"generate_report\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    compiled_graph = workflow.compile()\n",
    "    \n",
    "    return compiled_graph\n",
    "\n",
    "# Define a function to run the research workflow\n",
    "def run_research(query, top_n=3, graph=None, llm=None):\n",
    "    if graph is None or llm is None:\n",
    "        return None\n",
    "        \n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"top_n\": top_n,\n",
    "        \"status_message\": \"Starting research...\"\n",
    "    }\n",
    "    \n",
    "    # Invoke the graph\n",
    "    try:\n",
    "        final_state = graph.invoke(initial_state)\n",
    "        return final_state[\"research_report\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error running research workflow: {e}\")\n",
    "        return None\n",
    "\n",
    "# Export functions\n",
    "def export_report_to_markdown(research_report, output_file=\"research_report.md\"):\n",
    "    \"\"\"Export the research report to a markdown file.\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(f\"# Research Report: {research_report.topic}\\n\\n\")\n",
    "        f.write(f\"*Generated on: {research_report.timestamp}*\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Topic Summary\\n\\n\")\n",
    "        f.write(f\"{research_report.topic_summary}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Expanded Keywords\\n\\n\")\n",
    "        for keyword in research_report.expanded_keywords:\n",
    "            f.write(f\"- {keyword}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"## Paper Summaries\\n\\n\")\n",
    "        for i, summary in enumerate(research_report.paper_summaries):\n",
    "            f.write(f\"### Paper {i+1}: {summary.title}\\n\\n\")\n",
    "            f.write(f\"**Authors:** {', '.join(summary.authors)}\\n\\n\")\n",
    "            f.write(f\"**Summary:**\\n{summary.summary}\\n\\n\")\n",
    "            f.write(f\"**Methodology:**\\n{summary.methodology}\\n\\n\")\n",
    "            f.write(f\"**Key Contributions:**\\n{summary.contributions}\\n\\n\")\n",
    "            f.write(f\"**Limitations/Gaps:**\\n{summary.limitations}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Comparative Analysis\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Common Findings\\n\\n\")\n",
    "        for finding in research_report.comparative_analysis.common_findings:\n",
    "            f.write(f\"- {finding}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Contradictions\\n\\n\")\n",
    "        for contradiction in research_report.comparative_analysis.contradictions:\n",
    "            f.write(f\"- {contradiction}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Research Gaps\\n\\n\")\n",
    "        for gap in research_report.comparative_analysis.research_gaps:\n",
    "            f.write(f\"- {gap}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"### Future Directions\\n\\n\")\n",
    "        for direction in research_report.comparative_analysis.future_directions:\n",
    "            f.write(f\"- {direction}\\n\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def export_report_to_json(research_report, output_file=\"research_report.json\"):\n",
    "    \"\"\"Export the research report to a JSON file.\"\"\"\n",
    "    # Convert the Pydantic model to a dictionary\n",
    "    report_dict = research_report.dict()\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(report_dict, f, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Main Streamlit app\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Multi-Agent Research Assistant\",\n",
    "        page_icon=\"\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "\n",
    "    st.title(\" Multi-Agent Autonomous Research Assistant\")\n",
    "    \n",
    "    st.markdown(\"\"\"\n",
    "    This application uses a multi-agent system to automate the academic research process. \n",
    "    Enter a research topic, and the system will:\n",
    "    1. Expand your query into relevant keywords\n",
    "    2. Search for academic papers across multiple sources (arXiv and PubMed)\n",
    "    3. Rank the papers based on relevance, citations, and recency\n",
    "    4. Generate summaries of the top papers using a fine-tuned model\n",
    "    5. Perform comparative analysis to identify common themes and research gaps\n",
    "    6. Generate a comprehensive research report\n",
    "    \"\"\")\n",
    "    \n",
    "    # Initialize session state variables\n",
    "    if 'api_key_set' not in st.session_state:\n",
    "        st.session_state.api_key_set = False\n",
    "    if 'research_status' not in st.session_state:\n",
    "        st.session_state.research_status = \"\"\n",
    "    if 'report' not in st.session_state:\n",
    "        st.session_state.report = None\n",
    "    if 'research_graph' not in st.session_state:\n",
    "        st.session_state.research_graph = None\n",
    "    if 'llm' not in st.session_state:\n",
    "        st.session_state.llm = None\n",
    "    if 'model' not in st.session_state:\n",
    "        st.session_state.model = None\n",
    "    if 'tokenizer' not in st.session_state:\n",
    "        st.session_state.tokenizer = None\n",
    "    if 'progress' not in st.session_state:\n",
    "        st.session_state.progress = 0\n",
    "        \n",
    "    # Create a sidebar for API key input\n",
    "    with st.sidebar:\n",
    "        st.header(\"API Configuration\")\n",
    "        api_key = st.text_input(\"Enter Together.ai API Key\", type=\"password\")\n",
    "        openai_api_key = st.text_input(\"Enter OpenAI API Key (optional)\", type=\"password\")\n",
    "        \n",
    "        if st.button(\"Save API Keys\"):\n",
    "            if api_key:\n",
    "                os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
    "                together.api_key = api_key\n",
    "                st.session_state.api_key_set = True\n",
    "                st.success(\"Together.ai API key saved!\")\n",
    "                \n",
    "                # Load the LLM and prepare the graph\n",
    "                st.session_state.llm = get_llm()\n",
    "                \n",
    "                # Load the fine-tuned model\n",
    "                with st.spinner(\"Loading model...\"):\n",
    "                    st.session_state.model, st.session_state.tokenizer = load_fine_tuned_model()\n",
    "                \n",
    "                # Build the research graph\n",
    "                with st.spinner(\"Preparing research graph...\"):\n",
    "                    st.session_state.research_graph = build_research_graph(\n",
    "                        st.session_state.model,\n",
    "                        st.session_state.tokenizer,\n",
    "                        st.session_state.llm\n",
    "                    )\n",
    "            \n",
    "            if openai_api_key:\n",
    "                os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "                st.success(\"OpenAI API key saved!\")\n",
    "                \n",
    "                # Update LLM and graph if OpenAI API is preferred\n",
    "                st.session_state.llm = get_llm()\n",
    "                \n",
    "                # Rebuild the research graph with OpenAI\n",
    "                with st.spinner(\"Preparing research graph...\"):\n",
    "                    st.session_state.research_graph = build_research_graph(\n",
    "                        st.session_state.model,\n",
    "                        st.session_state.tokenizer,\n",
    "                        st.session_state.llm\n",
    "                    )\n",
    "        \n",
    "        st.divider()\n",
    "        st.header(\"About\")\n",
    "        st.markdown(\"\"\"\n",
    "        This research assistant uses a multi-agent system consisting of:\n",
    "        - **KeywordAgent**: Expands research queries\n",
    "        - **SearchAgent**: Searches academic repositories (arXiv, PubMed)\n",
    "        - **RankAgent**: Ranks papers by relevance\n",
    "        - **SummaryAgent**: Summarizes papers\n",
    "        - **CompareAgent**: Performs comparative analysis\n",
    "        \n",
    "        The agents are orchestrated using LangGraph.\n",
    "        \"\"\")\n",
    "    \n",
    "    # Main content area\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.header(\"Enter Research Topic\")\n",
    "        query = st.text_input(\"Research Topic\", placeholder=\"e.g., Large Language Models in healthcare\")\n",
    "        top_n = st.slider(\"Number of top papers to analyze\", min_value=1, max_value=5, value=3)\n",
    "        \n",
    "        if st.button(\"Start Research\", disabled=not st.session_state.api_key_set):\n",
    "            if not query:\n",
    "                st.error(\"Please enter a research topic.\")\n",
    "            elif st.session_state.research_graph is None:\n",
    "                st.error(\"Research graph is not ready. Please check API configuration.\")\n",
    "            else:\n",
    "                # Reset progress\n",
    "                st.session_state.progress = 0\n",
    "                st.session_state.research_status = \"Starting research workflow...\"\n",
    "                st.session_state.report = None\n",
    "                \n",
    "                # Create a placeholder for the progress bar\n",
    "                progress_placeholder = st.empty()\n",
    "                status_placeholder = st.empty()\n",
    "                \n",
    "                # Start the research in a background thread to keep UI responsive\n",
    "                import threading\n",
    "                \n",
    "                def update_progress():\n",
    "                    progress = 0\n",
    "                    stages = [\"Expanding keywords...\", \"Searching for papers...\", \n",
    "                              \"Ranking papers...\", \"Summarizing papers...\", \n",
    "                              \"Performing comparative analysis...\", \"Generating report...\"]\n",
    "                    while progress < len(stages) and st.session_state.report is None:\n",
    "                        progress_bar = progress_placeholder.progress(progress / len(stages))\n",
    "                        status_placeholder.info(stages[progress])\n",
    "                        st.session_state.progress = progress / len(stages)\n",
    "                        st.session_state.research_status = stages[progress]\n",
    "                        time.sleep(2)\n",
    "                        progress += 1\n",
    "                \n",
    "                def do_research():\n",
    "                    try:\n",
    "                        report = run_research(\n",
    "                            query=query, \n",
    "                            top_n=top_n,\n",
    "                            graph=st.session_state.research_graph,\n",
    "                            llm=st.session_state.llm\n",
    "                        )\n",
    "                        st.session_state.report = report\n",
    "                        st.session_state.progress = 1.0\n",
    "                        st.session_state.research_status = \"Research complete!\"\n",
    "                    except Exception as e:\n",
    "                        st.session_state.research_status = f\"Error: {str(e)}\"\n",
    "                        print(f\"Research error: {e}\")\n",
    "                \n",
    "                # Start progress thread\n",
    "                progress_thread = threading.Thread(target=update_progress)\n",
    "                progress_thread.daemon = True\n",
    "                progress_thread.start()\n",
    "                \n",
    "                # Start research thread\n",
    "                research_thread = threading.Thread(target=do_research)\n",
    "                research_thread.daemon = True\n",
    "                research_thread.start()\n",
    "                \n",
    "                # Wait for threads to complete (or timeout after 30 seconds)\n",
    "                time_waited = 0\n",
    "                while research_thread.is_alive() and time_waited < 180:\n",
    "                    time.sleep(1)\n",
    "                    time_waited += 1\n",
    "                    # Force a rerun every 5 seconds to update the UI\n",
    "                    if time_waited % 5 == 0:\n",
    "                        st.rerun()\n",
    "                \n",
    "                # Force a rerun to show results or error\n",
    "                st.rerun()\n",
    "    \n",
    "    with col2:\n",
    "        st.header(\"Research Status\")\n",
    "        status_placeholder = st.empty()\n",
    "        \n",
    "        if st.session_state.research_status:\n",
    "            status_placeholder.info(st.session_state.research_status)\n",
    "            # Show progress bar if ongoing\n",
    "            if 0 < st.session_state.progress < 1:\n",
    "                st.progress(st.session_state.progress)\n",
    "        else:\n",
    "            status_placeholder.info(\"Enter a research topic and click 'Start Research' to begin.\")\n",
    "    \n",
    "    # Display research results\n",
    "    if st.session_state.report is not None:\n",
    "        display_research_report(st.session_state.report)\n",
    "\n",
    "def display_research_report(report):\n",
    "    \"\"\"Display the research report in the Streamlit interface.\"\"\"\n",
    "    if isinstance(report, dict):\n",
    "        # If the report is already a dictionary (from session state)\n",
    "        report_dict = report\n",
    "    else:\n",
    "        # If the report is a Pydantic model\n",
    "        report_dict = {\n",
    "            \"topic\": report.topic,\n",
    "            \"topic_summary\": report.topic_summary,\n",
    "            \"expanded_keywords\": report.expanded_keywords,\n",
    "            \"paper_summaries\": [summary.dict() for summary in report.paper_summaries],\n",
    "            \"comparative_analysis\": report.comparative_analysis.dict(),\n",
    "            \"timestamp\": report.timestamp\n",
    "        }\n",
    "    \n",
    "    st.header(f\"Research Report: {report_dict['topic']}\")\n",
    "    st.caption(f\"Generated on: {report_dict['timestamp']}\")\n",
    "    \n",
    "    st.subheader(\"Topic Summary\")\n",
    "    st.write(report_dict['topic_summary'])\n",
    "    \n",
    "    st.subheader(\"Expanded Keywords\")\n",
    "    # Display keywords as a horizontal list of tags\n",
    "    keyword_html = ' '.join([f'<span style=\"background-color: #f0f2f6; padding: 5px 10px; margin-right: 10px; border-radius: 15px;\">{keyword}</span>' for keyword in report_dict['expanded_keywords']])\n",
    "    st.markdown(keyword_html, unsafe_allow_html=True)\n",
    "    \n",
    "    st.subheader(\"Paper Summaries\")\n",
    "    for i, summary in enumerate(report_dict['paper_summaries']):\n",
    "        with st.expander(f\"Paper {i+1}: {summary['title']}\", expanded=i==0):\n",
    "            st.markdown(f\"**Authors:** {', '.join(summary['authors'])}\")\n",
    "            st.markdown(f\"**Summary:**\")\n",
    "            st.write(summary['summary'])\n",
    "            \n",
    "            # Create three columns for methodology, contributions, and limitations\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.markdown(f\"**Methodology:**\")\n",
    "                st.write(summary['methodology'])\n",
    "            with col2:\n",
    "                st.markdown(f\"**Key Contributions:**\")\n",
    "                st.write(summary['contributions'])\n",
    "            with col3:\n",
    "                st.markdown(f\"**Limitations/Gaps:**\")\n",
    "                st.write(summary['limitations'])\n",
    "    \n",
    "    st.subheader(\"Comparative Analysis\")\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"Common Findings\", \"Contradictions\", \"Research Gaps\", \"Future Directions\"])\n",
    "    \n",
    "    with tab1:\n",
    "        for finding in report_dict['comparative_analysis']['common_findings']:\n",
    "            st.markdown(f\"- {finding}\")\n",
    "    \n",
    "    with tab2:\n",
    "        for contradiction in report_dict['comparative_analysis']['contradictions']:\n",
    "            st.markdown(f\"- {contradiction}\")\n",
    "    \n",
    "    with tab3:\n",
    "        for gap in report_dict['comparative_analysis']['research_gaps']:\n",
    "            st.markdown(f\"- {gap}\")\n",
    "    \n",
    "    with tab4:\n",
    "        for direction in report_dict['comparative_analysis']['future_directions']:\n",
    "            st.markdown(f\"- {direction}\")\n",
    "    \n",
    "    # Export options\n",
    "    st.subheader(\"Export Report\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    # Create the report object for export functions\n",
    "    if isinstance(report, dict):\n",
    "        from pydantic import parse_obj_as\n",
    "        report_obj = parse_obj_as(ResearchReport, report)\n",
    "    else:\n",
    "        report_obj = report\n",
    "    \n",
    "    with col1:\n",
    "        if st.button(\"Export as Markdown\"):\n",
    "            try:\n",
    "                output_file = export_report_to_markdown(report_obj)\n",
    "                with open(output_file, 'r') as f:\n",
    "                    md_content = f.read()\n",
    "                st.download_button(\n",
    "                    label=\"Download Markdown\",\n",
    "                    data=md_content,\n",
    "                    file_name=\"research_report.md\",\n",
    "                    mime=\"text/markdown\"\n",
    "                )\n",
    "                st.success(f\"Report exported as {output_file}\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error exporting to Markdown: {str(e)}\")\n",
    "    \n",
    "    with col2:\n",
    "        if st.button(\"Export as JSON\"):\n",
    "            try:\n",
    "                output_file = export_report_to_json(report_obj)\n",
    "                with open(output_file, 'r') as f:\n",
    "                    json_content = f.read()\n",
    "                st.download_button(\n",
    "                    label=\"Download JSON\",\n",
    "                    data=json_content,\n",
    "                    file_name=\"research_report.json\",\n",
    "                    mime=\"application/json\"\n",
    "                )\n",
    "                st.success(f\"Report exported as {output_file}\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error exporting to JSON: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Streamlit app, execute the following command in the terminal:\n",
    "```bash\n",
    "streamlit run app_research_assistant.py\n",
    "```\n",
    "\n",
    "## 7. Summary and Conclusion\n",
    "\n",
    "In this notebook, we have implemented a comprehensive Multi-Agent Autonomous Research Assistant that automates the academic research process. The system consists of five specialized agents:\n",
    "\n",
    "1. **KeywordAgent**: Expands user queries into comprehensive search terms\n",
    "2. **SearchAgent**: Retrieves relevant papers from academic repositories\n",
    "3. **RankAgent**: Scores and ranks papers based on multiple criteria\n",
    "4. **SummaryAgent**: Generates summaries of papers using a fine-tuned model\n",
    "5. **CompareAgent**: Performs comparative analysis to identify patterns and gaps\n",
    "\n",
    "These agents are orchestrated in a workflow that produces a comprehensive research report, making the literature review process more efficient and insightful. The system integrates with the fine-tuned summarization model from Part A to provide high-quality paper summaries.\n",
    "\n",
    "The Multi-Agent Research Assistant demonstrates the potential of using LangChain/LangGraph for complex orchestration of specialized agents to automate knowledge work. Future improvements could include supporting more data sources, improving the ranking algorithm, and enhancing the user interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
