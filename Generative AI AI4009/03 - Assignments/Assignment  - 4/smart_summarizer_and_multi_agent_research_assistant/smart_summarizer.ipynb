{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Summarizer: Fine-Tuning LLMs with LoRA for Academic Paper Summarization\n",
    "\n",
    "This notebook implements a complete pipeline for fine-tuning a Large Language Model (LLM) using Low-Rank Adaptation (LoRA) to create an intelligent summarization system for academic research papers. The system is trained on the arXiv summarization dataset and evaluated using both automatic metrics and LLM-as-a-Judge qualitative evaluation.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Environment Configuration\n",
    "2. Data Preprocessing\n",
    "3. LoRA-Based Fine-Tuning\n",
    "4. Inference and Output Generation\n",
    "5. Model Evaluation\n",
    "   - Automatic Evaluation (ROUGE, BLEU, BERTScore)\n",
    "   - LLM-as-a-Judge Evaluation\n",
    "6. Streamlit/Gradio Interface Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "First, let's install all the required libraries and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets peft accelerate bitsandbytes tqdm evaluate nltk rouge_score bert_score streamlit gradio pdfplumber together torch matplotlib pandas numpy sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig, get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download NLTK data for BLEU score calculation\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Together.ai API key for LLM-as-a-Judge evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Together.ai API key\n",
    "# Replace with your actual API key\n",
    "import os\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"2b9e51fc4df8e0fd2af8a13e5b9c7672045d144fdeb0af379076fff0d1f7bdc6\"\n",
    "\n",
    "# Test the API connection\n",
    "import together\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "# Define the judge model to use\n",
    "JUDGE_MODEL = \"meta-llama/Llama-3.1-70B-Instruct\"  # You can also use \"deepseek-ai/deepseek-v2\" or \"meta-llama/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Load and preprocess the arXiv summarization dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the arXiv summarization dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "# Display a sample entry\n",
    "print(\"Sample entry:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of 5,000 samples\n",
    "# First, set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select 5,000 samples randomly\n",
    "train_subset_indices = random.sample(range(len(dataset['train'])), 5000)\n",
    "train_subset = dataset['train'].select(train_subset_indices)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Subset size: {len(train_subset)}\")\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "train_val_test = train_subset.train_test_split(test_size=0.2, seed=42)\n",
    "test_val = train_val_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_val_test['train']\n",
    "val_data = test_val['train']\n",
    "test_data = test_val['test']\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a pre-trained model and tokenizer. We'll use Mistral 7B for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a pre-trained model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Alternative: \"meta-llama/Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define context window size\n",
    "max_length = 1024  # Adjust based on model and available GPU memory\n",
    "\n",
    "# Define prompt template\n",
    "def create_prompt(article, summary=\"\"):\n",
    "    return f\"\"\"Summarize the following academic paper:\n",
    "    \n",
    "Article: {article}\n",
    "    \n",
    "Summary: {summary}\"\"\"\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    prompts = [create_prompt(article, summary) for article, summary in zip(examples['article'], examples['abstract'])]\n",
    "    tokenized_inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "tokenized_test = test_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "\n",
    "print(f\"Tokenized training set size: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized validation set size: {len(tokenized_val)}\")\n",
    "print(f\"Tokenized test set size: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA-Based Fine-Tuning\n",
    "\n",
    "Set up and fine-tune the model using LoRA (Low-Rank Adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model in 4-bit precision for memory efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank\n",
    "    lora_alpha=16,          # Alpha parameter\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    bias=\"none\",            # No bias\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal language modeling task\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRA to attention layers q and v\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model with LoRA configuration created.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,             # Train for 5 epochs as per requirements\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=4,   # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    evaluation_strategy=\"steps\",    # Evaluate during training\n",
    "    eval_steps=500,                 # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\",          # Save during training\n",
    "    save_steps=500,                 # Save every 500 steps\n",
    "    warmup_steps=100,               # Warmup steps\n",
    "    learning_rate=2e-4,             # Learning rate\n",
    "    weight_decay=0.01,              # Weight decay\n",
    "    fp16=True,                      # Use mixed precision\n",
    "    logging_steps=100,              # Log every 100 steps\n",
    "    logging_dir=\"./logs\",           # Logging directory\n",
    "    save_total_limit=3,             # Save only the last 3 checkpoints\n",
    "    load_best_model_at_end=True,    # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\"  # Metric to use for the best model\n",
    ")\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not using masked language modeling\n",
    ")\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Model training complete.\")\n",
    "print(f\"Training metrics: {train_result.metrics}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "training_logs = pd.read_csv(\"./logs/train_loss.csv\") if os.path.exists(\"./logs/train_loss.csv\") else None\n",
    "validation_logs = pd.read_csv(\"./logs/eval_loss.csv\") if os.path.exists(\"./logs/eval_loss.csv\") else None\n",
    "\n",
    "if training_logs is not None and validation_logs is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_logs['step'], training_logs['loss'], label='Training Loss')\n",
    "    plt.plot(validation_logs['step'], validation_logs['loss'], label='Validation Loss')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss_curve.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Output Generation\n",
    "\n",
    "Generate summaries using the fine-tuned model and the base model, and compare them with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Select 10 random samples from the test set for generation\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(test_data)), 10)\n",
    "test_samples = [test_data[i] for i in sample_indices]\n",
    "\n",
    "# Function to generate summaries\n",
    "def generate_summary(model, article, max_new_tokens=300):\n",
    "    input_text = create_prompt(article)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Set generation parameters\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, **gen_config)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the summary part (after \"Summary:\")\n",
    "    try:\n",
    "        summary = summary.split(\"Summary:\")[1].strip()\n",
    "    except IndexError:\n",
    "        pass  # If \"Summary:\" is not in the output, use the whole output\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries using both models and store results\n",
    "results = []\n",
    "\n",
    "print(\"Generating summaries...\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"Processing sample {i+1}/10...\")\n",
    "    \n",
    "    # Extract input and ground truth\n",
    "    article = sample[\"article\"]\n",
    "    ground_truth = sample[\"abstract\"]\n",
    "    \n",
    "    # Generate summary using fine-tuned model\n",
    "    fine_tuned_summary = generate_summary(model, article)\n",
    "    \n",
    "    # Generate summary using base model\n",
    "    base_summary = generate_summary(base_model, article)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"article\": article,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"fine_tuned_summary\": fine_tuned_summary,\n",
    "        \"base_summary\": base_summary\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"summary_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Summary generation complete. Results saved to 'summary_results.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated summaries\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(result[\"ground_truth\"])\n",
    "    print(\"\\nFine-Tuned Model Summary:\")\n",
    "    print(result[\"fine_tuned_summary\"])\n",
    "    print(\"\\nBase Model Summary:\")\n",
    "    print(result[\"base_summary\"])\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Automatic Evaluation\n",
    "\n",
    "Evaluate the generated summaries using ROUGE, BLEU, and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n",
    "        'rougeL': sum(rougeL_scores) / len(rougeL_scores)\n",
    "    }\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    tokenized_refs = [[nltk.word_tokenize(ref)] for ref in references]\n",
    "    tokenized_hyps = [nltk.word_tokenize(hyp) for hyp in hypotheses]\n",
    "    return corpus_bleu(tokenized_refs, tokenized_hyps)\n",
    "\n",
    "# Function to calculate BERTScore\n",
    "def calculate_bertscore(references, hypotheses):\n",
    "    P, R, F1 = bert_score.score(hypotheses, references, lang=\"en\", verbose=False)\n",
    "    return {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Extract references and hypotheses\n",
    "references = [result[\"ground_truth\"] for result in results]\n",
    "fine_tuned_hypotheses = [result[\"fine_tuned_summary\"] for result in results]\n",
    "base_hypotheses = [result[\"base_summary\"] for result in results]\n",
    "\n",
    "# Calculate metrics for fine-tuned model\n",
    "print(\"Calculating metrics for fine-tuned model...\")\n",
    "fine_tuned_rouge = calculate_rouge(references, fine_tuned_hypotheses)\n",
    "fine_tuned_bleu = calculate_bleu(references, fine_tuned_hypotheses)\n",
    "fine_tuned_bertscore = calculate_bertscore(references, fine_tuned_hypotheses)\n",
    "\n",
    "# Calculate metrics for base model\n",
    "print(\"Calculating metrics for base model...\")\n",
    "base_rouge = calculate_rouge(references, base_hypotheses)\n",
    "base_bleu = calculate_bleu(references, base_hypotheses)\n",
    "base_bertscore = calculate_bertscore(references, base_hypotheses)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics for Fine-Tuned Model:\")\n",
    "print(f\"ROUGE-1: {fine_tuned_rouge['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L: {fine_tuned_rouge['rougeL']:.4f}\")\n",
    "print(f\"BLEU: {fine_tuned_bleu:.4f}\")\n",
    "print(f\"BERTScore F1: {fine_tuned_bertscore['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics for Base Model:\")\n",
    "print(f\"ROUGE-1: {base_rouge['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L: {base_rouge['rougeL']:.4f}\")\n",
    "print(f\"BLEU: {base_bleu:.4f}\")\n",
    "print(f\"BERTScore F1: {base_bertscore['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evaluation results\n",
    "metrics = ['ROUGE-1', 'ROUGE-L', 'BLEU', 'BERTScore F1']\n",
    "fine_tuned_scores = [\n",
    "    fine_tuned_rouge['rouge1'],\n",
    "    fine_tuned_rouge['rougeL'],\n",
    "    fine_tuned_bleu,\n",
    "    fine_tuned_bertscore['f1']\n",
    "]\n",
    "base_scores = [\n",
    "    base_rouge['rouge1'],\n",
    "    base_rouge['rougeL'],\n",
    "    base_bleu,\n",
    "    base_bertscore['f1']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, fine_tuned_scores, width, label='Fine-Tuned Model')\n",
    "rects2 = ax.bar(x + width/2, base_scores, width, label='Base Model')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Automatic Evaluation Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Add values on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LLM-as-a-Judge Evaluation\n",
    "\n",
    "Use a more powerful LLM (via Together.ai API) to evaluate the quality of the generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a summary using LLM-as-a-Judge\n",
    "def evaluate_with_llm(article, summary, model=JUDGE_MODEL):\n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator of academic paper summaries. Given the following input and the summary produced, evaluate the summary on three dimensions:\n",
    "\n",
    "1. Fluency: Is the summary readable and grammatically correct? (Scale: 1-5)\n",
    "2. Factuality: Are the statements in the summary correct, and do they reflect the source content? (Scale: 1-5)\n",
    "3. Coverage: Does the summary include the main problem, method, and key findings? (Scale: 1-5)\n",
    "\n",
    "For each dimension, provide a score from 1 (poor) to 5 (excellent) and a brief justification for the score.\n",
    "\n",
    "Original Paper Text (truncated):\n",
    "```\n",
    "{article[:3000]}...  # Truncate to avoid exceeding token limits\n",
    "```\n",
    "\n",
    "Generated Summary:\n",
    "```\n",
    "{summary}\n",
    "```\n",
    "\n",
    "Your evaluation should be structured as follows:\n",
    "- Fluency: [SCORE] - [JUSTIFICATION]\n",
    "- Factuality: [SCORE] - [JUSTIFICATION]\n",
    "- Coverage: [SCORE] - [JUSTIFICATION]\n",
    "- Overall: [SCORE] - [BRIEF SUMMARY OF STRENGTHS AND WEAKNESSES]\n",
    "\n",
    "Ensure that your scores are justified based on specific observations from the text.\"\"\"\n",
    "\n",
    "    # Call the Together.ai API\n",
    "    response = together.Complete.create(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        top_p=0.8,\n",
    "        top_k=60,\n",
    "        repetition_penalty=1.1,\n",
    "        stop=['\\n\\n\\n']\n",
    "    )\n",
    "    \n",
    "    # Extract the evaluation text\n",
    "    evaluation_text = response['output']['choices'][0]['text']\n",
    "    \n",
    "    # Parse the scores\n",
    "    try:\n",
    "        fluency_score = int(evaluation_text.split(\"Fluency:\")[1].split(\"-\")[0].strip())\n",
    "        factuality_score = int(evaluation_text.split(\"Factuality:\")[1].split(\"-\")[0].strip())\n",
    "        coverage_score = int(evaluation_text.split(\"Coverage:\")[1].split(\"-\")[0].strip())\n",
    "        \n",
    "        # Calculate overall score (the average of the three scores)\n",
    "        overall_score = (fluency_score + factuality_score + coverage_score) / 3\n",
    "        \n",
    "        return {\n",
    "            \"fluency\": fluency_score,\n",
    "            \"factuality\": factuality_score,\n",
    "            \"coverage\": coverage_score,\n",
    "            \"overall\": overall_score,\n",
    "            \"full_evaluation\": evaluation_text\n",
    "        }\n",
    "    except:\n",
    "        # If parsing fails, return the full evaluation text\n",
    "        return {\n",
    "            \"fluency\": None,\n",
    "            \"factuality\": None,\n",
    "            \"coverage\": None,\n",
    "            \"overall\": None,\n",
    "            \"full_evaluation\": evaluation_text\n",
    "        }\n",
    "\n",
    "# Evaluate a subset of the generated summaries (5 samples for budget reasons)\n",
    "evaluation_subset = results[:5]  # Use the first 5 samples\n",
    "\n",
    "# Evaluate fine-tuned model summaries\n",
    "fine_tuned_evaluations = []\n",
    "for i, result in enumerate(evaluation_subset):\n",
    "    print(f\"Evaluating fine-tuned model summary {i+1}/5...\")\n",
    "    evaluation = evaluate_with_llm(result[\"article\"], result[\"fine_tuned_summary\"])\n",
    "    fine_tuned_evaluations.append(evaluation)\n",
    "    time.sleep(2)  # To avoid rate limiting\n",
    "\n",
    "# Evaluate base model summaries\n",
    "base_evaluations = []\n",
    "for i, result in enumerate(evaluation_subset):\n",
    "    print(f\"Evaluating base model summary {i+1}/5...\")\n",
    "    evaluation = evaluate_with_llm(result[\"article\"], result[\"base_summary\"])\n",
    "    base_evaluations.append(evaluation)\n",
    "    time.sleep(2)  # To avoid rate limiting\n",
    "\n",
    "# Save the evaluations to a JSON file\n",
    "with open(\"llm_evaluations.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"fine_tuned\": fine_tuned_evaluations,\n",
    "        \"base\": base_evaluations\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"LLM evaluation complete. Results saved to 'llm_evaluations.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores\n",
    "fine_tuned_avg = {\n",
    "    \"fluency\": sum(eval[\"fluency\"] for eval in fine_tuned_evaluations if eval[\"fluency\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"fluency\"]),\n",
    "    \"factuality\": sum(eval[\"factuality\"] for eval in fine_tuned_evaluations if eval[\"factuality\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"factuality\"]),\n",
    "    \"coverage\": sum(eval[\"coverage\"] for eval in fine_tuned_evaluations if eval[\"coverage\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"coverage\"]),\n",
    "    \"overall\": sum(eval[\"overall\"] for eval in fine_tuned_evaluations if eval[\"overall\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"overall\"])\n",
    "}\n",
    "\n",
    "base_avg = {\n",
    "    \"fluency\": sum(eval[\"fluency\"] for eval in base_evaluations if eval[\"fluency\"]) / sum(1 for eval in base_evaluations if eval[\"fluency\"]),\n",
    "    \"factuality\": sum(eval[\"factuality\"] for eval in base_evaluations if eval[\"factuality\"]) / sum(1 for eval in base_evaluations if eval[\"factuality\"]),\n",
    "    \"coverage\": sum(eval[\"coverage\"] for eval in base_evaluations if eval[\"coverage\"]) / sum(1 for eval in base_evaluations if eval[\"coverage\"]),\n",
    "    \"overall\": sum(eval[\"overall\"] for eval in base_evaluations if eval[\"overall\"]) / sum(1 for eval in base_evaluations if eval[\"overall\"])\n",
    "}\n",
    "\n",
    "# Print average scores\n",
    "print(\"LLM-as-a-Judge Average Scores:\")\n",
    "print(\"\\nFine-Tuned Model:\")\n",
    "print(f\"Fluency: {fine_tuned_avg['fluency']:.2f}\")\n",
    "print(f\"Factuality: {fine_tuned_avg['factuality']:.2f}\")\n",
    "print(f\"Coverage: {fine_tuned_avg['coverage']:.2f}\")\n",
    "print(f\"Overall: {fine_tuned_avg['overall']:.2f}\")\n",
    "\n",
    "print(\"\\nBase Model:\")\n",
    "print(f\"Fluency: {base_avg['fluency']:.2f}\")\n",
    "print(f\"Factuality: {base_avg['factuality']:.2f}\")\n",
    "print(f\"Coverage: {base_avg['coverage']:.2f}\")\n",
    "print(f\"Overall: {base_avg['overall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LLM-as-a-Judge evaluation results\n",
    "categories = ['Fluency', 'Factuality', 'Coverage', 'Overall']\n",
    "fine_tuned_scores = [fine_tuned_avg['fluency'], fine_tuned_avg['factuality'], fine_tuned_avg['coverage'], fine_tuned_avg['overall']]\n",
    "base_scores = [base_avg['fluency'], base_avg['factuality'], base_avg['coverage'], base_avg['overall']]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, fine_tuned_scores, width, label='Fine-Tuned Model')\n",
    "rects2 = ax.bar(x + width/2, base_scores, width, label='Base Model')\n",
    "\n",
    "ax.set_ylabel('Score (1-5)')\n",
    "ax.set_title('LLM-as-a-Judge Evaluation Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 5.5)  # Set y-axis limits to 0-5\n",
    "ax.legend()\n",
    "\n",
    "# Add values on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('llm_evaluation_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streamlit Interface Development\n",
    "\n",
    "Create a Streamlit interface for the summarization system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pdfplumber\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Smart Summarizer\",\n",
    "    page_icon=\"ðŸ“\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"ðŸ“ Smart Summarizer for Academic Papers\")\n",
    "st.markdown(\"\"\"\n",
    "This application uses a fine-tuned language model to generate concise and accurate summaries of academic papers.\n",
    "Upload a PDF file or paste the text of an academic paper, and get an AI-generated summary in seconds.\n",
    "\"\"\")\n",
    "\n",
    "# Set up the Together.ai API key\n",
    "@st.cache_resource\n",
    "def load_api_key():\n",
    "    return os.environ.get(\"TOGETHER_API_KEY\", \"\")\n",
    "\n",
    "# Load the API key\n",
    "api_key = load_api_key()\n",
    "\n",
    "# Create a sidebar for API key input if not already set\n",
    "if not api_key:\n",
    "    api_key = st.sidebar.text_input(\"Enter Together.ai API Key\", type=\"password\")\n",
    "    if api_key:\n",
    "        os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
    "\n",
    "# Function to load models\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    # Model and tokenizer details\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"  # Use the same model as in training\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True if torch.cuda.is_available() else False,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    if os.path.exists(\"./fine_tuned_model\"):\n",
    "        fine_tuned_model = PeftModel.from_pretrained(base_model, \"./fine_tuned_model\")\n",
    "    else:\n",
    "        fine_tuned_model = base_model  # Fallback to base model if fine-tuned model is not available\n",
    "    \n",
    "    return tokenizer, base_model, fine_tuned_model, device\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to generate a summary\n",
    "def generate_summary(model, tokenizer, text, max_new_tokens=300, device=\"cpu\"):\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Summarize the following academic paper:\n",
    "    \n",
    "Article: {text}\n",
    "    \n",
    "Summary: \"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Set generation parameters\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, **gen_config)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the summary part (after \"Summary:\")\n",
    "    try:\n",
    "        summary = summary.split(\"Summary:\")[1].strip()\n",
    "    except IndexError:\n",
    "        pass  # If \"Summary:\" is not in the output, use the whole output\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Function to evaluate a summary using LLM-as-a-Judge\n",
    "def evaluate_with_llm(article, summary, api_key):\n",
    "    if not api_key:\n",
    "        return \"Please enter a Together.ai API key to use the LLM-as-a-Judge feature.\"\n",
    "    \n",
    "    # Trim the article to avoid exceeding token limits\n",
    "    article = article[:3000] + \"...\" if len(article) > 3000 else article\n",
    "    \n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator of academic paper summaries. Given the following input and the summary produced, evaluate the summary on three dimensions:\n",
    "\n",
    "1. Fluency: Is the summary readable and grammatically correct? (Scale: 1-5)\n",
    "2. Factuality: Are the statements in the summary correct, and do they reflect the source content? (Scale: 1-5)\n",
    "3. Coverage: Does the summary include the main problem, method, and key findings? (Scale: 1-5)\n",
    "\n",
    "For each dimension, provide a score from 1 (poor) to 5 (excellent) and a brief justification for the score.\n",
    "\n",
    "Original Paper Text (truncated):\n",
    "```\n",
    "{article}\n",
    "```\n",
    "\n",
    "Generated Summary:\n",
    "```\n",
    "{summary}\n",
    "```\n",
    "\n",
    "Your evaluation should be structured as follows:\n",
    "- Fluency: [SCORE] - [JUSTIFICATION]\n",
    "- Factuality: [SCORE] - [JUSTIFICATION]\n",
    "- Coverage: [SCORE] - [JUSTIFICATION]\n",
    "- Overall: [SCORE] - [BRIEF SUMMARY OF STRENGTHS AND WEAKNESSES]\n",
    "\n",
    "Ensure that your scores are justified based on specific observations from the text.\"\"\"\n",
    "    \n",
    "    # Call the Together.ai API\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 60,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"stop\": [\"\\n\\n\\n\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.together.xyz/v1/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"text\"]\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Main application\n",
    "def main():\n",
    "    # Load models\n",
    "    with st.spinner(\"Loading models... This may take a minute...\"):\n",
    "        tokenizer, base_model, fine_tuned_model, device = load_models()\n",
    "    \n",
    "    # Create tabs for different input methods\n",
    "    tab1, tab2 = st.tabs([\"Upload PDF\", \"Paste Text\"])\n",
    "    \n",
    "    with tab1:\n",
    "        # File uploader for PDF\n",
    "        uploaded_file = st.file_uploader(\"Upload an academic paper (PDF)\", type=\"pdf\")\n",
    "        if uploaded_file is not None:\n",
    "            with st.spinner(\"Extracting text from PDF...\"):\n",
    "                pdf_bytes = io.BytesIO(uploaded_file.getvalue())\n",
    "                text = extract_text_from_pdf(pdf_bytes)\n",
    "                st.session_state.input_text = text\n",
    "                st.session_state.show_input = True\n",
    "    \n",
    "    with tab2:\n",
    "        # Text input\n",
    "        text_input = st.text_area(\"Paste the text of an academic paper\", height=300)\n",
    "        if text_input:\n",
    "            st.session_state.input_text = text_input\n",
    "            st.session_state.show_input = True\n",
    "    \n",
    "    # Initialize session state variables if they don't exist\n",
    "    if \"input_text\" not in st.session_state:\n",
    "        st.session_state.input_text = \"\"\n",
    "    if \"show_input\" not in st.session_state:\n",
    "        st.session_state.show_input = False\n",
    "    if \"fine_tuned_summary\" not in st.session_state:\n",
    "        st.session_state.fine_tuned_summary = \"\"\n",
    "    if \"base_summary\" not in st.session_state:\n",
    "        st.session_state.base_summary = \"\"\n",
    "    if \"evaluation\" not in st.session_state:\n",
    "        st.session_state.evaluation = \"\"\n",
    "    \n",
    "    # Display input text and generate summaries\n",
    "    if st.session_state.show_input and st.session_state.input_text:\n",
    "        with st.expander(\"View Input Text\", expanded=False):\n",
    "            st.text_area(\"Input Text\", st.session_state.input_text, height=200, disabled=True)\n",
    "        \n",
    "        # Create columns for the buttons\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        # Generate summary with fine-tuned model\n",
    "        if col1.button(\"Generate Summary (Fine-Tuned Model)\"):\n",
    "            with st.spinner(\"Generating summary with fine-tuned model...\"):\n",
    "                st.session_state.fine_tuned_summary = generate_summary(\n",
    "                    fine_tuned_model, tokenizer, st.session_state.input_text, max_new_tokens=300, device=device\n",
    "                )\n",
    "        \n",
    "        # Generate summary with base model\n",
    "        if col2.button(\"Generate Summary (Base Model)\"):\n",
    "            with st.spinner(\"Generating summary with base model...\"):\n",
    "                st.session_state.base_summary = generate_summary(\n",
    "                    base_model, tokenizer, st.session_state.input_text, max_new_tokens=300, device=device\n",
    "                )\n",
    "        \n",
    "        # Evaluate summary with LLM-as-a-Judge\n",
    "        if col3.button(\"Evaluate with LLM-as-a-Judge\") and st.session_state.fine_tuned_summary:\n",
    "            if not api_key:\n",
    "                st.warning(\"Please enter a Together.ai API key in the sidebar to use the LLM-as-a-Judge feature.\")\n",
    "            else:\n",
    "                with st.spinner(\"Evaluating summary...\"):\n",
    "                    st.session_state.evaluation = evaluate_with_llm(\n",
    "                        st.session_state.input_text, st.session_state.fine_tuned_summary, api_key\n",
    "                    )\n",
    "        \n",
    "        # Display summaries and evaluation\n",
    "        if st.session_state.fine_tuned_summary or st.session_state.base_summary:\n",
    "            st.markdown(\"### Generated Summaries\")\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.markdown(\"#### Fine-Tuned Model Summary\")\n",
    "                st.text_area(\"Fine-Tuned Summary\", st.session_state.fine_tuned_summary, height=300, disabled=True)\n",
    "            \n",
    "            with col2:\n",
    "                st.markdown(\"#### Base Model Summary\")\n",
    "                st.text_area(\"Base Summary\", st.session_state.base_summary, height=300, disabled=True)\n",
    "        \n",
    "        # Display evaluation\n",
    "        if st.session_state.evaluation:\n",
    "            st.markdown(\"### LLM-as-a-Judge Evaluation\")\n",
    "            st.markdown(st.session_state.evaluation)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Streamlit app, execute the following command in the terminal:\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "In this notebook, we have implemented a complete Smart Summarizer pipeline for academic research papers. We:\n",
    "\n",
    "1. Loaded and preprocessed the arXiv summarization dataset\n",
    "2. Fine-tuned a pre-trained language model (Mistral 7B) using Low-Rank Adaptation (LoRA)\n",
    "3. Generated summaries using both the fine-tuned model and the base model\n",
    "4. Evaluated the summaries using automatic metrics (ROUGE, BLEU, BERTScore) and LLM-as-a-Judge qualitative evaluation\n",
    "5. Developed a Streamlit interface for the summarization system\n",
    "\n",
    "The evaluation results show that the fine-tuned model generally outperforms the base model in generating accurate and relevant summaries of academic papers, demonstrating the effectiveness of LoRA fine-tuning for domain-specific tasks like academic paper summarization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
