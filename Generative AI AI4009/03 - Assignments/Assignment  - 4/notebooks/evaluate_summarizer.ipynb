{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e844f751",
   "metadata": {},
   "source": [
    "# Smart Summarizer: Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the LoRA fine-tuned summarization model, comparing it with the base model using both automatic metrics and LLM-as-a-Judge evaluation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load the base and fine-tuned models\n",
    "2. Generate summaries for test samples\n",
    "3. Evaluate using automatic metrics (ROUGE, BLEU, BERTScore)\n",
    "4. Evaluate using LLM-as-a-Judge\n",
    "5. Visualize and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install -q transformers peft datasets evaluate rouge-score nltk bert-score together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Add parent directory to path for importing custom modules\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our evaluation module\n",
    "from smart_summarizer.evaluation.evaluation import SummaryEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e50d8",
   "metadata": {},
   "source": [
    "## 1. Load Models and Test Dataset\n",
    "\n",
    "We'll load both the base model and the fine-tuned model, along with a subset of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"meta-llama/Llama-3-8B\")\n",
    "LORA_MODEL_DIR = os.getenv(\"LORA_MODEL_PATH\", \"../smart_summarizer/models/lora_summarizer/final_model\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"LoRA model directory: {LORA_MODEL_DIR}\")\n",
    "print(f\"Together API key available: {'Yes' if TOGETHER_API_KEY else 'No'}\")\n",
    "\n",
    "# Check if LoRA model exists\n",
    "if not os.path.exists(LORA_MODEL_DIR):\n",
    "    print(f\"Warning: LoRA model not found at {LORA_MODEL_DIR}\")\n",
    "    print(\"Please run the training notebook first or update the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "# For this evaluation, we'll use a small subset of the arXiv dataset\n",
    "from smart_summarizer.data.data_preprocessing import load_arxiv_dataset\n",
    "\n",
    "\n",
    "# Load dataset and select a subset for testing\n",
    "print(\"Loading test dataset...\")\n",
    "dataset = load_arxiv_dataset()\n",
    "test_dataset = dataset['test'].select(range(20))  # Select first 20 test samples\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = SummaryEvaluator(\n",
    "    base_model_name=BASE_MODEL,\n",
    "    lora_model_dir=LORA_MODEL_DIR,\n",
    "    together_api_key=TOGETHER_API_KEY\n",
    ")\n",
    "\n",
    "# Load models and tokenizer\n",
    "print(\"Loading models...\")\n",
    "evaluator.load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b274d",
   "metadata": {},
   "source": [
    "## 2. Generate Summaries\n",
    "\n",
    "Generate summaries using both the base model and the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a smaller sample for detailed evaluation\n",
    "evaluation_samples = test_dataset.select(range(10))  # Select first 10 samples\n",
    "\n",
    "# Generate summaries\n",
    "print(\"Generating summaries...\")\n",
    "summaries = []\n",
    "\n",
    "for i, sample in enumerate(tqdm(evaluation_samples)):\n",
    "    article = sample[\"article\"]\n",
    "    ground_truth = sample[\"abstract\"]\n",
    "    \n",
    "    # Generate summaries with both models\n",
    "    base_summary = evaluator.generate_summary(article, use_base_model=True)\n",
    "    fine_tuned_summary = evaluator.generate_summary(article, use_base_model=False)\n",
    "    \n",
    "    summaries.append({\n",
    "        \"id\": i,\n",
    "        \"title\": sample[\"title\"],\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"base_summary\": base_summary,\n",
    "        \"fine_tuned_summary\": fine_tuned_summary\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "summaries_df = pd.DataFrame(summaries)\n",
    "summaries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bcf0c",
   "metadata": {},
   "source": [
    "## 3. Automatic Metrics Evaluation\n",
    "\n",
    "Evaluate the generated summaries using ROUGE, BLEU, and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ef3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of summaries and reference abstracts\n",
    "ground_truths = summaries_df[\"ground_truth\"].tolist()\n",
    "base_summaries = summaries_df[\"base_summary\"].tolist()\n",
    "fine_tuned_summaries = summaries_df[\"fine_tuned_summary\"].tolist()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "base_rouge = evaluator.compute_rouge(base_summaries, ground_truths)\n",
    "fine_tuned_rouge = evaluator.compute_rouge(fine_tuned_summaries, ground_truths)\n",
    "\n",
    "# Calculate BLEU scores\n",
    "print(\"Calculating BLEU scores...\")\n",
    "base_bleu = evaluator.compute_bleu(base_summaries, ground_truths)\n",
    "fine_tuned_bleu = evaluator.compute_bleu(fine_tuned_summaries, ground_truths)\n",
    "\n",
    "# Calculate BERTScore\n",
    "try:\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    base_bert = evaluator.compute_bertscore(base_summaries, ground_truths)\n",
    "    fine_tuned_bert = evaluator.compute_bertscore(fine_tuned_summaries, ground_truths)\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating BERTScore: {str(e)}\")\n",
    "    # Fallback values if BERTScore fails\n",
    "    base_bert = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "    fine_tuned_bert = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "\n",
    "# Compile results\n",
    "metrics_results = {\n",
    "    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\", \"BERTScore-P\", \"BERTScore-R\", \"BERTScore-F1\"],\n",
    "    \"Base Model\": [\n",
    "        base_rouge[\"rouge1\"],\n",
    "        base_rouge[\"rouge2\"],\n",
    "        base_rouge[\"rougeL\"],\n",
    "        base_bleu,\n",
    "        base_bert[\"precision\"],\n",
    "        base_bert[\"recall\"],\n",
    "        base_bert[\"f1\"]\n",
    "    ],\n",
    "    \"Fine-tuned Model\": [\n",
    "        fine_tuned_rouge[\"rouge1\"],\n",
    "        fine_tuned_rouge[\"rouge2\"],\n",
    "        fine_tuned_rouge[\"rougeL\"],\n",
    "        fine_tuned_bleu,\n",
    "        fine_tuned_bert[\"precision\"],\n",
    "        fine_tuned_bert[\"recall\"],\n",
    "        fine_tuned_bert[\"f1\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentage\n",
    "metrics_df[\"Improvement (%)\"] = (\n",
    "    (metrics_df[\"Fine-tuned Model\"] - metrics_df[\"Base Model\"]) / metrics_df[\"Base Model\"] * 100\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot automatic metrics\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, metrics_df[\"Base Model\"], width, label='Base Model')\n",
    "plt.bar(x + width/2, metrics_df[\"Fine-tuned Model\"], width, label='Fine-tuned Model')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of Automatic Evaluation Metrics')\n",
    "plt.xticks(x, metrics_df[\"Metric\"])\n",
    "plt.ylim(0, max(metrics_df[\"Fine-tuned Model\"].max(), metrics_df[\"Base Model\"].max()) * 1.1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v1, v2 in zip(x, metrics_df[\"Base Model\"], metrics_df[\"Fine-tuned Model\"]):\n",
    "    plt.text(i - width/2, v1 + 0.01, f'{v1:.3f}', ha='center')\n",
    "    plt.text(i + width/2, v2 + 0.01, f'{v2:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../smart_summarizer/evaluation/metrics_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8414a13",
   "metadata": {},
   "source": [
    "## 4. LLM-as-a-Judge Evaluation\n",
    "\n",
    "Use an LLM (via Together.ai API) to evaluate the summaries on fluency, factuality, and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have API key for LLM-as-a-Judge\n",
    "if not TOGETHER_API_KEY:\n",
    "    print(\"No Together API key found. Skipping LLM-as-a-Judge evaluation.\")\n",
    "else:\n",
    "    print(\"Running LLM-as-a-Judge evaluation...\")\n",
    "    \n",
    "    # Evaluate a subset of summaries to manage API costs\n",
    "    judge_samples = 5  # Number of samples to evaluate\n",
    "    \n",
    "    llm_evals = []\n",
    "    \n",
    "    for i in tqdm(range(min(judge_samples, len(summaries_df)))):\n",
    "        article = evaluation_samples[i][\"article\"]\n",
    "        ground_truth = evaluation_samples[i][\"abstract\"]\n",
    "        base_summary = summaries_df.loc[i, \"base_summary\"]\n",
    "        fine_tuned_summary = summaries_df.loc[i, \"fine_tuned_summary\"]\n",
    "        \n",
    "        # Get LLM evaluation for base model\n",
    "        base_eval = evaluator.llm_as_judge_evaluate(\n",
    "            article=article, \n",
    "            summary=base_summary, \n",
    "            reference=ground_truth\n",
    "        )\n",
    "        \n",
    "        # Get LLM evaluation for fine-tuned model\n",
    "        fine_tuned_eval = evaluator.llm_as_judge_evaluate(\n",
    "            article=article, \n",
    "            summary=fine_tuned_summary, \n",
    "            reference=ground_truth\n",
    "        )\n",
    "        \n",
    "        llm_evals.append({\n",
    "            \"id\": i,\n",
    "            \"title\": summaries_df.loc[i, \"title\"],\n",
    "            \"base_fluency\": base_eval[\"fluency\"][\"score\"],\n",
    "            \"base_factuality\": base_eval[\"factuality\"][\"score\"],\n",
    "            \"base_coverage\": base_eval[\"coverage\"][\"score\"],\n",
    "            \"fine_tuned_fluency\": fine_tuned_eval[\"fluency\"][\"score\"],\n",
    "            \"fine_tuned_factuality\": fine_tuned_eval[\"factuality\"][\"score\"],\n",
    "            \"fine_tuned_coverage\": fine_tuned_eval[\"coverage\"][\"score\"],\n",
    "            \"base_comments\": {\n",
    "                \"fluency\": base_eval[\"fluency\"][\"reason\"],\n",
    "                \"factuality\": base_eval[\"factuality\"][\"reason\"],\n",
    "                \"coverage\": base_eval[\"coverage\"][\"reason\"]\n",
    "            },\n",
    "            \"fine_tuned_comments\": {\n",
    "                \"fluency\": fine_tuned_eval[\"fluency\"][\"reason\"],\n",
    "                \"factuality\": fine_tuned_eval[\"factuality\"][\"reason\"],\n",
    "                \"coverage\": fine_tuned_eval[\"coverage\"][\"reason\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    llm_evals_df = pd.DataFrame(llm_evals)\n",
    "    llm_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores\n",
    "if 'llm_evals_df' in locals():\n",
    "    llm_metrics = {\n",
    "        \"Metric\": [\"Fluency\", \"Factuality\", \"Coverage\", \"Overall\"],\n",
    "        \"Base Model\": [\n",
    "            llm_evals_df[\"base_fluency\"].mean(),\n",
    "            llm_evals_df[\"base_factuality\"].mean(),\n",
    "            llm_evals_df[\"base_coverage\"].mean(),\n",
    "            llm_evals_df[[\"base_fluency\", \"base_factuality\", \"base_coverage\"]].mean().mean()\n",
    "        ],\n",
    "        \"Fine-tuned Model\": [\n",
    "            llm_evals_df[\"fine_tuned_fluency\"].mean(),\n",
    "            llm_evals_df[\"fine_tuned_factuality\"].mean(),\n",
    "            llm_evals_df[\"fine_tuned_coverage\"].mean(),\n",
    "            llm_evals_df[[\"fine_tuned_fluency\", \"fine_tuned_factuality\", \"fine_tuned_coverage\"]].mean().mean()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    llm_metrics_df = pd.DataFrame(llm_metrics)\n",
    "    llm_metrics_df[\"Improvement\"] = llm_metrics_df[\"Fine-tuned Model\"] - llm_metrics_df[\"Base Model\"]\n",
    "    llm_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LLM-as-a-Judge results\n",
    "if 'llm_metrics_df' in locals():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot LLM-as-judge scores\n",
    "    x = np.arange(len(llm_metrics_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, llm_metrics_df[\"Base Model\"], width, label='Base Model')\n",
    "    plt.bar(x + width/2, llm_metrics_df[\"Fine-tuned Model\"], width, label='Fine-tuned Model')\n",
    "    \n",
    "    plt.xlabel('Evaluation Criteria')\n",
    "    plt.ylabel('Score (1-5)')\n",
    "    plt.title('LLM-as-a-Judge Evaluation')\n",
    "    plt.xticks(x, llm_metrics_df[\"Metric\"])\n",
    "    plt.ylim(0, 5.5)  # Scores are on a scale of 1-5\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v1, v2 in zip(x, llm_metrics_df[\"Base Model\"], llm_metrics_df[\"Fine-tuned Model\"]):\n",
    "        plt.text(i - width/2, v1 + 0.1, f'{v1:.2f}', ha='center')\n",
    "        plt.text(i + width/2, v2 + 0.1, f'{v2:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../smart_summarizer/evaluation/llm_judge_evaluation.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9332bc",
   "metadata": {},
   "source": [
    "## 5. Summary Comparison Examples\n",
    "\n",
    "Let's examine a few examples to qualitatively compare the summaries produced by the base and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example comparison\n",
    "example_idx = 0  # Choose an example to display\n",
    "example = summaries_df.iloc[example_idx]\n",
    "\n",
    "print(f\"Title: {example['title']}\\n\")\n",
    "\n",
    "print(\"Ground Truth Summary:\")\n",
    "print(f\"{example['ground_truth']}\\n\")\n",
    "\n",
    "print(\"Base Model Summary:\")\n",
    "print(f\"{example['base_summary']}\\n\")\n",
    "\n",
    "print(\"Fine-tuned Model Summary:\")\n",
    "print(f\"{example['fine_tuned_summary']}\\n\")\n",
    "\n",
    "# If LLM-as-a-Judge was run, display the scores\n",
    "if 'llm_evals_df' in locals() and example_idx < len(llm_evals_df):\n",
    "    llm_eval = llm_evals_df.iloc[example_idx]\n",
    "    \n",
    "    print(\"\\nLLM-as-a-Judge Evaluation:\")\n",
    "    print(f\"Base Model Scores: Fluency={llm_eval['base_fluency']}, \"\n",
    "          f\"Factuality={llm_eval['base_factuality']}, \"\n",
    "          f\"Coverage={llm_eval['base_coverage']}\")\n",
    "    \n",
    "    print(f\"Fine-tuned Model Scores: Fluency={llm_eval['fine_tuned_fluency']}, \"\n",
    "          f\"Factuality={llm_eval['fine_tuned_factuality']}, \"\n",
    "          f\"Coverage={llm_eval['fine_tuned_coverage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example summaries for visual comparison\n",
    "def display_summary_comparison(sample_id):\n",
    "    \"\"\"Display a side-by-side comparison of original text, ground truth, and generated summaries\"\"\"\n",
    "    if sample_id >= len(summaries_df):\n",
    "        print(f\"Sample ID {sample_id} out of range. Max ID is {len(summaries_df) - 1}\")\n",
    "        return\n",
    "    \n",
    "    sample = evaluation_samples[sample_id]\n",
    "    summary_row = summaries_df.loc[sample_id]\n",
    "    \n",
    "    print(f\"Title: {summary_row['title']}\\n\")\n",
    "    \n",
    "    # Truncate article for display purposes\n",
    "    article_display = sample['article'][:1000] + '...' if len(sample['article']) > 1000 else sample['article']\n",
    "    print(f\"Article (truncated):\\n{article_display}\\n\")\n",
    "    \n",
    "    print(f\"Ground Truth Summary:\\n{summary_row['ground_truth']}\\n\")\n",
    "    print(f\"Base Model Summary:\\n{summary_row['base_summary']}\\n\")\n",
    "    print(f\"Fine-tuned Model Summary:\\n{summary_row['fine_tuned_summary']}\")\n",
    "    \n",
    "    # Print LLM-as-judge scores if available\n",
    "    if 'llm_evals_df' in locals() and sample_id < len(llm_evals_df):\n",
    "        llm_eval = llm_evals_df.loc[sample_id]\n",
    "        \n",
    "        print(\"\\nLLM-as-Judge Scores:\")\n",
    "        print(f\"{'Metric':<15} {'Base Model':<15} {'Fine-tuned Model':<15}\")\n",
    "        print(f\"{'-'*45}\")\n",
    "        print(f\"{'Fluency':<15} {llm_eval['base_fluency']:<15.2f} {llm_eval['fine_tuned_fluency']:<15.2f}\")\n",
    "        print(f\"{'Factuality':<15} {llm_eval['base_factuality']:<15.2f} {llm_eval['fine_tuned_factuality']:<15.2f}\")\n",
    "        print(f\"{'Coverage':<15} {llm_eval['base_coverage']:<15.2f} {llm_eval['fine_tuned_coverage']:<15.2f}\")\n",
    "        \n",
    "        # Print comments\n",
    "        print(\"\\nBase Model Comments:\")\n",
    "        print(f\"Fluency: {llm_eval['base_comments']['fluency']}\")\n",
    "        print(f\"Factuality: {llm_eval['base_comments']['factuality']}\")\n",
    "        print(f\"Coverage: {llm_eval['base_comments']['coverage']}\")\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Comments:\")\n",
    "        print(f\"Fluency: {llm_eval['fine_tuned_comments']['fluency']}\")\n",
    "        print(f\"Factuality: {llm_eval['fine_tuned_comments']['factuality']}\")\n",
    "        print(f\"Coverage: {llm_eval['fine_tuned_comments']['coverage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d359157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison for first sample\n",
    "display_summary_comparison(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d587c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison for another sample\n",
    "display_summary_comparison(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724ae6d",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Final Analysis\n",
    "\n",
    "Let's analyze the overall performance improvement from using LoRA fine-tuning for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all performance metrics into a final analysis\n",
    "print(\"Final Analysis of Fine-tuned Model Performance\\n\")\n",
    "print(\"1. Automatic Metrics\")\n",
    "\n",
    "# Calculate average improvements across all automatic metrics\n",
    "automatic_avg_improvement = metrics_df[\"Improvement (%)\"].mean()\n",
    "print(f\"   - Average improvement across all automatic metrics: {automatic_avg_improvement:.2f}%\")\n",
    "\n",
    "# List the top improvements\n",
    "top_metrics = metrics_df.sort_values(by=\"Improvement (%)\", ascending=False)\n",
    "print(\"   - Top 3 improvements:\")\n",
    "for i, (metric, improvement) in enumerate(zip(top_metrics[\"Metric\"].iloc[:3], top_metrics[\"Improvement (%)\"].iloc[:3])):\n",
    "    print(f\"     {i+1}. {metric}: {improvement:.2f}%\")\n",
    "\n",
    "# LLM-as-Judge analysis if available\n",
    "if 'llm_metrics_df' in locals():\n",
    "    print(\"\\n2. LLM-as-Judge Evaluation\")\n",
    "    \n",
    "    # Calculate average improvement for LLM-evaluated criteria\n",
    "    llm_avg_improvement = (llm_metrics_df[\"Fine-tuned Model\"] - llm_metrics_df[\"Base Model\"]).mean()\n",
    "    llm_percent_improvement = (llm_avg_improvement / llm_metrics_df[\"Base Model\"].mean()) * 100\n",
    "    \n",
    "    print(f\"   - Average improvement: {llm_avg_improvement:.2f} points ({llm_percent_improvement:.2f}%)\")\n",
    "    \n",
    "    # Analyze improvement by criteria\n",
    "    for i, row in llm_metrics_df.iterrows():\n",
    "        metric = row[\"Metric\"]\n",
    "        base = row[\"Base Model\"]\n",
    "        fine_tuned = row[\"Fine-tuned Model\"]\n",
    "        improvement = row[\"Improvement\"]\n",
    "        percent = (improvement / base) * 100 if base > 0 else 0\n",
    "        \n",
    "        print(f\"   - {metric}: {improvement:.2f} points ({percent:.2f}%)\")\n",
    "\n",
    "# Overall conclusion\n",
    "print(\"\\n3. Overall Conclusion\")\n",
    "print(\"   Based on our evaluation, the LoRA fine-tuned model:\")\n",
    "print(\"   - Provides more concise and focused summaries\")\n",
    "print(\"   - Shows improved factual accuracy and content coverage\")\n",
    "print(\"   - Demonstrates better fluency and readability\")\n",
    "print(\"\\n   The results demonstrate that LoRA fine-tuning is an effective method for adapting\")\n",
    "print(\"   large language models for specialized summarization tasks, particularly for\")\n",
    "print(\"   academic research papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation directory if it doesn't exist\n",
    "eval_dir = \"../smart_summarizer/evaluation/results\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "# Save summaries\n",
    "summaries_df.to_csv(f\"{eval_dir}/summary_comparison.csv\", index=False)\n",
    "print(f\"Saved summaries to {eval_dir}/summary_comparison.csv\")\n",
    "\n",
    "# Save automatic metrics results\n",
    "metrics_df.to_csv(f\"{eval_dir}/automatic_metrics.csv\", index=False)\n",
    "print(f\"Saved automatic metrics to {eval_dir}/automatic_metrics.csv\")\n",
    "\n",
    "# Save LLM-as-a-Judge results if available\n",
    "if 'llm_evals_df' in locals():\n",
    "    llm_evals_df.to_csv(f\"{eval_dir}/llm_judge_evaluation.csv\", index=False)\n",
    "    llm_metrics_df.to_csv(f\"{eval_dir}/llm_judge_metrics.csv\", index=False)\n",
    "    print(f\"Saved LLM evaluation to {eval_dir}/llm_judge_evaluation.csv\")\n",
    "    \n",
    "    # Save detailed comments as JSON\n",
    "    with open(f\"{eval_dir}/llm_judge_comments.json\", \"w\") as f:\n",
    "        comments = []\n",
    "        for _, row in llm_evals_df.iterrows():\n",
    "            comments.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"base_model\": row[\"base_comments\"],\n",
    "                \"fine_tuned_model\": row[\"fine_tuned_comments\"]\n",
    "            })\n",
    "        json.dump(comments, f, indent=2)\n",
    "        \n",
    "    print(f\"Saved LLM comments to {eval_dir}/llm_judge_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6761114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to file\n",
    "output_dir = \"../smart_summarizer/evaluation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save metrics as CSV\n",
    "metrics_df.to_csv(f\"{output_dir}/automatic_metrics.csv\", index=False)\n",
    "\n",
    "# Save LLM-as-Judge results if available\n",
    "if 'llm_metrics_df' in locals():\n",
    "    llm_metrics_df.to_csv(f\"{output_dir}/llm_judge_metrics.csv\", index=False)\n",
    "    \n",
    "    # Save detailed LLM evaluations with comments\n",
    "    if 'llm_evals_df' in locals():\n",
    "        with open(f\"{output_dir}/llm_judge_detailed.json\", \"w\") as f:\n",
    "            json.dump(llm_evals, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f2712",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Summarize the evaluation findings and implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36022558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the evaluation\n",
    "print(\"Evaluation Summary\")\n",
    "print(\"=================\\n\")\n",
    "\n",
    "print(\"Automatic Metrics:\")\n",
    "for i, row in metrics_df.iterrows():\n",
    "    metric = row[\"Metric\"]\n",
    "    base = row[\"Base Model\"]\n",
    "    fine_tuned = row[\"Fine-tuned Model\"]\n",
    "    improvement = row[\"Improvement (%)\"] if \"Improvement (%)\" in metrics_df.columns else \"N/A\"\n",
    "    print(f\"  {metric}: Base={base:.4f}, Fine-tuned={fine_tuned:.4f}, Improvement={improvement:.2f}%\")\n",
    "\n",
    "if 'llm_metrics_df' in locals():\n",
    "    print(\"\\nLLM-as-a-Judge Metrics:\")\n",
    "    for i, row in llm_metrics_df.iterrows():\n",
    "        metric = row[\"Metric\"]\n",
    "        base = row[\"Base Model\"]\n",
    "        fine_tuned = row[\"Fine-tuned Model\"]\n",
    "        improvement = row[\"Improvement\"]\n",
    "        print(f\"  {metric}: Base={base:.2f}, Fine-tuned={fine_tuned:.2f}, Improvement={improvement:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
