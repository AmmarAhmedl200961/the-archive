{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Smart Summarizer: LoRA Fine-Tuning with Unsloth (Colab/T4 Ready)\n",
        "\n",
        "This notebook implements Part A of the assignment using the [Unsloth](https://github.com/unslothai/unsloth) library for efficient LLM fine-tuning on a Colab T4 GPU.\n",
        "\n",
        "**Steps:**\n",
        "1. Data loading & preprocessing\n",
        "2. LoRA fine-tuning with Unsloth\n",
        "3. Inference (summarization)\n",
        "4. Evaluation (ROUGE, BLEU, BERTScore, LLM-as-a-Judge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install unsloth datasets peft transformers accelerate bitsandbytes evaluate bert-score rouge-score nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b0e0bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Load arXiv summarization dataset\n",
        "dataset = load_dataset('ccdv/arxiv-summarization')\n",
        "\n",
        "# Select 5,000 random samples from the train split\n",
        "random.seed(42)\n",
        "subset = dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "# Extract input (article) and target (abstract)\n",
        "inputs = [item['article'] for item in subset]\n",
        "targets = [item['abstract'] for item in subset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### Tokenization & Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "token = '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'\n",
        "\n",
        "# Choose a base model (Llama-3-8B or Mistral-7B)\n",
        "BASE_MODEL = 'meta-llama/Meta-Llama-3-8B'  # or 'unsloth/mistral-7b-bnb-4bit'\n",
        "\n",
        "# Load tokenizer\n",
        "model, tokenizer = FastTokenizer.from_pretrained(BASE_MODEL, max_seq_length=max_seq_length, dtype=dtype, load_in_4bit=load_in_4bit, token=token)\n",
        "\n",
        "# Tokenize inputs and targets\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['input'], truncation=True, padding='max_length', max_length=2048)\n",
        "\n",
        "data = [{'input': i, 'target': t} for i, t in zip(inputs, targets)]\n",
        "\n",
        "# Split data\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. LoRA Fine-Tuning with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLoraTrainer, FastModel\n",
        "\n",
        "# LoRA config\n",
        "lora_config = {\n",
        "    'r': 8,\n",
        "    'alpha': 16,\n",
        "    'dropout': 0.1,\n",
        "    'target_modules': ['q_proj', 'v_proj'],  # for Llama/Mistral\n",
        "    'use_gradient_checkpointing': 'unsloth',  # for Llama/Mistral\n",
        "}\n",
        "\n",
        "# Load model with LoRA\n",
        "model = FastModel.from_pretrained(BASE_MODEL, lora_config=lora_config)\n",
        "\n",
        "# Prepare datasets for Unsloth\n",
        "def format_example(example):\n",
        "    return {\n",
        "        'input_ids': tokenizer.encode(example['input'], truncation=True, max_length=2048),\n",
        "        'labels': tokenizer.encode(example['target'], truncation=True, max_length=512)\n",
        "    }\n",
        "\n",
        "train_dataset = list(map(format_example, train_data))\n",
        "val_dataset = list(map(format_example, val_data))\n",
        "\n",
        "# Training arguments\n",
        "training_args = {\n",
        "    'epochs': 4,\n",
        "    'batch_size': 1,\n",
        "    'eval_steps': 100,\n",
        "    'save_steps': 500,\n",
        "    'logging_steps': 50,\n",
        "    'lr': 2e-4,\n",
        "    'fp16': True\n",
        "}\n",
        "\n",
        "# Trainer\n",
        "trainer = FastLoraTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    **training_args\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained('lora_summarizer_final')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Inference: Generate Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Select 10 random test samples\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(test_data), 10, replace=False)\n",
        "test_samples = [test_data[i] for i in sample_indices]\n",
        "\n",
        "# Generate summaries with fine-tuned model\n",
        "ft_summaries = []\n",
        "for sample in test_samples:\n",
        "    input_ids = tokenizer.encode(sample['input'], return_tensors='pt', truncation=True, max_length=2048)\n",
        "    output = model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True)\n",
        "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    ft_summaries.append(summary)\n",
        "\n",
        "# Generate summaries with base model\n",
        "base_model = FastModel.from_pretrained(BASE_MODEL)\n",
        "base_summaries = []\n",
        "for sample in test_samples:\n",
        "    input_ids = tokenizer.encode(sample['input'], return_tensors='pt', truncation=True, max_length=2048)\n",
        "    output = base_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True)\n",
        "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    base_summaries.append(summary)\n",
        "\n",
        "# Ground-truth summaries\n",
        "gt_summaries = [sample['target'] for sample in test_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Evaluation: ROUGE, BLEU, BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def compute_metrics(preds, refs):\n",
        "    results = {'rouge1': [], 'rougeL': [], 'bleu': [], 'bertscore': []}\n",
        "    P, R, F1 = bert_score(preds, refs, lang='en', rescale_with_baseline=True)\n",
        "    for i in range(len(preds)):\n",
        "        rouge_res = rouge.compute(predictions=[preds[i]], references=[refs[i]])\n",
        "        bleu = sentence_bleu([refs[i].split()], preds[i].split(), smoothing_function=SmoothingFunction().method1)\n",
        "        results['rouge1'].append(rouge_res['rouge1'])\n",
        "        results['rougeL'].append(rouge_res['rougeL'])\n",
        "        results['bleu'].append(bleu)\n",
        "        results['bertscore'].append(F1[i].item())\n",
        "    return results\n",
        "\n",
        "ft_metrics = compute_metrics(ft_summaries, gt_summaries)\n",
        "base_metrics = compute_metrics(base_summaries, gt_summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### Visualize Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = ['ROUGE-1', 'ROUGE-L', 'BLEU', 'BERTScore']\n",
        "ft_means = [np.mean(ft_metrics['rouge1']), np.mean(ft_metrics['rougeL']), np.mean(ft_metrics['bleu']), np.mean(ft_metrics['bertscore'])]\n",
        "base_means = [np.mean(base_metrics['rouge1']), np.mean(base_metrics['rougeL']), np.mean(base_metrics['bleu']), np.mean(base_metrics['bertscore'])]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, ft_means, width, label='Fine-tuned')\n",
        "rects2 = ax.bar(x + width/2, base_means, width, label='Base')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Evaluation Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. LLM-as-a-Judge (Qualitative Evaluation via Together.ai API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Example: LLM-as-a-Judge prompt and API call (pseudo-code, fill in your Together.ai key and endpoint)\n",
        "import requests\n",
        "\n",
        "TOGETHER_API_KEY = '2b9e51fc4df8e0fd2af8a13e5b9c7672045d144fdeb0af379076fff0d1f7bdc6'\n",
        "JUDGE_MODEL = 'Meta-Llama-3.1-70B-Instruct-Turbo'\n",
        "\n",
        "def llm_judge(input_text, summary):\n",
        "    prompt = f'''Given the following input and the summary produced, evaluate the summary on \\n1. Fluency \\n2. Factuality \\n3. Coverage \\nUse a score from 1 (poor) to 5 (excellent) for each. Provide a short justification for each score.\\nInput: {input_text}\\nGenerated Summary: {summary}'''\n",
        "    response = requests.post(\n",
        "        'https://api.together.xyz/v1/chat/completions',\n",
        "        headers={'Authorization': f'Bearer {TOGETHER_API_KEY}'\n",
        "                 ,'Content-Type': 'application/json'},\n",
        "        json={\n",
        "            'model': JUDGE_MODEL,\n",
        "            'messages': [{'role': 'user', 'content': prompt}],\n",
        "            'max_tokens': 256\n",
        "        }\n",
        "    )\n",
        "    return response.json()['output']\n",
        "\n",
        "# Example usage (run for each test sample)\n",
        "for i in range(10):\n",
        "    print(llm_judge(test_samples[i]['input'], ft_summaries[i]))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
