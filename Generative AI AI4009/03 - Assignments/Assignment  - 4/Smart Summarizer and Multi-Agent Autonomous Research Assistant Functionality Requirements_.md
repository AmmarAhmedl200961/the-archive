### Project Requirements Document: Smart Summarizer & Multi-Agent Autonomous Research Assistant

The following table outlines the detailed functional requirements of the Smart Summarizer and Multi-Agent Autonomous Research Assistant system.

| Requirement ID | Description | User Story | Expected Behavior/Outcome |
|---|---|---|---|
| SS-FR001 | Data Loading and Preprocessing | As a user/developer, I need to load the arXiv summarization dataset so that I can prepare it for model training. | The system should load the specified dataset from Hugging Face. |
| SS-FR002 | Data Subsetting | As a user/developer, I need to select a subset of 5,000 samples from the dataset to manage computational resources. | The system should select 5,000 samples from the loaded dataset. |
| SS-FR003 | Input/Target Extraction | As a user/developer, I need to extract the article text as input and the abstract/summary as the target for training. | The system should correctly identify and separate the input (article) and target (abstract/summary) for each sample. |
| SS-FR004 | Data Tokenization | As a user/developer, I need to tokenize the dataset using the base model's tokenizer to prepare it for model input. | The system should tokenize both the input and target texts using the specified tokenizer. |
| SS-FR005 | Data Splitting | As a user/developer, I need to split the tokenized data into training, validation, and test sets with specified percentages to facilitate model training and evaluation. | The system should split the data into 80% training, 10% validation, and 10% test sets. |
| SS-FR006 | Model Selection | As a user/developer, I need to select a suitable pre-trained LLM for fine-tuning. | The system should allow selection of llama-3-7b or mistral-7b as the base model. |
| SS-FR007 | LoRA Integration | As a user/developer, I need to integrate LoRA into the base model using the HuggingFace PEFT library to enable parameter-efficient fine-tuning. | The system should successfully integrate LoRA with specified configuration parameters (r=8, alpha=16, dropout=0.1) applied to attention layers q and v. |
| SS-FR008 | Model Training | As a user/developer, I need to train the LoRA-integrated model for a specified number of epochs using the training data. | The system should train the model for 4 to 5 epochs, tracking training and validation loss. |
| SS-FR009 | Model Saving | As a user/developer, I need to save the trained LoRA model so I can use it for inference and evaluation. | The system should save the fine-tuned model weights and configuration. |
| SS-FR010 | Summary Generation (Fine-tuned) | As a user, I want to generate summaries for unseen test samples using the fine-tuned model. | The system should generate summaries for 10 samples from the test set using the fine-tuned model. |
| SS-FR011 | Summary Generation (Base) | As a user, I want to generate summaries for the same test samples using the base model for comparison. | The system should generate summaries for the 10 test samples using the original pre-trained model. |
| SS-FR012 | Ground Truth Retrieval | As a user, I need access to the ground-truth abstracts/summaries for comparison. | The system should retrieve and make available the original abstracts/summaries for the 10 test samples. |
| SS-FR013 | Automatic Evaluation Calculation | As a user/developer, I need to calculate automatic evaluation metrics (ROUGE-1, ROUGE-L, BLEU, BERTScore) for generated summaries. | The system should compute the specified metrics for both fine-tuned and base model summaries against the ground truth. |
| SS-FR014 | Automatic Evaluation Visualization | As a user/developer, I need to visualize the automatic evaluation scores to easily compare model performance. | The system should present the ROUGE, BLEU, and BERTScore results using bar charts or tables. |
| SS-FR015 | LLM-as-a-Judge Evaluation Setup | As a user/developer, I need to use a powerful LLM to qualitatively evaluate summaries based on fluency, factuality, and coverage. | The system should interface with a specified LLM (e.g., via Together.ai API) to perform evaluations. |
| SS-FR016 | LLM-as-a-Judge Scoring | As a user, I want the LLM to rate summaries on a scale of 1 to 5 for fluency, factuality, and coverage. | The LLM should output scores and short justifications for each criterion and summary. |
| SS-FR017 | LLM-as-a-Judge Average Calculation | As a user/developer, I need to calculate the average scores for each evaluation dimension from the LLM-as-a-Judge results. | The system should compute the average fluency, factuality, and coverage scores across the evaluated summaries. |
| SS-FR018 | Application UI (Upload) | As a user, I want to upload a research paper (PDF or plain text) through a web interface. | The Streamlit/Gradio application should provide a file upload mechanism. |
| SS-FR019 | Application UI (Summarize) | As a user, I want to click a button to initiate the summarization process for the uploaded paper. | The UI should have a "Summarize" button that triggers the fine-tuned model inference. |
| SS-FR020 | Application UI (Display Summary) | As a user, I want to view the generated summary of the uploaded paper. | The UI should display the summary produced by the fine-tuned model. |
| SS-FR021 | Application UI (Compare Summaries) | As a user, I want to compare the fine-tuned model's summary with the base model's summary. | The UI should provide an option to display the base model's summary alongside the fine-tuned one. |
| SS-FR022 | Application UI (Display LLM-as-a-Judge Scores) | As a user, I want to see the LLM-as-a-Judge scores for the generated summary in the application. | The UI should display the fluency, factuality, and coverage scores from the LLM-as-a-Judge evaluation for the generated summary. |
| RA-FR001 | Keyword Expansion | As a user, I want the system to generate expanded and related keywords from my initial research query to improve search coverage. | The KeywordAgent should use an LLM to produce a list of relevant search terms. |
| RA-FR002 | Literature Search | As a user, I want the system to search academic databases using the expanded keywords to find relevant papers. | The SearchAgent should interface with academic search APIs (e.g., arXiv, Semantic Scholar, PubMed) and retrieve paper metadata. |
| RA-FR003 | Paper Ranking | As a user, I want the system to rank the retrieved papers based on multiple criteria to identify the most significant ones. | The RankAgent should score papers based on citation count, publication date, and LLM-inferred relevance. |
| RA-FR004 | Paper Selection | As a user, I want the system to select the top-ranked papers for detailed summarization and analysis. | The system should identify and select the top 3-5 papers based on the ranking. |
| RA-FR005 | Paper Summarization (Agent) | As a user, I want the system to generate structured summaries of the selected top papers using the fine-tuned summarization model. | The SummaryAgent should process the content of the selected papers and produce summaries using the LoRA fine-tuned model. |
| RA-FR006 | Comparative Analysis | As a user, I want the system to compare the summaries of the top papers to find common themes, contradictions, and research gaps. | The CompareAgent should use an LLM to analyze the summaries and identify consensus, disagreements, and underexplored areas. |
| RA-FR007 | Research Report Generation | As a user, I want the system to compile the findings into a structured research report. | The system should generate a report including a topic summary, a ranked list of top papers with details, and a comparative analysis section. |
| RA-FR008 | Report Export/Visualization | As a user, I want to be able to export or visualize the generated research report. | The system should allow exporting the report (e.g., as PDF) or displaying it within a UI (e.g., Streamlit). |
