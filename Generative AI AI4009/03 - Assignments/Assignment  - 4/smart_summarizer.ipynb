{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Summarizer: Fine-Tuning LLMs with LoRA for Academic Paper Summarization\n",
    "\n",
    "This notebook implements a complete pipeline for fine-tuning a Large Language Model (LLM) using Low-Rank Adaptation (LoRA) to create an intelligent summarization system for academic research papers. The system is trained on the arXiv summarization dataset and evaluated using both automatic metrics and LLM-as-a-Judge qualitative evaluation.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Environment Configuration\n",
    "2. Data Preprocessing\n",
    "3. LoRA-Based Fine-Tuning\n",
    "4. Inference and Output Generation\n",
    "5. Model Evaluation\n",
    "   - Automatic Evaluation (ROUGE, BLEU, BERTScore)\n",
    "   - LLM-as-a-Judge Evaluation\n",
    "6. Streamlit/Gradio Interface Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "First, let's install all the required libraries and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
      "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets peft accelerate bitsandbytes tqdm evaluate nltk rouge_score bert_score streamlit gradio pdfplumber together torch matplotlib pandas numpy sentencepiece -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'torch==2.6.0' -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig, get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download NLTK data for BLEU score calculation\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Together.ai API key for LLM-as-a-Judge evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Together.ai API key\n",
    "# Replace with your actual API key\n",
    "import os\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"2b9e51fc4df8e0fd2af8a13e5b9c7672045d144fdeb0af379076fff0d1f7bdc6\"\n",
    "\n",
    "# Test the API connection\n",
    "import together\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "# Define the judge model to use\n",
    "JUDGE_MODEL = \"meta-llama/Llama-3.1-70B-Instruct\"  # You can also use \"deepseek-ai/deepseek-v2\" or \"meta-llama/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Load and preprocess the arXiv summarization dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n",
      "Sample entry:\n",
      "{'article': 'additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . \\n it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . \\n many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0 , see e.g. @xcite . in the last years \\n many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded . \\n this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.g. @xcite for the general case and @xcite for additive models . \\n therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e.g. @xcite . \\n such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to @xcite .    in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied . \\n our leading example covers learning rates for quantile regression based on the lipschitz continuous but non - differentiable pinball loss function , which is also called check function in the literature , see e.g. @xcite and @xcite for parametric quantile regression and @xcite , @xcite , and @xcite for kernel based quantile regression . \\n we will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own . \\n of course , a practical approach might be to fit both models and compare their risks evaluated for test data . \\n for the same reason we will also not cover sparsity . \\n consistency of support vector machines generated by additive kernels for additive models was considered in @xcite . in this paper \\n we establish learning rates for these algorithms . \\n let us recall the framework with a complete separable metric space @xmath3 as the input space and a closed subset @xmath4 of @xmath5 as the output space . \\n a borel probability measure @xmath6 on @xmath7 is used to model the learning problem and an independent and identically distributed sample @xmath8 is drawn according to @xmath6 for learning . \\n a loss function @xmath9 is used to measure the quality of a prediction function @xmath10 by the local error @xmath11 . \\n _ throughout the paper we assume that @xmath12 is measurable , @xmath13 , convex with respect to the third variable , and uniformly lipschitz continuous satisfying @xmath14 with a finite constant @xmath15 . \\n _    support vector machines ( svms ) considered here are kernel - based regularization schemes in a reproducing kernel hilbert space ( rkhs ) @xmath0 generated by a mercer kernel @xmath16 . with a shifted loss function @xmath17 introduced for dealing \\n even with heavy - tailed distributions as @xmath18 , they take the form @xmath19 where for a general borel measure @xmath20 on @xmath21 , the function @xmath22 is defined by @xmath23 where @xmath24 is a regularization parameter . \\n the idea to shift a loss function has a long history , see e.g. @xcite in the context of m - estimators . \\n it was shown in @xcite that @xmath22 is also a minimizer of the following optimization problem involving the original loss function @xmath12 if a minimizer exists : @xmath25    the additive model we consider consists of the _ input space decomposition _ \\n @xmath26 with each @xmath27 a complete separable metric space and a _ hypothesis space _ \\n @xmath28 where @xmath29 is a set of functions @xmath30 each of which is also identified as a map @xmath31 from @xmath3 to @xmath5 . \\n hence the functions from @xmath32 take the additive form @xmath33 . \\n we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity @xmath34 is an element of the set @xmath35 which is a subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 each quantity @xmath38 is an element of the full input space @xmath36 , where @xmath39 . \\n because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols . \\n the additive kernel @xmath40 is defined in terms of mercer kernels @xmath41 on @xmath27 as @xmath42 it generates an rkhs @xmath0 which can be written in terms of the rkhs @xmath43 generated by @xmath41 on @xmath27 corresponding to the form ( [ additive ] ) as @xmath44 with norm given by @xmath45 the norm of @xmath46 satisfies @xmath47    to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels . \\n the first example deals with gaussian rbf kernels . \\n all proofs will be given in section [ proofsection ] . \\n [ gaussadd ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and @xmath52.\\\\ ] ] the additive kernel @xmath53 is given by @xmath54 furthermore , the product kernel @xmath55 is the standard gaussian kernel given by @xmath56 define a gaussian function @xmath57 on @xmath58 ^ 2 $ ] depending only on one variable by @xmath59 then @xmath60 but @xmath61 where @xmath62 denotes the rkhs generated by the standard gaussian rbf kernel @xmath63 . \\n the second example is about sobolev kernels . \\n [ sobolvadd ] let @xmath64 , @xmath65 $ ] and @xmath58^s.$ ] let @xmath66 : = \\\\bigl\\\\{u\\\\in l_2([0,1 ] ) ; d^\\\\alpha u \\\\in l_2([0,1 ] ) \\\\mbox{~for~all~}|\\\\alpha|\\\\le 1\\\\bigr\\\\}\\\\ ] ] be the sobolev space consisting of all square integrable univariate functions whose derivative is also square integrable . \\n it is an rkhs with a mercer kernel @xmath67 defined on @xmath68 ^ 2 $ ] . \\n if we take all the mercer kernels @xmath69 to be @xmath67 , then @xmath70 $ ] for each @xmath71 . \\n the additive kernel @xmath72 is also a mercer kernel and defines an rkhs @xmath73\\\\right\\\\}.\\\\ ] ] however , the multivariate sobolev space @xmath74^s)$ ] , consisting of all square integrable functions whose partial derivatives are all square integrable , contains discontinuous functions and is not an rkhs . \\n denote the marginal distribution of @xmath6 on @xmath27 as @xmath75 . under the assumption that @xmath76 for each @xmath71 and that @xmath43 is dense in @xmath29 in the @xmath77-metric , it was proved in @xcite that @xmath78 in probability as long as @xmath79 satisfies @xmath80 and @xmath81 . \\n the rest of the paper has the following structure . \\n section [ ratessection ] contains our main results on learning rates for svms based on additive kernels . learning rates for quantile regression \\n are treated as important special cases . \\n section [ comparisonsection ] contains a comparison of our results with other learning rates published recently . \\n section [ proofsection ] contains all the proofs and some results which can be interesting in their own . \\n in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in @xcite . \\n the rates are about asymptotic behaviors of the excess risk @xmath82 and take the form @xmath83 with @xmath84 . \\n they will be stated under three kinds of conditions involving the hypothesis space @xmath0 , the measure @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 . \\n the first condition is about the approximation ability of the hypothesis space @xmath0 . \\n since the output function @xmath19 is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space @xmath0 with respect to the optimal risk @xmath86 measured by the following approximation error . \\n [ defapprox ] the approximation error of the triple @xmath87 is defined as @xmath88    to estimate the approximation error , we make an assumption about the minimizer of the risk @xmath89    for each @xmath90 , define the integral operator @xmath91 associated with the kernel @xmath41 by @xmath92 we mention that @xmath93 is a compact and positive operator on @xmath94 . hence we can find its normalized eigenpairs @xmath95 such that @xmath96 is an orthonormal basis of @xmath94 and @xmath97 as @xmath98 . fix @xmath99 . \\n then we can define the @xmath100-th power @xmath101 of @xmath93 by @xmath102 this is a positive and bounded operator and its range is well - defined . \\n the assumption @xmath103 means @xmath104 lies in this range . \\n [ assumption1 ] we assume @xmath105 and @xmath106 where for some @xmath107 and each @xmath108 , @xmath109 is a function of the form @xmath110 with some @xmath111 . \\n the case @xmath112 of assumption [ assumption1 ] means each @xmath113 lies in the rkhs @xmath43 . \\n a standard condition in the literature ( e.g. , @xcite ) for achieving decays of the form @xmath114 for the approximation error ( [ approxerrordef ] ) is @xmath115 with some @xmath116 . here \\n the operator @xmath117 is defined by @xmath118 in general , this can not be written in an additive form . \\n however , the hypothesis space ( [ additive ] ) takes an additive form @xmath119 . \\n so it is natural for us to impose an additive expression @xmath120 for the target function @xmath121 with the component functions @xmath113 satisfying the power condition @xmath110 . \\n the above natural assumption leads to a technical difficulty in estimating the approximation error : the function @xmath113 has no direct connection to the marginal distribution @xmath122 projected onto @xmath27 , hence existing methods in the literature ( e.g. , @xcite ) can not be applied directly . \\n note that on the product space @xmath123 , there is no natural probability measure projected from @xmath6 , and the risk on @xmath124 is not defined .    our idea to overcome the difficulty is to introduce an intermediate function @xmath125 . \\n it may not minimize a risk ( which is not even defined ) . \\n however , it approximates the component function @xmath113 well . \\n when we add up such functions @xmath126 , we get a good approximation of the target function @xmath121 , and thereby a good estimate of the approximation error . \\n this is the first novelty of the paper . \\n [ approxerrorthm ] under assumption [ assumption1 ] , we have @xmath127 where @xmath128 is the constant given by @xmath129      the second condition for our learning rates is about the capacity of the hypothesis space measured by @xmath130-empirical covering numbers .    let @xmath131 be a set of functions on @xmath21 and @xmath132 for every @xmath133 the * covering number of @xmath131 * with respect to the empirical metric @xmath134 , given by @xmath135 is defined as @xmath136 and the * @xmath130-empirical covering number * of @xmath137 is defined as @xmath138    [ assumption2 ] we assume @xmath139 and that for some @xmath140 , @xmath141 and every @xmath142 , the @xmath130-empirical covering number of the unit ball of @xmath43 satisfies @xmath143    the second novelty of this paper is to observe that the additive nature of the hypothesis space yields the following nice bound with a dimension - independent power exponent for the covering numbers of the balls of the hypothesis space @xmath0 , to be proved in section [ samplesection ] . \\n [ capacitythm ] under assumption [ assumption2 ] , for any @xmath144 and @xmath145 , we have @xmath146    the bound for the covering numbers stated in theorem [ capacitythm ] is special : the power @xmath147 is independent of the number @xmath148 of the components in the additive model . \\n it is well - known @xcite in the literature of function spaces that the covering numbers of balls of the sobolev space @xmath149 on the cube @xmath150^s$ ] of the euclidean space @xmath151 with regularity index @xmath152 has the following asymptotic behavior with @xmath153 : @xmath154 here the power @xmath155 depends linearly on the dimension @xmath148 . \\n similar dimension - dependent bounds for the covering numbers of the rkhss associated with gaussian rbf - kernels can be found in @xcite . \\n the special bound in theorem [ capacitythm ] demonstrates an advantage of the additive model in terms of capacity of the additive hypothesis space . \\n the third condition for our learning rates is about the noise level in the measure @xmath6 with respect to the hypothesis space . before stating the general condition \\n , we consider a special case for quantile regression , to illustrate our general results . \\n let @xmath156 be a quantile parameter . \\n the quantile regression function @xmath157 is defined by its value @xmath158 to be a @xmath159-quantile of @xmath160 , i.e. , a value @xmath161 satisfying @xmath162 the regularization scheme for quantile regression considered here takes the form ( [ algor ] ) with the loss function @xmath12 given by the pinball loss as @xmath163    a noise condition on @xmath6 for quantile regression is defined in @xcite as follows . to this end , let @xmath164 be a probability measure on @xmath165 and @xmath166 . then a real number @xmath167 is called @xmath159-quantile of @xmath164 , if and only if @xmath167 belongs to the set @xmath168\\\\bigr ) \\\\ge \\n \\\\tau     \\\\mbox{~~and~~ } q\\\\bigl([t , \\\\infty)\\\\bigr ) \\\\ge 1-\\\\tau\\\\bigr\\\\}\\\\,.\\\\ ] ] it is well - known that @xmath169 is a compact interval . \\n [ noisecond ] let @xmath166 .    1 . \\n a probability measure @xmath164 on @xmath165 is said to have a * @xmath159-quantile of type @xmath170 * , if there exist a @xmath159-quantile @xmath171 and a constant @xmath172 such that , for all @xmath173 $ ] , we have @xmath174 2 . \\n let @xmath175 $ ] . \\n we say that a probability measure @xmath20 on @xmath176 has a * @xmath159-quantile of @xmath177-average type @xmath170 * if the conditional probability measure @xmath178 has @xmath179-almost surely a @xmath159-quantile of type @xmath170 and the function @xmath180 where @xmath181 is the constant defined in part ( 1 ) , satisfies @xmath182 . \\n one can show that a distribution @xmath164 having a @xmath159-quantile of type @xmath170 has a unique @xmath159-quantile @xmath183 . \\n moreover , if @xmath164 has a lebesgue density @xmath184 then @xmath164 has a @xmath159-quantile of type @xmath170 if @xmath184 is bounded away from zero on @xmath185 $ ] since we can use @xmath186\\\\}$ ] in ( [ tauquantileoftype2formula ] ) . \\n this assumption is general enough to cover many distributions used in parametric statistics such as gaussian , student s @xmath187 , and logistic distributions ( with @xmath188 ) , gamma and log - normal distributions ( with @xmath189 ) , and uniform and beta distributions ( with @xmath190 $ ] ) . \\n the following theorem , to be proved in section [ proofsection ] , gives a learning rate for the regularization scheme ( [ algor ] ) in the special case of quantile regression . \\n [ quantilethm ] suppose that @xmath191 almost surely for some constant @xmath192 , and that each kernel @xmath41 is @xmath193 with @xmath194 for some @xmath195 . \\n if assumption [ assumption1 ] holds with @xmath112 and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath198 and @xmath199 , with confidence at least @xmath200 we have @xmath201 where @xmath202 is a constant independent of @xmath203 and @xmath204 and @xmath205    please note that the exponent @xmath206 given by ( [ quantilerates2 ] ) for the learning rate in ( [ quantilerates ] ) is independent of the quantile level @xmath159 , of the number @xmath148 of additive components in @xmath207 , and of the dimensions @xmath208 and @xmath209 further note that @xmath210 , if @xmath211 , and @xmath212 if @xmath213 . because @xmath214 can be arbitrarily close to @xmath215 , the learning rate , which is independent of the dimension @xmath216 and given by theorem [ quantilethm ] , is close to @xmath217 for large values of @xmath177 and is close to @xmath218 or better , if @xmath211 .      to state our general learning rates \\n , we need an assumption on a _ variance - expectation bound _ which is similar to definition [ noisecond ] in the special case of quantile regression . \\n [ assumption3 ] we assume that there exist an exponent @xmath219 $ ] and a positive constant @xmath220 such that @xmath221    assumption [ assumption3 ] always holds true for @xmath222 . if the triple @xmath223 satisfies some conditions , the exponent @xmath224 can be larger . \\n for example , when @xmath12 is the pinball loss ( [ pinloss ] ) and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath225 for some @xmath196 $ ] and @xmath226 as defined in @xcite , then @xmath227 . \\n [ mainratesthm ] suppose that @xmath228 is bounded by a constant @xmath229 almost surely . under assumptions [ assumption1 ] to [ assumption3 ] , \\n if we take @xmath198 and @xmath230 for some @xmath231 , then for any @xmath232 , with confidence at least @xmath200 we have @xmath233 where @xmath234 is given by @xmath235 and @xmath202 is constant independent of @xmath203 or @xmath204 ( to be given explicitly in the proof ) . \\n we now add some theoretical and numerical comparisons on the goodness of our learning rates with those from the literature . as already mentioned in the introduction \\n , some reasons for the popularity of additive models are flexibility , increased interpretability , and ( often ) a reduced proneness of the curse of high dimensions . \\n hence it is important to check , whether the learning rate given in theorem [ mainratesthm ] under the assumption of an additive model favourably compares to ( essentially ) optimal learning rates without this assumption . in other words , \\n we need to demonstrate that the main goal of this paper is achieved by theorem [ quantilethm ] and theorem [ mainratesthm ] , i.e. that an svm based on an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , provided the assumption of an additive model is satisfied . \\n our learning rate in theorem [ quantilethm ] is new and optimal in the literature of svm for quantile regression . \\n most learning rates in the literature of svm for quantile regression are given for projected output functions @xmath236 , while it is well known that projections improve learning rates @xcite . here the projection operator @xmath237 is defined for any measurable function @xmath10 by @xmath238 sometimes this is called clipping . \\n such results are given in @xcite . \\n for example , under the assumptions that @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , the approximation error condition ( [ approxerrorb ] ) is satisfied for some @xmath239 , and that for some constants @xmath240 , the sequence of eigenvalues @xmath241 of the integral operator @xmath117 satisfies @xmath242 for every @xmath243 , it was shown in @xcite that with confidence at least @xmath200 , @xmath244 where @xmath245 here the parameter @xmath246 measures the capacity of the rkhs @xmath247 and it plays a similar role as half of the parameter @xmath147 in assumption 2 . for a @xmath193 kernel and @xmath112 \\n , one can choose @xmath246 and @xmath147 to be arbitrarily small and the above power index @xmath248 can be taken as @xmath249 . \\n the learning rate in theorem [ quantilethm ] may be improved by relaxing assumption 1 to a sobolev smoothness condition for @xmath121 and a regularity condition for the marginal distribution @xmath250 . \\n for example , one may use a gaussian kernel @xmath251 depending on the sample size @xmath203 and @xcite achieve the approximation error condition ( [ approxerrorb ] ) for some @xmath252 . \\n this is done for quantile regression in @xcite . \\n since we are mainly interested in additive models , we shall not discuss such an extension . \\n [ gaussmore ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and the additive kernel @xmath72 be given by ( [ gaussaddform ] ) with @xmath253 in example [ gaussadd ] as @xmath52.\\\\ ] ] if the function @xmath121 is given by ( [ gaussfcn ] ) , @xmath191 almost surely for some constant @xmath192 , and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath145 and @xmath199 , ( [ quantilerates ] ) holds with confidence at least @xmath200 .    it is unknown whether the above learning rate can be derived by existing approaches in the literature ( e.g. @xcite ) even after projection . \\n note that the kernel in the above example is independent of the sample size . \\n it would be interesting to see whether there exists some @xmath99 such that the function @xmath57 defined by ( [ gaussfcn ] ) lies in the range of the operator @xmath254 . \\n the existence of such a positive index would lead to the approximation error condition ( [ approxerrorb ] ) , see @xcite .    let us now add some numerical comparisons on the goodness of our learning rates given by theorem [ mainratesthm ] with those given by @xcite . \\n their corollary 4.12 gives ( essentially ) minmax optimal learning rates for ( clipped ) svms in the context of nonparametric quantile regression using one gaussian rbf kernel on the whole input space under appropriate smoothness assumptions of the target function . \\n let us consider the case that the distribution @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , where @xmath255 , and assume that both corollary 4.12 in @xcite and our theorem [ mainratesthm ] are applicable . \\n i.e. , we assume in particular that @xmath6 is a probability measure on @xmath256 $ ] and that the marginal distribution @xmath257 has a lebesgue density @xmath258 for some @xmath259 . furthermore , suppose that the optimal decision function @xmath260 has ( to make theorem [ mainratesthm ] applicable with @xmath261 $ ] ) the additive structure @xmath207 with each @xmath104 as stated in assumption [ assumption1 ] , where @xmath262 and @xmath263 , with minimal risk @xmath86 and additionally fulfills ( to make corollary 4.12 in @xcite applicable ) @xmath264 where @xmath265 $ ] and @xmath266 denotes a besov space with smoothness parameter @xmath267 . \\n the intuitive meaning of @xmath248 is , that increasing values of @xmath248 correspond to increased smoothness . \\n we refer to ( * ? ? ? * and p. 44 ) for details on besov spaces . \\n it is well - known that the besov space @xmath268 contains the sobolev space @xmath269 for @xmath270 , @xmath271 , and @xmath272 , and that @xmath273 . \\n we mention that if all @xmath41 are suitably chosen wendland kernels , their reproducing kernel hilbert spaces @xmath43 are sobolev spaces , see ( * ? ? ? \\n * thm . 10.35 , p. 160 ) . \\n furthermore , we use the same sequence of regularizing parameters as in ( * ? ? ? \\n 4.9 , cor . 4.12 ) , i.e. , @xmath274 where @xmath275 , @xmath276 , @xmath277 $ ] , and @xmath278 is some user - defined positive constant independent of @xmath279 . for \\n reasons of simplicity , let us fix @xmath280 . \\n then ( * ? ? ? \\n 4.12 ) gives learning rates for the risk of svms for @xmath159-quantile regression , if a single gaussian rbf - kernel on @xmath281 is used for @xmath159-quantile functions of @xmath177-average type @xmath170 with @xmath255 , which are of order @xmath282 hence the learning rate in theorem [ quantilethm ] is better than the one in ( * ? ? ? \\n 4.12 ) in this situation , if @xmath283 provided the assumption of the additive model is valid . \\n table [ table1 ] lists the values of @xmath284 from ( [ explicitratescz2 ] ) for some finite values of the dimension @xmath216 , where @xmath285 . \\n all of these values of @xmath284 are positive with the exceptions if @xmath286 or @xmath287 . \\n this is in contrast to the corresponding exponent in the learning rate by ( * ? ? \\n * cor . 4.12 ) , because @xmath288    table [ table2 ] and figures [ figure1 ] to [ figure2 ] give additional information on the limit @xmath289 . \\n of course , higher values of the exponent indicates faster rates of convergence . \\n it is obvious , that an svm based on an additive kernel has a significantly faster rate of convergence in higher dimensions @xmath216 compared to svm based on a single gaussian rbf kernel defined on the whole input space , of course under the assumption that the additive model is valid . \\n the figures seem to indicate that our learning rate from theorem [ mainratesthm ] is probably not optimal for small dimensions . however , the main focus of the present paper is on high dimensions . \\n .[table1 ] the table lists the limits of the exponents @xmath290 from ( * ? ? ? \\n * cor . 4.12 ) and @xmath291 from theorem [ mainratesthm ] , respectively , if the regularizing parameter @xmath292 is chosen in an optimal manner for the nonparametric setup , i.e. @xmath293 , with @xmath294 for @xmath295 and @xmath296 . \\n recall that @xmath297 $ ] . \\n [ cols= \" > , > , > , > \" , ]', 'abstract': 'additive models play an important role in semiparametric statistics . \\n this paper gives learning rates for regularized kernel based methods for additive models . \\n these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . \\n additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .    * \\n key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine .'}\n"
     ]
    }
   ],
   "source": [
    "# Load the arXiv summarization dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "# Display a sample entry\n",
    "print(\"Sample entry:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 203037\n",
      "Subset size: 5000\n",
      "Training set size: 4000\n",
      "Validation set size: 500\n",
      "Test set size: 500\n"
     ]
    }
   ],
   "source": [
    "# Select a subset of 5,000 samples\n",
    "# First, set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select 5,000 samples randomly\n",
    "train_subset_indices = random.sample(range(len(dataset['train'])), 5000)\n",
    "train_subset = dataset['train'].select(train_subset_indices)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Subset size: {len(train_subset)}\")\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "train_val_test = train_subset.train_test_split(test_size=0.2, seed=42)\n",
    "test_val = train_val_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_val_test['train']\n",
    "val_data = test_val['train']\n",
    "test_data = test_val['test']\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a pre-trained model and tokenizer. We'll use Mistral 7B for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bc6cf391f4438fb5ef2d2f114f5d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc796aab8bd46429a76873d2bce9748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e933f4969491494e6b0acf10fea9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c385f2b2564d4938b85d98f4d61dd256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training set size: 4000\n",
      "Tokenized validation set size: 500\n",
      "Tokenized test set size: 500\n"
     ]
    }
   ],
   "source": [
    "# Select a pre-trained model and tokenizer\n",
    "token = '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'\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Alternative: \"meta-llama/Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define context window size\n",
    "max_length = 1024  # Adjust based on model and available GPU memory\n",
    "\n",
    "# Define prompt template\n",
    "def create_prompt(article, summary=\"\"):\n",
    "    return f\"\"\"Summarize the following academic paper:\n",
    "    \n",
    "Article: {article}\n",
    "    \n",
    "Summary: {summary}\"\"\"\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    prompts = [create_prompt(article, summary) for article, summary in zip(examples['article'], examples['abstract'])]\n",
    "    tokenized_inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "tokenized_test = test_data.map(tokenize_function, batched=True, remove_columns=['article', 'abstract'])\n",
    "\n",
    "print(f\"Tokenized training set size: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized validation set size: {len(tokenized_val)}\")\n",
    "print(f\"Tokenized test set size: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA-Based Fine-Tuning\n",
    "\n",
    "Set up and fine-tune the model using LoRA (Low-Rank Adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a48e333bdb4ba79298db2f61fd3883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c233e64c8954c63966920038b172e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3700d2ba77bd439d848d18cb322e2582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c1e8b667fd4085bb30384b80df5abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c824fd3a58a84a1d8ef9d632907abd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d28577a35c49eab5a860ab687ac242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40a2b7ff4114e3b81f7ca25478b6357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LoRA configuration created.\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model in 4-bit precision for memory efficiency\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Set compute dtype to match input or use bfloat16 if available\n",
    "    bnb_4bit_compute_dtype=torch.float16, # or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank\n",
    "    lora_alpha=16,          # Alpha parameter\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    bias=\"none\",            # No bias\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal language modeling task\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRA to attention layers q and v\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model with LoRA configuration created.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 2:52:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.659500</td>\n",
       "      <td>1.687167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.618600</td>\n",
       "      <td>1.688935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Training metrics: {'train_runtime': 10334.3083, 'train_samples_per_second': 1.935, 'train_steps_per_second': 0.121, 'total_flos': 8.7417667190784e+17, 'train_loss': 1.64264345703125, 'epoch': 5.0}\n",
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, # Add this line\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,             # Train for 5 epochs as per requirements\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=4,   # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    eval_strategy=\"steps\",    # Evaluate during training\n",
    "    eval_steps=500,                 # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\",          # Save during training\n",
    "    save_steps=500,                 # Save every 500 steps\n",
    "    warmup_steps=100,               # Warmup steps\n",
    "    learning_rate=2e-4,             # Learning rate\n",
    "    weight_decay=0.01,              # Weight decay\n",
    "    fp16=True,                      # Use mixed precision\n",
    "    logging_steps=100,              # Log every 100 steps\n",
    "    logging_dir=\"./logs\",           # Logging directory\n",
    "    report_to='none',               # Disable reporting to external services\n",
    "    save_total_limit=3,             # Save only the last 3 checkpoints\n",
    "    load_best_model_at_end=True,    # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\"  # Metric to use for the best model\n",
    ")\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not using masked language modeling\n",
    ")\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Model training complete.\")\n",
    "print(f\"Training metrics: {train_result.metrics}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss history: [1.7069, 1.6682, 1.7046, 1.6628, 1.6595, 1.637, 1.6397, 1.6255, 1.6128, 1.6186, 1.5967, 1.5947]\n",
      "Evaluation loss history: [1.6871665716171265, 1.6889350414276123]\n"
     ]
    }
   ],
   "source": [
    "# extract train and eval losses\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "\n",
    "for log in trainer.state.log_history:\n",
    "    if 'loss' in log:\n",
    "        train_loss.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        eval_loss.append(log['eval_loss'])\n",
    "\n",
    "# print or process the logged losses\n",
    "print(\"Training loss history:\", train_loss)\n",
    "print(\"Evaluation loss history:\", eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will not work as logs are tfboard logs\n",
    "# Plot training and validation losses\n",
    "training_logs = pd.read_csv(\"./logs/train_loss.csv\") if os.path.exists(\"./logs/train_loss.csv\") else None\n",
    "validation_logs = pd.read_csv(\"./logs/eval_loss.csv\") if os.path.exists(\"./logs/eval_loss.csv\") else None\n",
    "\n",
    "if training_logs is not None and validation_logs is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_logs['step'], training_logs['loss'], label='Training Loss')\n",
    "    plt.plot(validation_logs['step'], validation_logs['loss'], label='Validation Loss')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss_curve.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Output Generation\n",
    "\n",
    "Generate summaries using the fine-tuned model and the base model, and compare them with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter load cell\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load the arXiv summarization dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "\n",
    "# Select a subset of 5,000 samples\n",
    "# First, set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select 5,000 samples randomly\n",
    "train_subset_indices = random.sample(range(len(dataset['train'])), 5000)\n",
    "train_subset = dataset['train'].select(train_subset_indices)\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "train_val_test = train_subset.train_test_split(test_size=0.2, seed=42)\n",
    "test_val = train_val_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_val_test['train']\n",
    "val_data = test_val['train']\n",
    "test_data = test_val['test']\n",
    "\n",
    "token = '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'\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Alternative: \"meta-llama/Llama-3-8B\"\n",
    "\n",
    "def create_prompt(article, summary=\"\"):\n",
    "    return f\"\"\"Summarize the following academic paper:\n",
    "    \n",
    "Article: {article}\n",
    "    \n",
    "Summary: {summary}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer from ./fine_tuned_model...\n",
      "Tokenizer loaded.\n",
      "Loading base model mistralai/Mistral-7B-v0.1 and applying PEFT adapters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed24ceff648749b7a5df4cb656dff62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model (base + PEFT adapters) loaded.\n",
      "Fine-tuned model loaded on device: cuda:0\n",
      "Selected 10 samples for inference.\n",
      "\n",
      "Generating summaries with fine-tuned model...\n",
      "Using max_new_tokens=200 for generation.\n",
      "Processing sample 1/10 with fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred during fine-tuned model loading or inference: CUDA out of memory. Tried to allocate 26.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 7.76 GiB is free. Process 29206 has 6.98 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Cell 1 failed.\n",
      "\n",
      "Releasing fine-tuned model memory...\n",
      "Attempting to free memory for PeftModelForCausalLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2588/2614807242.py\", line 173, in <module>\n",
      "    fine_tuned_summary = generate_summary(model, tokenizer, article, max_new_tokens=generation_max_tokens, device=current_device)\n",
      "  File \"/tmp/ipykernel_2588/2614807242.py\", line 57, in generate_summary\n",
      "    output = model.generate(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/peft_model.py\", line 1875, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2465, in generate\n",
      "    result = self._sample(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3431, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 810, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 506, in forward\n",
      "    causal_mask = self._update_causal_mask(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 624, in _update_causal_mask\n",
      "    causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 690, in _prepare_4d_causal_attention_mask_with_cache_position\n",
      "    causal_mask = torch.full(\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 7.76 GiB is free. Process 29206 has 6.98 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache emptied.\n",
      "Model variable deleted and memory potentially freed.\n",
      "Releasing temporary base model object used for PEFT...\n",
      "Attempting to free memory for MistralForCausalLM...\n",
      "CUDA cache emptied.\n",
      "Model variable deleted and memory potentially freed.\n",
      "\n",
      "Cell 1 execution complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Fine-tuned Model Inference & Temporary Save\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel # Import PeftModel\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import sys # Import sys to potentially exit on critical errors\n",
    "\n",
    "# --- Assume these variables are defined from prior setup cells ---\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "fine_tuned_model_path = \"./fine_tuned_model\"\n",
    "# test_data = [...] # Make sure this is loaded\n",
    "# create_prompt = lambda article: ... # Make sure this is defined\n",
    "# token = \"...\" # Your Hugging Face token\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Your device\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Helper functions (included for robustness after potential restart) ---\n",
    "def generate_summary(model, tokenizer, article, max_new_tokens=300, device=\"cuda\"):\n",
    "    input_text = create_prompt(article)\n",
    "    try:\n",
    "        # Ensure inputs are on the correct device\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        attention_mask = tokenizer(input_text, return_tensors=\"pt\").attention_mask # Get attention mask\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device) # Move attention mask to device\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error preparing tensor for device {device}: {e}\")\n",
    "         # Fallback to cpu if device fails, or handle error as appropriate\n",
    "         input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "         attention_mask = tokenizer(input_text, return_tensors=\"pt\").attention_mask.to(\"cpu\")\n",
    "         print(\"Moved input_ids and attention_mask to CPU as fallback.\")\n",
    "\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        # \"pad_token_id\": tokenizer.eos_token_id # This is often handled automatically if pad_token is set\n",
    "    }\n",
    "    # Add attention_mask to generation call\n",
    "    # Mistral/Llama models can warn about attention_mask if pad_token == eos_token,\n",
    "    # explicitly passing it can sometimes help or is recommended.\n",
    "    # Note: Ensure pad_token is set correctly on the tokenizer.\n",
    "    # If you set pad_token = eos_token, the warning is expected but passing the mask is still good practice.\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask, # Pass attention mask\n",
    "            **gen_config\n",
    "        )\n",
    "\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the summary part (after \"Summary:\")\n",
    "    try:\n",
    "        # Assuming the prompt structure leads to \"Summary: ...\"\n",
    "        summary = summary.split(\"Summary:\")[1].strip()\n",
    "    except IndexError:\n",
    "        # If \"Summary:\" is not in the output, use the whole output or handle differently\n",
    "        # print(\"Warning: 'Summary:' not found in generated text. Using full output.\")\n",
    "        pass # Use the whole output if split fails\n",
    "\n",
    "    return summary\n",
    "\n",
    "def free_model_memory(model):\n",
    "    \"\"\"Attempts to free memory occupied by a PyTorch model.\"\"\"\n",
    "    print(f\"Attempting to free memory for {type(model).__name__ if model else 'None'}...\")\n",
    "    if model is not None:\n",
    "        # Dereference the model\n",
    "        del model\n",
    "        # Run Python garbage collection\n",
    "        gc.collect()\n",
    "        # Empty CUDA cache if available\n",
    "        if torch.cuda.is_available():\n",
    "             # Move allocated tensors to CPU before clearing cache if possible,\n",
    "             # though del and gc.collect() should handle model tensors.\n",
    "             # A full empty_cache() is often the most effective.\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"CUDA cache emptied.\")\n",
    "        print(\"Model variable deleted and memory potentially freed.\")\n",
    "    else:\n",
    "         print(\"Model variable was already None.\")\n",
    "# --- End Helper functions ---\n",
    "\n",
    "\n",
    "# --- Load Tokenizer and Fine-tuned Model ---\n",
    "fine_tuned_model_path = \"./fine_tuned_model\" # Path where you saved adapters and tokenizer\n",
    "\n",
    "if not os.path.exists(fine_tuned_model_path) or not os.path.exists(os.path.join(fine_tuned_model_path, \"adapter_model.safetensors\")): # Check for key adapter file\n",
    "    print(f\"Error: Fine-tuned model files not found at {fine_tuned_model_path}\")\n",
    "    print(\"Please ensure the training cell was run and the model was saved successfully.\")\n",
    "    # In a notebook, you might stop here or raise an error.\n",
    "    # In a script, you might sys.exit(1)\n",
    "    tokenizer = None # Ensure tokenizer is None if files not found\n",
    "else:\n",
    "    print(f\"Loading tokenizer from {fine_tuned_model_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Set pad token if not set\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "    base_model_for_peft = None # Initialize\n",
    "    model = None # Initialize\n",
    "    current_device = \"cpu\" # Default device\n",
    "\n",
    "    # Only attempt to load model if tokenizer was loaded\n",
    "    if tokenizer is not None:\n",
    "        print(f\"Loading base model {model_name} and applying PEFT adapters...\")\n",
    "        # Load the base model using the SAME configuration as training setup\n",
    "        # Ensure this matches the setup in your training cell\n",
    "        try:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16, # MUST match training setup\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "\n",
    "            base_model_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                torch_dtype=torch.float16, # MUST match training setup\n",
    "                device_map=\"auto\", # Let HF handle device placement\n",
    "                trust_remote_code=True,\n",
    "                token=token\n",
    "            )\n",
    "            model = PeftModel.from_pretrained(base_model_for_peft, fine_tuned_model_path)\n",
    "            print(\"Fine-tuned model (base + PEFT adapters) loaded.\")\n",
    "\n",
    "            # Determine the actual device the model is on AFTER loading\n",
    "            current_device = str(next(model.parameters()).device) if hasattr(model, 'parameters') and next(model.parameters(), None) is not None else \"cpu\"\n",
    "            print(f\"Fine-tuned model loaded on device: {current_device}\")\n",
    "\n",
    "            # --- Select Samples (repeat selection to be safe if test_data changes) ---\n",
    "            random.seed(42) # Use the same seed to get the same samples\n",
    "            if len(test_data) < 10:\n",
    "                sample_indices = range(len(test_data))\n",
    "                print(f\"Warning: test_data has only {len(test_data)} samples, using all available samples.\")\n",
    "            else:\n",
    "                 sample_indices = random.sample(range(len(test_data)), 10)\n",
    "            test_samples = [test_data[i] for i in sample_indices]\n",
    "            print(f\"Selected {len(test_samples)} samples for inference.\")\n",
    "\n",
    "\n",
    "            # --- Generate Summaries with Fine-tuned Model ---\n",
    "            results_temp = [] # Temporary storage\n",
    "\n",
    "            print(\"\\nGenerating summaries with fine-tuned model...\")\n",
    "            # Ensure test_samples is not empty\n",
    "            if test_samples:\n",
    "                # --- ADJUST max_new_tokens HERE IF NEEDED TO AVOID OOM DURING GENERATION ---\n",
    "                # Example: reducing from 300 to 200\n",
    "                generation_max_tokens = 200 # <-- ADJUST THIS VALUE\n",
    "                print(f\"Using max_new_tokens={generation_max_tokens} for generation.\")\n",
    "\n",
    "                for i, sample in enumerate(test_samples):\n",
    "                    print(f\"Processing sample {i+1}/{len(test_samples)} with fine-tuned model...\")\n",
    "                    article = sample[\"article\"]\n",
    "                    ground_truth = sample[\"abstract\"]\n",
    "\n",
    "                    # Generate summary\n",
    "                    fine_tuned_summary = generate_summary(model, tokenizer, article, max_new_tokens=generation_max_tokens, device=current_device)\n",
    "\n",
    "                    # Store results temporarily\n",
    "                    results_temp.append({\n",
    "                        \"article\": article,\n",
    "                        \"ground_truth\": ground_truth,\n",
    "                        \"fine_tuned_summary\": fine_tuned_summary,\n",
    "                        # base_summary will be added later\n",
    "                    })\n",
    "            else:\n",
    "                print(\"No test samples selected. Skipping fine-tuned inference.\")\n",
    "\n",
    "            print(\"\\nFinished generating summaries with fine-tuned model.\")\n",
    "\n",
    "            # --- Save Temporary Results ---\n",
    "            temp_results_file = \"fine_tuned_results_temp.json\"\n",
    "            if results_temp: # Only save if there are results\n",
    "                print(f\"Saving temporary results to '{temp_results_file}'...\")\n",
    "                with open(temp_results_file, \"w\") as f:\n",
    "                    json.dump(results_temp, f, indent=4)\n",
    "                print(\"Temporary results saved.\")\n",
    "            else:\n",
    "                 print(\"No temporary results to save.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred during fine-tuned model loading or inference: {e}\")\n",
    "            # Print traceback for more details\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"Cell 1 failed.\")\n",
    "\n",
    "        finally:\n",
    "            # --- Release Fine-tuned Model Memory ---\n",
    "            # Ensure variables exist before trying to free\n",
    "            if 'model' in locals() and model is not None:\n",
    "                 print(\"\\nReleasing fine-tuned model memory...\")\n",
    "                 free_model_memory(model)\n",
    "                 model = None # Explicitly set to None\n",
    "\n",
    "            # Also release the base model object used for loading PEFT if it exists and wasn't implicitly freed\n",
    "            if 'base_model_for_peft' in locals() and base_model_for_peft is not None:\n",
    "                 print(\"Releasing temporary base model object used for PEFT...\")\n",
    "                 free_model_memory(base_model_for_peft)\n",
    "                 base_model_for_peft = None\n",
    "\n",
    "            # Keep tokenizer if needed for the base model in the next cell\n",
    "            # If you anticipate the tokenizer also consuming significant memory and need to free it:\n",
    "            # if 'tokenizer' in locals() and tokenizer is not None:\n",
    "            #    free_model_memory(tokenizer) # Implement memory freeing for tokenizer if needed\n",
    "            #    tokenizer = None # Then reload tokenizer in the next cell\n",
    "\n",
    "    else: # If tokenizer loading failed\n",
    "         print(\"Skipping model loading and inference as tokenizer could not be loaded.\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 1 execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28589cea43a45ddbca91e3ea27f39b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n",
      "Processing sample 1/10...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 52.83 GiB. GPU 0 has a total capacity of 44.53 GiB of which 27.88 GiB is free. Process 33247 has 16.64 GiB memory in use. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 5.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Generate summary using fine-tuned model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m fine_tuned_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Generate summary using base model\u001b[39;00m\n\u001b[1;32m     58\u001b[0m base_summary \u001b[38;5;241m=\u001b[39m generate_summary(base_model, article)\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[0;34m(model, article, max_new_tokens)\u001b[0m\n\u001b[1;32m     22\u001b[0m gen_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_new_tokens,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Generate summary\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Extract just the summary part (after \"Summary:\")\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:3431\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:810\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    806\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    807\u001b[0m )\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 810\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:506\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 506\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:624\u001b[0m, in \u001b[0;36mMistralModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    617\u001b[0m     target_length \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    618\u001b[0m         attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m past_seen_tokens \u001b[38;5;241m+\u001b[39m sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_4d_causal_attention_mask_with_cache_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_unmask_unattended(causal_mask, min_dtype)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:690\u001b[0m, in \u001b[0;36mMistralModel._prepare_4d_causal_attention_mask_with_cache_position\u001b[0;34m(attention_mask, sequence_length, target_length, dtype, device, cache_position, batch_size, config, past_key_values)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     min_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 690\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     diagonal_attend_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(target_length, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m>\u001b[39m cache_position\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;66;03m# if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\u001b[39;00m\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;66;03m# the check is needed to verify is current checkpoint was trained with sliding window or not\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 52.83 GiB. GPU 0 has a total capacity of 44.53 GiB of which 27.88 GiB is free. Process 33247 has 16.64 GiB memory in use. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 5.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Load the base model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Select 10 random samples from the test set for generation\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(test_data)), 10)\n",
    "test_samples = [test_data[i] for i in sample_indices]\n",
    "\n",
    "# Function to generate summaries\n",
    "def generate_summary(model, article, max_new_tokens=300):\n",
    "    input_text = create_prompt(article)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Set generation parameters\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, **gen_config)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the summary part (after \"Summary:\")\n",
    "    try:\n",
    "        summary = summary.split(\"Summary:\")[1].strip()\n",
    "    except IndexError:\n",
    "        pass  # If \"Summary:\" is not in the output, use the whole output\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries using both models and store results\n",
    "results = []\n",
    "\n",
    "print(\"Generating summaries...\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"Processing sample {i+1}/10...\")\n",
    "    \n",
    "    # Extract input and ground truth\n",
    "    article = sample[\"article\"]\n",
    "    ground_truth = sample[\"abstract\"]\n",
    "    \n",
    "    # Generate summary using fine-tuned model\n",
    "    fine_tuned_summary = generate_summary(model, article)\n",
    "    \n",
    "    # Generate summary using base model\n",
    "    base_summary = generate_summary(base_model, article)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"article\": article,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"fine_tuned_summary\": fine_tuned_summary,\n",
    "        \"base_summary\": base_summary\n",
    "    })\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"summary_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Summary generation complete. Results saved to 'summary_results.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated summaries\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(result[\"ground_truth\"])\n",
    "    print(\"\\nFine-Tuned Model Summary:\")\n",
    "    print(result[\"fine_tuned_summary\"])\n",
    "    print(\"\\nBase Model Summary:\")\n",
    "    print(result[\"base_summary\"])\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Automatic Evaluation\n",
    "\n",
    "Evaluate the generated summaries using ROUGE, BLEU, and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n",
    "        'rougeL': sum(rougeL_scores) / len(rougeL_scores)\n",
    "    }\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    tokenized_refs = [[nltk.word_tokenize(ref)] for ref in references]\n",
    "    tokenized_hyps = [nltk.word_tokenize(hyp) for hyp in hypotheses]\n",
    "    return corpus_bleu(tokenized_refs, tokenized_hyps)\n",
    "\n",
    "# Function to calculate BERTScore\n",
    "def calculate_bertscore(references, hypotheses):\n",
    "    P, R, F1 = bert_score.score(hypotheses, references, lang=\"en\", verbose=False)\n",
    "    return {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Extract references and hypotheses\n",
    "references = [result[\"ground_truth\"] for result in results]\n",
    "fine_tuned_hypotheses = [result[\"fine_tuned_summary\"] for result in results]\n",
    "base_hypotheses = [result[\"base_summary\"] for result in results]\n",
    "\n",
    "# Calculate metrics for fine-tuned model\n",
    "print(\"Calculating metrics for fine-tuned model...\")\n",
    "fine_tuned_rouge = calculate_rouge(references, fine_tuned_hypotheses)\n",
    "fine_tuned_bleu = calculate_bleu(references, fine_tuned_hypotheses)\n",
    "fine_tuned_bertscore = calculate_bertscore(references, fine_tuned_hypotheses)\n",
    "\n",
    "# Calculate metrics for base model\n",
    "print(\"Calculating metrics for base model...\")\n",
    "base_rouge = calculate_rouge(references, base_hypotheses)\n",
    "base_bleu = calculate_bleu(references, base_hypotheses)\n",
    "base_bertscore = calculate_bertscore(references, base_hypotheses)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics for Fine-Tuned Model:\")\n",
    "print(f\"ROUGE-1: {fine_tuned_rouge['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L: {fine_tuned_rouge['rougeL']:.4f}\")\n",
    "print(f\"BLEU: {fine_tuned_bleu:.4f}\")\n",
    "print(f\"BERTScore F1: {fine_tuned_bertscore['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics for Base Model:\")\n",
    "print(f\"ROUGE-1: {base_rouge['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L: {base_rouge['rougeL']:.4f}\")\n",
    "print(f\"BLEU: {base_bleu:.4f}\")\n",
    "print(f\"BERTScore F1: {base_bertscore['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evaluation results\n",
    "metrics = ['ROUGE-1', 'ROUGE-L', 'BLEU', 'BERTScore F1']\n",
    "fine_tuned_scores = [\n",
    "    fine_tuned_rouge['rouge1'],\n",
    "    fine_tuned_rouge['rougeL'],\n",
    "    fine_tuned_bleu,\n",
    "    fine_tuned_bertscore['f1']\n",
    "]\n",
    "base_scores = [\n",
    "    base_rouge['rouge1'],\n",
    "    base_rouge['rougeL'],\n",
    "    base_bleu,\n",
    "    base_bertscore['f1']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, fine_tuned_scores, width, label='Fine-Tuned Model')\n",
    "rects2 = ax.bar(x + width/2, base_scores, width, label='Base Model')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Automatic Evaluation Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Add values on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LLM-as-a-Judge Evaluation\n",
    "\n",
    "Use a more powerful LLM (via Together.ai API) to evaluate the quality of the generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a summary using LLM-as-a-Judge\n",
    "def evaluate_with_llm(article, summary, model=JUDGE_MODEL):\n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator of academic paper summaries. Given the following input and the summary produced, evaluate the summary on three dimensions:\n",
    "\n",
    "1. Fluency: Is the summary readable and grammatically correct? (Scale: 1-5)\n",
    "2. Factuality: Are the statements in the summary correct, and do they reflect the source content? (Scale: 1-5)\n",
    "3. Coverage: Does the summary include the main problem, method, and key findings? (Scale: 1-5)\n",
    "\n",
    "For each dimension, provide a score from 1 (poor) to 5 (excellent) and a brief justification for the score.\n",
    "\n",
    "Original Paper Text (truncated):\n",
    "```\n",
    "{article[:3000]}...  # Truncate to avoid exceeding token limits\n",
    "```\n",
    "\n",
    "Generated Summary:\n",
    "```\n",
    "{summary}\n",
    "```\n",
    "\n",
    "Your evaluation should be structured as follows:\n",
    "- Fluency: [SCORE] - [JUSTIFICATION]\n",
    "- Factuality: [SCORE] - [JUSTIFICATION]\n",
    "- Coverage: [SCORE] - [JUSTIFICATION]\n",
    "- Overall: [SCORE] - [BRIEF SUMMARY OF STRENGTHS AND WEAKNESSES]\n",
    "\n",
    "Ensure that your scores are justified based on specific observations from the text.\"\"\"\n",
    "\n",
    "    # Call the Together.ai API\n",
    "    response = together.Complete.create(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        top_p=0.8,\n",
    "        top_k=60,\n",
    "        repetition_penalty=1.1,\n",
    "        stop=['\\n\\n\\n']\n",
    "    )\n",
    "    \n",
    "    # Extract the evaluation text\n",
    "    evaluation_text = response['output']['choices'][0]['text']\n",
    "    \n",
    "    # Parse the scores\n",
    "    try:\n",
    "        fluency_score = int(evaluation_text.split(\"Fluency:\")[1].split(\"-\")[0].strip())\n",
    "        factuality_score = int(evaluation_text.split(\"Factuality:\")[1].split(\"-\")[0].strip())\n",
    "        coverage_score = int(evaluation_text.split(\"Coverage:\")[1].split(\"-\")[0].strip())\n",
    "        \n",
    "        # Calculate overall score (the average of the three scores)\n",
    "        overall_score = (fluency_score + factuality_score + coverage_score) / 3\n",
    "        \n",
    "        return {\n",
    "            \"fluency\": fluency_score,\n",
    "            \"factuality\": factuality_score,\n",
    "            \"coverage\": coverage_score,\n",
    "            \"overall\": overall_score,\n",
    "            \"full_evaluation\": evaluation_text\n",
    "        }\n",
    "    except:\n",
    "        # If parsing fails, return the full evaluation text\n",
    "        return {\n",
    "            \"fluency\": None,\n",
    "            \"factuality\": None,\n",
    "            \"coverage\": None,\n",
    "            \"overall\": None,\n",
    "            \"full_evaluation\": evaluation_text\n",
    "        }\n",
    "\n",
    "# Evaluate a subset of the generated summaries (5 samples for budget reasons)\n",
    "evaluation_subset = results[:5]  # Use the first 5 samples\n",
    "\n",
    "# Evaluate fine-tuned model summaries\n",
    "fine_tuned_evaluations = []\n",
    "for i, result in enumerate(evaluation_subset):\n",
    "    print(f\"Evaluating fine-tuned model summary {i+1}/5...\")\n",
    "    evaluation = evaluate_with_llm(result[\"article\"], result[\"fine_tuned_summary\"])\n",
    "    fine_tuned_evaluations.append(evaluation)\n",
    "    time.sleep(2)  # To avoid rate limiting\n",
    "\n",
    "# Evaluate base model summaries\n",
    "base_evaluations = []\n",
    "for i, result in enumerate(evaluation_subset):\n",
    "    print(f\"Evaluating base model summary {i+1}/5...\")\n",
    "    evaluation = evaluate_with_llm(result[\"article\"], result[\"base_summary\"])\n",
    "    base_evaluations.append(evaluation)\n",
    "    time.sleep(2)  # To avoid rate limiting\n",
    "\n",
    "# Save the evaluations to a JSON file\n",
    "with open(\"llm_evaluations.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"fine_tuned\": fine_tuned_evaluations,\n",
    "        \"base\": base_evaluations\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"LLM evaluation complete. Results saved to 'llm_evaluations.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores\n",
    "fine_tuned_avg = {\n",
    "    \"fluency\": sum(eval[\"fluency\"] for eval in fine_tuned_evaluations if eval[\"fluency\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"fluency\"]),\n",
    "    \"factuality\": sum(eval[\"factuality\"] for eval in fine_tuned_evaluations if eval[\"factuality\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"factuality\"]),\n",
    "    \"coverage\": sum(eval[\"coverage\"] for eval in fine_tuned_evaluations if eval[\"coverage\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"coverage\"]),\n",
    "    \"overall\": sum(eval[\"overall\"] for eval in fine_tuned_evaluations if eval[\"overall\"]) / sum(1 for eval in fine_tuned_evaluations if eval[\"overall\"])\n",
    "}\n",
    "\n",
    "base_avg = {\n",
    "    \"fluency\": sum(eval[\"fluency\"] for eval in base_evaluations if eval[\"fluency\"]) / sum(1 for eval in base_evaluations if eval[\"fluency\"]),\n",
    "    \"factuality\": sum(eval[\"factuality\"] for eval in base_evaluations if eval[\"factuality\"]) / sum(1 for eval in base_evaluations if eval[\"factuality\"]),\n",
    "    \"coverage\": sum(eval[\"coverage\"] for eval in base_evaluations if eval[\"coverage\"]) / sum(1 for eval in base_evaluations if eval[\"coverage\"]),\n",
    "    \"overall\": sum(eval[\"overall\"] for eval in base_evaluations if eval[\"overall\"]) / sum(1 for eval in base_evaluations if eval[\"overall\"])\n",
    "}\n",
    "\n",
    "# Print average scores\n",
    "print(\"LLM-as-a-Judge Average Scores:\")\n",
    "print(\"\\nFine-Tuned Model:\")\n",
    "print(f\"Fluency: {fine_tuned_avg['fluency']:.2f}\")\n",
    "print(f\"Factuality: {fine_tuned_avg['factuality']:.2f}\")\n",
    "print(f\"Coverage: {fine_tuned_avg['coverage']:.2f}\")\n",
    "print(f\"Overall: {fine_tuned_avg['overall']:.2f}\")\n",
    "\n",
    "print(\"\\nBase Model:\")\n",
    "print(f\"Fluency: {base_avg['fluency']:.2f}\")\n",
    "print(f\"Factuality: {base_avg['factuality']:.2f}\")\n",
    "print(f\"Coverage: {base_avg['coverage']:.2f}\")\n",
    "print(f\"Overall: {base_avg['overall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LLM-as-a-Judge evaluation results\n",
    "categories = ['Fluency', 'Factuality', 'Coverage', 'Overall']\n",
    "fine_tuned_scores = [fine_tuned_avg['fluency'], fine_tuned_avg['factuality'], fine_tuned_avg['coverage'], fine_tuned_avg['overall']]\n",
    "base_scores = [base_avg['fluency'], base_avg['factuality'], base_avg['coverage'], base_avg['overall']]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, fine_tuned_scores, width, label='Fine-Tuned Model')\n",
    "rects2 = ax.bar(x + width/2, base_scores, width, label='Base Model')\n",
    "\n",
    "ax.set_ylabel('Score (1-5)')\n",
    "ax.set_title('LLM-as-a-Judge Evaluation Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 5.5)  # Set y-axis limits to 0-5\n",
    "ax.legend()\n",
    "\n",
    "# Add values on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('llm_evaluation_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streamlit Interface Development\n",
    "\n",
    "Create a Streamlit interface for the summarization system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pdfplumber\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Smart Summarizer\",\n",
    "    page_icon=\"\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title and description\n",
    "st.title(\" Smart Summarizer for Academic Papers\")\n",
    "st.markdown(\"\"\"\n",
    "This application uses a fine-tuned language model to generate concise and accurate summaries of academic papers.\n",
    "Upload a PDF file or paste the text of an academic paper, and get an AI-generated summary in seconds.\n",
    "\"\"\")\n",
    "\n",
    "# Set up the Together.ai API key\n",
    "@st.cache_resource\n",
    "def load_api_key():\n",
    "    return os.environ.get(\"TOGETHER_API_KEY\", \"\")\n",
    "\n",
    "# Load the API key\n",
    "api_key = load_api_key()\n",
    "\n",
    "# Create a sidebar for API key input if not already set\n",
    "if not api_key:\n",
    "    api_key = st.sidebar.text_input(\"Enter Together.ai API Key\", type=\"password\")\n",
    "    if api_key:\n",
    "        os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
    "\n",
    "# Function to load models\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    # Model and tokenizer details\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"  # Use the same model as in training\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True if torch.cuda.is_available() else False,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    if os.path.exists(\"./fine_tuned_model\"):\n",
    "        fine_tuned_model = PeftModel.from_pretrained(base_model, \"./fine_tuned_model\")\n",
    "    else:\n",
    "        fine_tuned_model = base_model  # Fallback to base model if fine-tuned model is not available\n",
    "    \n",
    "    return tokenizer, base_model, fine_tuned_model, device\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to generate a summary\n",
    "def generate_summary(model, tokenizer, text, max_new_tokens=300, device=\"cpu\"):\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Summarize the following academic paper:\n",
    "    \n",
    "Article: {text}\n",
    "    \n",
    "Summary: \"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Set generation parameters\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    # Generate summary\n",
    "    output = model.generate(input_ids, **gen_config)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the summary part (after \"Summary:\")\n",
    "    try:\n",
    "        summary = summary.split(\"Summary:\")[1].strip()\n",
    "    except IndexError:\n",
    "        pass  # If \"Summary:\" is not in the output, use the whole output\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Function to evaluate a summary using LLM-as-a-Judge\n",
    "def evaluate_with_llm(article, summary, api_key):\n",
    "    if not api_key:\n",
    "        return \"Please enter a Together.ai API key to use the LLM-as-a-Judge feature.\"\n",
    "    \n",
    "    # Trim the article to avoid exceeding token limits\n",
    "    article = article[:3000] + \"...\" if len(article) > 3000 else article\n",
    "    \n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator of academic paper summaries. Given the following input and the summary produced, evaluate the summary on three dimensions:\n",
    "\n",
    "1. Fluency: Is the summary readable and grammatically correct? (Scale: 1-5)\n",
    "2. Factuality: Are the statements in the summary correct, and do they reflect the source content? (Scale: 1-5)\n",
    "3. Coverage: Does the summary include the main problem, method, and key findings? (Scale: 1-5)\n",
    "\n",
    "For each dimension, provide a score from 1 (poor) to 5 (excellent) and a brief justification for the score.\n",
    "\n",
    "Original Paper Text (truncated):\n",
    "```\n",
    "{article}\n",
    "```\n",
    "\n",
    "Generated Summary:\n",
    "```\n",
    "{summary}\n",
    "```\n",
    "\n",
    "Your evaluation should be structured as follows:\n",
    "- Fluency: [SCORE] - [JUSTIFICATION]\n",
    "- Factuality: [SCORE] - [JUSTIFICATION]\n",
    "- Coverage: [SCORE] - [JUSTIFICATION]\n",
    "- Overall: [SCORE] - [BRIEF SUMMARY OF STRENGTHS AND WEAKNESSES]\n",
    "\n",
    "Ensure that your scores are justified based on specific observations from the text.\"\"\"\n",
    "    \n",
    "    # Call the Together.ai API\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 60,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"stop\": [\"\\n\\n\\n\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.together.xyz/v1/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"text\"]\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Main application\n",
    "def main():\n",
    "    # Load models\n",
    "    with st.spinner(\"Loading models... This may take a minute...\"):\n",
    "        tokenizer, base_model, fine_tuned_model, device = load_models()\n",
    "    \n",
    "    # Create tabs for different input methods\n",
    "    tab1, tab2 = st.tabs([\"Upload PDF\", \"Paste Text\"])\n",
    "    \n",
    "    with tab1:\n",
    "        # File uploader for PDF\n",
    "        uploaded_file = st.file_uploader(\"Upload an academic paper (PDF)\", type=\"pdf\")\n",
    "        if uploaded_file is not None:\n",
    "            with st.spinner(\"Extracting text from PDF...\"):\n",
    "                pdf_bytes = io.BytesIO(uploaded_file.getvalue())\n",
    "                text = extract_text_from_pdf(pdf_bytes)\n",
    "                st.session_state.input_text = text\n",
    "                st.session_state.show_input = True\n",
    "    \n",
    "    with tab2:\n",
    "        # Text input\n",
    "        text_input = st.text_area(\"Paste the text of an academic paper\", height=300)\n",
    "        if text_input:\n",
    "            st.session_state.input_text = text_input\n",
    "            st.session_state.show_input = True\n",
    "    \n",
    "    # Initialize session state variables if they don't exist\n",
    "    if \"input_text\" not in st.session_state:\n",
    "        st.session_state.input_text = \"\"\n",
    "    if \"show_input\" not in st.session_state:\n",
    "        st.session_state.show_input = False\n",
    "    if \"fine_tuned_summary\" not in st.session_state:\n",
    "        st.session_state.fine_tuned_summary = \"\"\n",
    "    if \"base_summary\" not in st.session_state:\n",
    "        st.session_state.base_summary = \"\"\n",
    "    if \"evaluation\" not in st.session_state:\n",
    "        st.session_state.evaluation = \"\"\n",
    "    \n",
    "    # Display input text and generate summaries\n",
    "    if st.session_state.show_input and st.session_state.input_text:\n",
    "        with st.expander(\"View Input Text\", expanded=False):\n",
    "            st.text_area(\"Input Text\", st.session_state.input_text, height=200, disabled=True)\n",
    "        \n",
    "        # Create columns for the buttons\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        # Generate summary with fine-tuned model\n",
    "        if col1.button(\"Generate Summary (Fine-Tuned Model)\"):\n",
    "            with st.spinner(\"Generating summary with fine-tuned model...\"):\n",
    "                st.session_state.fine_tuned_summary = generate_summary(\n",
    "                    fine_tuned_model, tokenizer, st.session_state.input_text, max_new_tokens=300, device=device\n",
    "                )\n",
    "        \n",
    "        # Generate summary with base model\n",
    "        if col2.button(\"Generate Summary (Base Model)\"):\n",
    "            with st.spinner(\"Generating summary with base model...\"):\n",
    "                st.session_state.base_summary = generate_summary(\n",
    "                    base_model, tokenizer, st.session_state.input_text, max_new_tokens=300, device=device\n",
    "                )\n",
    "        \n",
    "        # Evaluate summary with LLM-as-a-Judge\n",
    "        if col3.button(\"Evaluate with LLM-as-a-Judge\") and st.session_state.fine_tuned_summary:\n",
    "            if not api_key:\n",
    "                st.warning(\"Please enter a Together.ai API key in the sidebar to use the LLM-as-a-Judge feature.\")\n",
    "            else:\n",
    "                with st.spinner(\"Evaluating summary...\"):\n",
    "                    st.session_state.evaluation = evaluate_with_llm(\n",
    "                        st.session_state.input_text, st.session_state.fine_tuned_summary, api_key\n",
    "                    )\n",
    "        \n",
    "        # Display summaries and evaluation\n",
    "        if st.session_state.fine_tuned_summary or st.session_state.base_summary:\n",
    "            st.markdown(\"### Generated Summaries\")\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.markdown(\"#### Fine-Tuned Model Summary\")\n",
    "                st.text_area(\"Fine-Tuned Summary\", st.session_state.fine_tuned_summary, height=300, disabled=True)\n",
    "            \n",
    "            with col2:\n",
    "                st.markdown(\"#### Base Model Summary\")\n",
    "                st.text_area(\"Base Summary\", st.session_state.base_summary, height=300, disabled=True)\n",
    "        \n",
    "        # Display evaluation\n",
    "        if st.session_state.evaluation:\n",
    "            st.markdown(\"### LLM-as-a-Judge Evaluation\")\n",
    "            st.markdown(st.session_state.evaluation)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Streamlit app, execute the following command in the terminal:\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "In this notebook, we have implemented a complete Smart Summarizer pipeline for academic research papers. We:\n",
    "\n",
    "1. Loaded and preprocessed the arXiv summarization dataset\n",
    "2. Fine-tuned a pre-trained language model (Mistral 7B) using Low-Rank Adaptation (LoRA)\n",
    "3. Generated summaries using both the fine-tuned model and the base model\n",
    "4. Evaluated the summaries using automatic metrics (ROUGE, BLEU, BERTScore) and LLM-as-a-Judge qualitative evaluation\n",
    "5. Developed a Streamlit interface for the summarization system\n",
    "\n",
    "The evaluation results show that the fine-tuned model generally outperforms the base model in generating accurate and relevant summaries of academic papers, demonstrating the effectiveness of LoRA fine-tuning for domain-specific tasks like academic paper summarization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
