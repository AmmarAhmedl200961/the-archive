{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7SiAjeAzgZE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lQECgZd606kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP\n",
        "NLP stands for Natural Language Processing. It is the branch of Artificial Intelligence that gives the ability to machine understand and process human languages. Human languages can be in the form of text or audio format.\n",
        "\n",
        "# What is text pre-processing?\n",
        "\n",
        "Text pre-processing is the process of transforming unstructured text to structured text to prepare it for analysis.\n",
        "\n",
        "When you pre-process text before feeding it to algorithms, you increase the accuracy and efficiency of said algorithms by removing noise and other inconsistencies in the text that can make it hard for the computer to understand.\n",
        "\n",
        "Making the text easier to understand also helps to reduce the time and resources required for the computer to pre-process data.\n",
        "\n",
        "Processes involved in text pre-processing\n",
        "To properly pre-process your text and get it in the right state to perform further analysis and actions with it, there are quite a few operations that need to be done on the text and a couple of steps to be followed to get a well structured text.\n",
        "\n",
        "#Tokenization\n",
        "Tokenization is the first stage of the process.\n",
        "\n",
        "Here your text is analysed and then broken down into chunks called ‘tokens’ which can either be words or phrases. This allows the computer to work on your text token by token rather than working on the entire text in the following stages.\n",
        "\n",
        "The two main types of tokenisation are word and sentence tokenisation.\n",
        "\n",
        "Word tokenisation is the most common kind of tokenisation.\n",
        "\n",
        "Here, each token is a word, meaning the algorithm breaks down the entire text into individual words:"
      ],
      "metadata": {
        "id": "-1pkvB5V07zy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoIDLMKk5b8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Wisdoms daughter walks alone. The mark of Athena burns through rome'\n",
        "\n",
        "words = text.split()\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqrGhNKk2AB5",
        "outputId": "501d5717-3192-4658-cea7-37176a9027bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wisdoms', 'daughter', 'walks', 'alone.', 'The', 'mark', 'of', 'Athena', 'burns', 'through', 'rome']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, sentence tokenisation breaks down text into sentences instead of words. It is a less common type of tokenisation only used in few Natural Language Processing (NLP) tasks.\n",
        "\n",
        "# Case normalisation\n",
        "\n",
        "This technique converts all the letters in your text to a single case, either uppercase or lowercase.\n",
        "\n",
        "Case normalisation ensures that your data is stored in a consistent format and makes it easier to work with the data."
      ],
      "metadata": {
        "id": "3sH4R1sa2I-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"'To Sleep Or NOT to SLEep, THAT is THe Question'\"\n",
        "\n",
        "def lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "lower_case = lower_case(text)#converts everthing to lowercase\n",
        "print(lower_case)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reCcu6Xn2YU3",
        "outputId": "8d5808f9-c023-44f7-8b19-c7a27842a032"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'to sleep or not to sleep, that is the question'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Stemming words like coding, coder, and coded all have the same base word which is code.\n",
        "\n",
        "ML models most-often-than-not understand that these words are all derived from one base word. They can work with your text without the tenses, prefixes, and suffixes that we as humans would normally need to make sense of it.\n",
        "\n",
        "Stemming your texts not only helps to reduce the number of words the model has to work with, and by extension improves the efficiency of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "s7QAsfjk3hvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example text\n",
        "text = \"She enjoys coding, coded many projects, and is a skilled coder.\"\n",
        "\n",
        "# Tokenize and stem each word\n",
        "stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaMHc4NF3ke0",
        "outputId": "785ffd6c-2d0a-4706-a0f7-bbf2eae42493"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['she', 'enjoy', 'coding,', 'code', 'mani', 'projects,', 'and', 'is', 'a', 'skill', 'coder.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatisation\n",
        "This method is very similar to stemming in that it is also used to identify the base of words. It is however a more complex and accurate technique than stemming.\n",
        "\n",
        "Lemmatization, unlike stemming, reduces words to their base or dictionary form (lemma), ensuring the root word remains meaningful."
      ],
      "metadata": {
        "id": "H3PI3ak_3k7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required NLTK data, like WordNet and others\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional, for enhanced language support\n",
        "nltk.download('punkt')     # If you need tokenizers\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example text\n",
        "text = \"She enjoys coding, coded many projects, and is a skilled coder.\"\n",
        "\n",
        "# Tokenize and lemmatize each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in text.split()]\n",
        "\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdNtVUux30UY",
        "outputId": "ee544d38-42b2-4d4d-84ed-82fb97009b3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['She', 'enjoy', 'coding,', 'cod', 'many', 'projects,', 'and', 'be', 'a', 'skilled', 'coder.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punctuation removal\n",
        "\n",
        "During human conversations, punctuation marks like `‘’, ! , [, }, *, #, /, ?, and ‘’` are incredibly relevant and necessary to have a proper conversation. Thelp to fully convey the message of the writer."
      ],
      "metadata": {
        "id": "XgOiT66I2eqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = ' (to love is to destroy, and to be loved, is to be \"the\" one <destroyed>} '\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    punctuation = re.compile(r'[{};():,.\"/<>-]')\n",
        "    text = punctuation.sub(' ', text)\n",
        "    return text\n",
        "\n",
        "clean_text = remove_punctuations(text)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWEm1LZF2hto",
        "outputId": "ca30a67b-1a5e-4eda-87e7-465d17eccd8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  to love is to destroy  and to be loved  is to be  the  one  destroyed   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accent removal\n",
        "This process is about removing language specific character symbols from text.\n",
        "\n",
        "Some characters are written with specific accents or symbols to either imply a different pronunciation or to signify that words containing such accented texts have a different meaning."
      ],
      "metadata": {
        "id": "vi5c8u5i3AQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"her fiancé's résumé is beautiful\"\n",
        "\n",
        "def remove_accents(text):\n",
        "    accents = re.compile(u\"[\\u0300-\\u036F]|é|è\")\n",
        "    text = accents.sub(u\"e\", text)\n",
        "    return text\n",
        "\n",
        "cleaned_text = remove_accents(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok_mUmUf3DyG",
        "outputId": "fcff905b-c1e9-4f76-f53d-d507203b10aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "her fiance's resume is beautiful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Tasks\n",
        "### **Note** you will perform all the task using dataset.txt file\n",
        "\n",
        "# Task 1 Read the Dataset from a File:\n",
        "In this task you will read the dataset from a text file (dataset.txt)."
      ],
      "metadata": {
        "id": "vWopYLM3t12m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open('datset.txt', 'r') as file:\n",
        "  dataset = file.readlines()\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bow6L7utxaV",
        "outputId": "a88acbc1-d3af-4793-cdea-e07f4790cc6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The car is driven on the road.\\n', 'The truck is parked in the lot.\\n', 'This pasta is delicious and affordable.\\n', 'I enjoy coding in Python.\\n', 'Artificial Intelligence is transforming industries.\\n', 'The weather is sunny today.\\n', 'She loves reading books.\\n', 'The cake tastes amazing.\\n', 'Learning new things every day is fulfilling.\\n', 'The sky is clear and blue.\\n', 'The dog chased the cat around the yard.\\n', 'He is studying for his final exams.\\n', 'The phone battery is running low.\\n', 'Nature has a calming effect on the mind.\\n', 'The coffee is too hot to drink right now.\\n', 'She enjoys painting landscapes in her free time.\\n', 'The train arrived at the station early.\\n', 'He is preparing a presentation for work.\\n', 'I am excited about the upcoming event.\\n', 'The movie was thrilling and full of suspense.\\n', 'Running in the park is a great way to start the day.\\n', 'The artist painted a beautiful portrait of the woman.\\n', 'She is learning French in her spare time.\\n', 'The garden is blooming with colorful flowers.\\n', 'The bird sang a melodious tune in the morning.\\n', 'The conference is scheduled for next week.\\n', 'He is working on a new programming project.\\n', 'The soup is warm and comforting on a cold day.\\n', 'The tree swayed gently in the breeze.\\n', 'She is practicing yoga to improve her flexibility.\\n', 'The car needs a wash after the long road trip.\\n', 'He is reading a mystery novel that he finds intriguing.\\n', 'The sunset over the ocean was breathtaking.\\n', 'She is studying the behavior of marine animals.\\n', 'The bakery sells freshly baked bread every morning.\\n', 'He is organizing a charity event to help the community.\\n', 'The restaurant serves a variety of international cuisines.\\n', 'She is learning to play the guitar in her free time.\\n', 'The mountain view from the cabin is stunning.\\n', 'He is training for the marathon next month.\\n', 'The river flows peacefully through the valley.\\n', 'The city is known for its vibrant nightlife.\\n', 'She is baking cookies for the neighborhood children.\\n', 'The garden is filled with the fragrance of roses.\\n', 'The coffee shop is always crowded in the mornings.\\n', 'The wind is howling through the trees on this stormy night.\\n', 'The music festival attracted people from all over the country.\\n', 'The airplane is flying at an altitude of 30,000 feet.\\n', 'She is writing a book about her travel experiences.\\n', 'The road trip to the mountains was filled with adventure.\\n', 'He is building a new deck for his backyard.\\n', 'The software update fixed several bugs in the system.\\n', 'The library is quiet and perfect for studying.\\n', 'She is volunteering at the animal shelter on weekends.\\n', 'The beach is a popular destination during the summer.\\n', 'He is assembling a model airplane in his workshop.\\n', 'The chef prepared a gourmet meal for the guests.\\n', 'The sunrise over the hills was spectacular.\\n', 'She is planning a surprise party for her best friend.\\n', 'The museum exhibits ancient artifacts from various cultures.\\n', 'The forest is home to a diverse range of wildlife.\\n', 'The students are preparing for the upcoming exams.\\n', 'The car engine needs a tune-up to run smoothly.\\n', 'She is knitting a scarf for the winter season.\\n', 'The project deadline is approaching fast.\\n', 'The dog is barking loudly at the passing car.\\n', 'He is building a robot for the science fair.\\n', 'The flowers in the garden are in full bloom.\\n', 'She is exploring new career opportunities in the tech industry.\\n', 'The concert was an unforgettable experience.\\n', 'The computer crashed and lost all the unsaved work.\\n', 'The marathon runners are pushing their limits to the finish line.\\n', 'The bakery is known for its delicious pastries.\\n', 'She is organizing a charity auction for a good cause.\\n', 'The stars in the night sky were shining brightly.\\n', 'He is renovating his house to create more space.\\n', 'The team is working hard to meet the project deadline.\\n', 'The coffee machine is broken and needs to be repaired.\\n', 'The bicycle ride through the countryside was refreshing.\\n', 'The bookstore offers a wide selection of novels and non-fiction.\\n', \"She is preparing a presentation for her company's board meeting.\\n\", 'The rain is pouring down heavily on the roof.\\n', 'The piano recital was a success, and the audience applauded.\\n', 'The drone captured stunning aerial footage of the landscape.\\n', 'He is designing a website for his new business.\\n', 'The holiday season is a time for family gatherings and celebrations.\\n', 'The train journey through the mountains was breathtaking.\\n', 'She is learning to code in Python to enhance her skills.\\n', 'The smartphone app has many useful features.\\n', 'The festival featured music, food, and cultural performances.\\n', 'The wildlife photographer captured rare images of the animals.\\n', 'He is saving money for his dream vacation.\\n', 'The waves crashed against the shore during the storm.\\n', 'The theater was packed for the opening night of the play.\\n', \"She is baking a cake for her sister's birthday.\\n\", 'The airplane landed safely despite the turbulence.\\n', 'The garden is a peaceful retreat from the busy city.\\n', 'He is practicing his presentation for the upcoming meeting.\\n', 'The store is offering a discount on all electronics this weekend.\\n', 'The city park is a great place for a picnic.\\n', 'The football team is preparing for the championship game.\\n', 'She is writing a blog about her experiences as a digital nomad.\\n', 'The sun is setting behind the mountains, casting a golden glow.\\n', 'He is painting the walls of his living room a bright color.\\n', 'The yoga class helped her feel relaxed and rejuvenated.\\n', 'The construction workers are building a new skyscraper downtown.\\n', 'The new restaurant in town is getting rave reviews.\\n', 'She is organizing a community cleanup event in her neighborhood.\\n', 'The train station is bustling with commuters during rush hour.\\n', 'He is studying the effects of climate change on polar ice caps.\\n', 'The book he is reading is a science fiction novel about space travel.\\n', 'The garden is full of butterflies and bees.\\n', 'She is practicing for her upcoming piano recital.\\n', 'The technology conference attracted attendees from around the world.\\n', 'He is upgrading his computer with the latest hardware components.\\n', 'The beach is a perfect place to relax and unwind.\\n', 'She is volunteering at the food bank to help those in need.\\n', 'The mountain trail offers breathtaking views of the surrounding landscape.\\n', 'The new software release includes several improvements and features.\\n', 'The stars are visible in the clear night sky.\\n', 'He is working on a new app that tracks fitness goals.\\n', 'The library is hosting a workshop on creative writing.\\n', 'She is planning a road trip with her friends next month.\\n', 'The farmer is planting crops for the upcoming harvest season.\\n', 'The waterfall cascades down the rocky cliff into the river below.\\n', 'The museum is showcasing a collection of modern art pieces.\\n', \"He is preparing a speech for his cousin's wedding.\\n\", 'The sunset over the ocean was a beautiful end to the day.\\n', 'The new movie is getting positive reviews from critics.\\n', 'She is decorating her home for the holiday season.\\n', 'The soccer match was thrilling, with both teams playing their best.\\n', 'He is learning a new programming language for his job.\\n', 'The bakery smells of freshly baked bread and pastries.\\n', 'The river is flowing swiftly after the heavy rain.\\n', 'She is writing a poem about the changing seasons.\\n', 'The airplane took off smoothly and reached cruising altitude quickly.\\n', 'The park is filled with children playing on the swings and slides.\\n', 'He is planning to start his own business next year.\\n', 'The coffee shop has a cozy atmosphere, perfect for reading a book.\\n', 'The wildlife reserve is home to many endangered species.\\n', 'She is sewing a quilt as a gift for her grandmother.\\n', 'The car broke down on the way to the airport.\\n', 'The team is brainstorming ideas for the new project.\\n', 'The dog is wagging its tail happily as it runs in the yard.\\n', 'She is learning to play the piano with the help of a tutor.\\n', 'The restaurant serves delicious homemade pasta.\\n', 'He is building a treehouse for his children in the backyard.\\n', 'The sun is shining brightly on this warm summer day.\\n', 'She is packing her suitcase for her upcoming vacation.\\n', 'The bookstore is having a sale on all hardcover books.\\n', 'The garden is filled with the scent of blooming flowers.\\n', 'He is organizing his office to make it more efficient.\\n', 'The movie was so exciting that everyone in the audience was on edge.\\n', 'The chef is experimenting with new recipes for the menu.\\n', 'She is taking a photography class to improve her skills.\\n', 'The train is running late due to the heavy rain.\\n', 'He is repairing the roof of his house after the storm.\\n', 'The yoga class focuses on breathing techniques and relaxation.\\n', 'The city skyline is breathtaking at night with all the lights.\\n', 'She is writing a novel about her travels around the world.\\n', 'The football team won the championship game in a thrilling finish.\\n', 'He is studying the history of ancient civilizations for his research paper.\\n', 'The beach is crowded with people enjoying the sunny weather.\\n', 'She is knitting a sweater for her newborn niece.\\n', 'The car dealership is offering discounts on all new models.\\n', 'He is designing a new logo for his company.\\n', 'The weather forecast predicts snow for the weekend.\\n', 'The library is quiet, a perfect place to study.\\n', 'She is taking dance lessons to prepare for her wedding.\\n', 'The construction of the new bridge is ahead of schedule.\\n', 'The movie premiere was attended by many famous celebrities.\\n', 'He is training for a triathlon that includes swimming, cycling, and running.\\n', 'The art gallery is displaying a collection of abstract paintings.\\n', 'She is planting flowers in her garden to attract butterflies.\\n', 'The airplane is flying over the mountains on its way to the destination.\\n', 'He is learning to play the guitar by watching online tutorials.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3xB_lqcuUGm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YaaE1oSKtw1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Text Pre-processing and Tokenization\n",
        "Given a document, perform text cleaning (remove HTML tags, emojis, and special characters), convert text to lowercase, and then tokenize it into words.\n"
      ],
      "metadata": {
        "id": "Q-ewHJVIpTa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhM0WhR7-_sg",
        "outputId": "294452fe-c938-4278-c81b-49411b04132a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading emoji-2.13.2-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "  \"\"\"Cleans and tokenizes text.\n",
        "\n",
        "  Args:\n",
        "    text: The input text.\n",
        "\n",
        "  Returns:\n",
        "    A list of tokens.\n",
        "  \"\"\"\n",
        "  # Remove HTML tags\n",
        "  text = re.sub('<[^<]+?>|[{};():,.\"/<>-]', '', text)\n",
        "\n",
        "  # Remove emojis and special characters\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize into words\n",
        "  tokens = text.split()\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "for line in dataset:\n",
        "    tokens = clean_and_tokenize(line)\n",
        "    print(tokens)"
      ],
      "metadata": {
        "id": "aVuUbiCdthaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aec2ef2-09cb-486b-9627-9ebbd9dabdcb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'car', 'is', 'driven', 'on', 'the', 'road']\n",
            "['the', 'truck', 'is', 'parked', 'in', 'the', 'lot']\n",
            "['this', 'pasta', 'is', 'delicious', 'and', 'affordable']\n",
            "['i', 'enjoy', 'coding', 'in', 'python']\n",
            "['artificial', 'intelligence', 'is', 'transforming', 'industries']\n",
            "['the', 'weather', 'is', 'sunny', 'today']\n",
            "['she', 'loves', 'reading', 'books']\n",
            "['the', 'cake', 'tastes', 'amazing']\n",
            "['learning', 'new', 'things', 'every', 'day', 'is', 'fulfilling']\n",
            "['the', 'sky', 'is', 'clear', 'and', 'blue']\n",
            "['the', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'yard']\n",
            "['he', 'is', 'studying', 'for', 'his', 'final', 'exams']\n",
            "['the', 'phone', 'battery', 'is', 'running', 'low']\n",
            "['nature', 'has', 'a', 'calming', 'effect', 'on', 'the', 'mind']\n",
            "['the', 'coffee', 'is', 'too', 'hot', 'to', 'drink', 'right', 'now']\n",
            "['she', 'enjoys', 'painting', 'landscapes', 'in', 'her', 'free', 'time']\n",
            "['the', 'train', 'arrived', 'at', 'the', 'station', 'early']\n",
            "['he', 'is', 'preparing', 'a', 'presentation', 'for', 'work']\n",
            "['i', 'am', 'excited', 'about', 'the', 'upcoming', 'event']\n",
            "['the', 'movie', 'was', 'thrilling', 'and', 'full', 'of', 'suspense']\n",
            "['running', 'in', 'the', 'park', 'is', 'a', 'great', 'way', 'to', 'start', 'the', 'day']\n",
            "['the', 'artist', 'painted', 'a', 'beautiful', 'portrait', 'of', 'the', 'woman']\n",
            "['she', 'is', 'learning', 'french', 'in', 'her', 'spare', 'time']\n",
            "['the', 'garden', 'is', 'blooming', 'with', 'colorful', 'flowers']\n",
            "['the', 'bird', 'sang', 'a', 'melodious', 'tune', 'in', 'the', 'morning']\n",
            "['the', 'conference', 'is', 'scheduled', 'for', 'next', 'week']\n",
            "['he', 'is', 'working', 'on', 'a', 'new', 'programming', 'project']\n",
            "['the', 'soup', 'is', 'warm', 'and', 'comforting', 'on', 'a', 'cold', 'day']\n",
            "['the', 'tree', 'swayed', 'gently', 'in', 'the', 'breeze']\n",
            "['she', 'is', 'practicing', 'yoga', 'to', 'improve', 'her', 'flexibility']\n",
            "['the', 'car', 'needs', 'a', 'wash', 'after', 'the', 'long', 'road', 'trip']\n",
            "['he', 'is', 'reading', 'a', 'mystery', 'novel', 'that', 'he', 'finds', 'intriguing']\n",
            "['the', 'sunset', 'over', 'the', 'ocean', 'was', 'breathtaking']\n",
            "['she', 'is', 'studying', 'the', 'behavior', 'of', 'marine', 'animals']\n",
            "['the', 'bakery', 'sells', 'freshly', 'baked', 'bread', 'every', 'morning']\n",
            "['he', 'is', 'organizing', 'a', 'charity', 'event', 'to', 'help', 'the', 'community']\n",
            "['the', 'restaurant', 'serves', 'a', 'variety', 'of', 'international', 'cuisines']\n",
            "['she', 'is', 'learning', 'to', 'play', 'the', 'guitar', 'in', 'her', 'free', 'time']\n",
            "['the', 'mountain', 'view', 'from', 'the', 'cabin', 'is', 'stunning']\n",
            "['he', 'is', 'training', 'for', 'the', 'marathon', 'next', 'month']\n",
            "['the', 'river', 'flows', 'peacefully', 'through', 'the', 'valley']\n",
            "['the', 'city', 'is', 'known', 'for', 'its', 'vibrant', 'nightlife']\n",
            "['she', 'is', 'baking', 'cookies', 'for', 'the', 'neighborhood', 'children']\n",
            "['the', 'garden', 'is', 'filled', 'with', 'the', 'fragrance', 'of', 'roses']\n",
            "['the', 'coffee', 'shop', 'is', 'always', 'crowded', 'in', 'the', 'mornings']\n",
            "['the', 'wind', 'is', 'howling', 'through', 'the', 'trees', 'on', 'this', 'stormy', 'night']\n",
            "['the', 'music', 'festival', 'attracted', 'people', 'from', 'all', 'over', 'the', 'country']\n",
            "['the', 'airplane', 'is', 'flying', 'at', 'an', 'altitude', 'of', '30000', 'feet']\n",
            "['she', 'is', 'writing', 'a', 'book', 'about', 'her', 'travel', 'experiences']\n",
            "['the', 'road', 'trip', 'to', 'the', 'mountains', 'was', 'filled', 'with', 'adventure']\n",
            "['he', 'is', 'building', 'a', 'new', 'deck', 'for', 'his', 'backyard']\n",
            "['the', 'software', 'update', 'fixed', 'several', 'bugs', 'in', 'the', 'system']\n",
            "['the', 'library', 'is', 'quiet', 'and', 'perfect', 'for', 'studying']\n",
            "['she', 'is', 'volunteering', 'at', 'the', 'animal', 'shelter', 'on', 'weekends']\n",
            "['the', 'beach', 'is', 'a', 'popular', 'destination', 'during', 'the', 'summer']\n",
            "['he', 'is', 'assembling', 'a', 'model', 'airplane', 'in', 'his', 'workshop']\n",
            "['the', 'chef', 'prepared', 'a', 'gourmet', 'meal', 'for', 'the', 'guests']\n",
            "['the', 'sunrise', 'over', 'the', 'hills', 'was', 'spectacular']\n",
            "['she', 'is', 'planning', 'a', 'surprise', 'party', 'for', 'her', 'best', 'friend']\n",
            "['the', 'museum', 'exhibits', 'ancient', 'artifacts', 'from', 'various', 'cultures']\n",
            "['the', 'forest', 'is', 'home', 'to', 'a', 'diverse', 'range', 'of', 'wildlife']\n",
            "['the', 'students', 'are', 'preparing', 'for', 'the', 'upcoming', 'exams']\n",
            "['the', 'car', 'engine', 'needs', 'a', 'tuneup', 'to', 'run', 'smoothly']\n",
            "['she', 'is', 'knitting', 'a', 'scarf', 'for', 'the', 'winter', 'season']\n",
            "['the', 'project', 'deadline', 'is', 'approaching', 'fast']\n",
            "['the', 'dog', 'is', 'barking', 'loudly', 'at', 'the', 'passing', 'car']\n",
            "['he', 'is', 'building', 'a', 'robot', 'for', 'the', 'science', 'fair']\n",
            "['the', 'flowers', 'in', 'the', 'garden', 'are', 'in', 'full', 'bloom']\n",
            "['she', 'is', 'exploring', 'new', 'career', 'opportunities', 'in', 'the', 'tech', 'industry']\n",
            "['the', 'concert', 'was', 'an', 'unforgettable', 'experience']\n",
            "['the', 'computer', 'crashed', 'and', 'lost', 'all', 'the', 'unsaved', 'work']\n",
            "['the', 'marathon', 'runners', 'are', 'pushing', 'their', 'limits', 'to', 'the', 'finish', 'line']\n",
            "['the', 'bakery', 'is', 'known', 'for', 'its', 'delicious', 'pastries']\n",
            "['she', 'is', 'organizing', 'a', 'charity', 'auction', 'for', 'a', 'good', 'cause']\n",
            "['the', 'stars', 'in', 'the', 'night', 'sky', 'were', 'shining', 'brightly']\n",
            "['he', 'is', 'renovating', 'his', 'house', 'to', 'create', 'more', 'space']\n",
            "['the', 'team', 'is', 'working', 'hard', 'to', 'meet', 'the', 'project', 'deadline']\n",
            "['the', 'coffee', 'machine', 'is', 'broken', 'and', 'needs', 'to', 'be', 'repaired']\n",
            "['the', 'bicycle', 'ride', 'through', 'the', 'countryside', 'was', 'refreshing']\n",
            "['the', 'bookstore', 'offers', 'a', 'wide', 'selection', 'of', 'novels', 'and', 'nonfiction']\n",
            "['she', 'is', 'preparing', 'a', 'presentation', 'for', 'her', 'companys', 'board', 'meeting']\n",
            "['the', 'rain', 'is', 'pouring', 'down', 'heavily', 'on', 'the', 'roof']\n",
            "['the', 'piano', 'recital', 'was', 'a', 'success', 'and', 'the', 'audience', 'applauded']\n",
            "['the', 'drone', 'captured', 'stunning', 'aerial', 'footage', 'of', 'the', 'landscape']\n",
            "['he', 'is', 'designing', 'a', 'website', 'for', 'his', 'new', 'business']\n",
            "['the', 'holiday', 'season', 'is', 'a', 'time', 'for', 'family', 'gatherings', 'and', 'celebrations']\n",
            "['the', 'train', 'journey', 'through', 'the', 'mountains', 'was', 'breathtaking']\n",
            "['she', 'is', 'learning', 'to', 'code', 'in', 'python', 'to', 'enhance', 'her', 'skills']\n",
            "['the', 'smartphone', 'app', 'has', 'many', 'useful', 'features']\n",
            "['the', 'festival', 'featured', 'music', 'food', 'and', 'cultural', 'performances']\n",
            "['the', 'wildlife', 'photographer', 'captured', 'rare', 'images', 'of', 'the', 'animals']\n",
            "['he', 'is', 'saving', 'money', 'for', 'his', 'dream', 'vacation']\n",
            "['the', 'waves', 'crashed', 'against', 'the', 'shore', 'during', 'the', 'storm']\n",
            "['the', 'theater', 'was', 'packed', 'for', 'the', 'opening', 'night', 'of', 'the', 'play']\n",
            "['she', 'is', 'baking', 'a', 'cake', 'for', 'her', 'sisters', 'birthday']\n",
            "['the', 'airplane', 'landed', 'safely', 'despite', 'the', 'turbulence']\n",
            "['the', 'garden', 'is', 'a', 'peaceful', 'retreat', 'from', 'the', 'busy', 'city']\n",
            "['he', 'is', 'practicing', 'his', 'presentation', 'for', 'the', 'upcoming', 'meeting']\n",
            "['the', 'store', 'is', 'offering', 'a', 'discount', 'on', 'all', 'electronics', 'this', 'weekend']\n",
            "['the', 'city', 'park', 'is', 'a', 'great', 'place', 'for', 'a', 'picnic']\n",
            "['the', 'football', 'team', 'is', 'preparing', 'for', 'the', 'championship', 'game']\n",
            "['she', 'is', 'writing', 'a', 'blog', 'about', 'her', 'experiences', 'as', 'a', 'digital', 'nomad']\n",
            "['the', 'sun', 'is', 'setting', 'behind', 'the', 'mountains', 'casting', 'a', 'golden', 'glow']\n",
            "['he', 'is', 'painting', 'the', 'walls', 'of', 'his', 'living', 'room', 'a', 'bright', 'color']\n",
            "['the', 'yoga', 'class', 'helped', 'her', 'feel', 'relaxed', 'and', 'rejuvenated']\n",
            "['the', 'construction', 'workers', 'are', 'building', 'a', 'new', 'skyscraper', 'downtown']\n",
            "['the', 'new', 'restaurant', 'in', 'town', 'is', 'getting', 'rave', 'reviews']\n",
            "['she', 'is', 'organizing', 'a', 'community', 'cleanup', 'event', 'in', 'her', 'neighborhood']\n",
            "['the', 'train', 'station', 'is', 'bustling', 'with', 'commuters', 'during', 'rush', 'hour']\n",
            "['he', 'is', 'studying', 'the', 'effects', 'of', 'climate', 'change', 'on', 'polar', 'ice', 'caps']\n",
            "['the', 'book', 'he', 'is', 'reading', 'is', 'a', 'science', 'fiction', 'novel', 'about', 'space', 'travel']\n",
            "['the', 'garden', 'is', 'full', 'of', 'butterflies', 'and', 'bees']\n",
            "['she', 'is', 'practicing', 'for', 'her', 'upcoming', 'piano', 'recital']\n",
            "['the', 'technology', 'conference', 'attracted', 'attendees', 'from', 'around', 'the', 'world']\n",
            "['he', 'is', 'upgrading', 'his', 'computer', 'with', 'the', 'latest', 'hardware', 'components']\n",
            "['the', 'beach', 'is', 'a', 'perfect', 'place', 'to', 'relax', 'and', 'unwind']\n",
            "['she', 'is', 'volunteering', 'at', 'the', 'food', 'bank', 'to', 'help', 'those', 'in', 'need']\n",
            "['the', 'mountain', 'trail', 'offers', 'breathtaking', 'views', 'of', 'the', 'surrounding', 'landscape']\n",
            "['the', 'new', 'software', 'release', 'includes', 'several', 'improvements', 'and', 'features']\n",
            "['the', 'stars', 'are', 'visible', 'in', 'the', 'clear', 'night', 'sky']\n",
            "['he', 'is', 'working', 'on', 'a', 'new', 'app', 'that', 'tracks', 'fitness', 'goals']\n",
            "['the', 'library', 'is', 'hosting', 'a', 'workshop', 'on', 'creative', 'writing']\n",
            "['she', 'is', 'planning', 'a', 'road', 'trip', 'with', 'her', 'friends', 'next', 'month']\n",
            "['the', 'farmer', 'is', 'planting', 'crops', 'for', 'the', 'upcoming', 'harvest', 'season']\n",
            "['the', 'waterfall', 'cascades', 'down', 'the', 'rocky', 'cliff', 'into', 'the', 'river', 'below']\n",
            "['the', 'museum', 'is', 'showcasing', 'a', 'collection', 'of', 'modern', 'art', 'pieces']\n",
            "['he', 'is', 'preparing', 'a', 'speech', 'for', 'his', 'cousins', 'wedding']\n",
            "['the', 'sunset', 'over', 'the', 'ocean', 'was', 'a', 'beautiful', 'end', 'to', 'the', 'day']\n",
            "['the', 'new', 'movie', 'is', 'getting', 'positive', 'reviews', 'from', 'critics']\n",
            "['she', 'is', 'decorating', 'her', 'home', 'for', 'the', 'holiday', 'season']\n",
            "['the', 'soccer', 'match', 'was', 'thrilling', 'with', 'both', 'teams', 'playing', 'their', 'best']\n",
            "['he', 'is', 'learning', 'a', 'new', 'programming', 'language', 'for', 'his', 'job']\n",
            "['the', 'bakery', 'smells', 'of', 'freshly', 'baked', 'bread', 'and', 'pastries']\n",
            "['the', 'river', 'is', 'flowing', 'swiftly', 'after', 'the', 'heavy', 'rain']\n",
            "['she', 'is', 'writing', 'a', 'poem', 'about', 'the', 'changing', 'seasons']\n",
            "['the', 'airplane', 'took', 'off', 'smoothly', 'and', 'reached', 'cruising', 'altitude', 'quickly']\n",
            "['the', 'park', 'is', 'filled', 'with', 'children', 'playing', 'on', 'the', 'swings', 'and', 'slides']\n",
            "['he', 'is', 'planning', 'to', 'start', 'his', 'own', 'business', 'next', 'year']\n",
            "['the', 'coffee', 'shop', 'has', 'a', 'cozy', 'atmosphere', 'perfect', 'for', 'reading', 'a', 'book']\n",
            "['the', 'wildlife', 'reserve', 'is', 'home', 'to', 'many', 'endangered', 'species']\n",
            "['she', 'is', 'sewing', 'a', 'quilt', 'as', 'a', 'gift', 'for', 'her', 'grandmother']\n",
            "['the', 'car', 'broke', 'down', 'on', 'the', 'way', 'to', 'the', 'airport']\n",
            "['the', 'team', 'is', 'brainstorming', 'ideas', 'for', 'the', 'new', 'project']\n",
            "['the', 'dog', 'is', 'wagging', 'its', 'tail', 'happily', 'as', 'it', 'runs', 'in', 'the', 'yard']\n",
            "['she', 'is', 'learning', 'to', 'play', 'the', 'piano', 'with', 'the', 'help', 'of', 'a', 'tutor']\n",
            "['the', 'restaurant', 'serves', 'delicious', 'homemade', 'pasta']\n",
            "['he', 'is', 'building', 'a', 'treehouse', 'for', 'his', 'children', 'in', 'the', 'backyard']\n",
            "['the', 'sun', 'is', 'shining', 'brightly', 'on', 'this', 'warm', 'summer', 'day']\n",
            "['she', 'is', 'packing', 'her', 'suitcase', 'for', 'her', 'upcoming', 'vacation']\n",
            "['the', 'bookstore', 'is', 'having', 'a', 'sale', 'on', 'all', 'hardcover', 'books']\n",
            "['the', 'garden', 'is', 'filled', 'with', 'the', 'scent', 'of', 'blooming', 'flowers']\n",
            "['he', 'is', 'organizing', 'his', 'office', 'to', 'make', 'it', 'more', 'efficient']\n",
            "['the', 'movie', 'was', 'so', 'exciting', 'that', 'everyone', 'in', 'the', 'audience', 'was', 'on', 'edge']\n",
            "['the', 'chef', 'is', 'experimenting', 'with', 'new', 'recipes', 'for', 'the', 'menu']\n",
            "['she', 'is', 'taking', 'a', 'photography', 'class', 'to', 'improve', 'her', 'skills']\n",
            "['the', 'train', 'is', 'running', 'late', 'due', 'to', 'the', 'heavy', 'rain']\n",
            "['he', 'is', 'repairing', 'the', 'roof', 'of', 'his', 'house', 'after', 'the', 'storm']\n",
            "['the', 'yoga', 'class', 'focuses', 'on', 'breathing', 'techniques', 'and', 'relaxation']\n",
            "['the', 'city', 'skyline', 'is', 'breathtaking', 'at', 'night', 'with', 'all', 'the', 'lights']\n",
            "['she', 'is', 'writing', 'a', 'novel', 'about', 'her', 'travels', 'around', 'the', 'world']\n",
            "['the', 'football', 'team', 'won', 'the', 'championship', 'game', 'in', 'a', 'thrilling', 'finish']\n",
            "['he', 'is', 'studying', 'the', 'history', 'of', 'ancient', 'civilizations', 'for', 'his', 'research', 'paper']\n",
            "['the', 'beach', 'is', 'crowded', 'with', 'people', 'enjoying', 'the', 'sunny', 'weather']\n",
            "['she', 'is', 'knitting', 'a', 'sweater', 'for', 'her', 'newborn', 'niece']\n",
            "['the', 'car', 'dealership', 'is', 'offering', 'discounts', 'on', 'all', 'new', 'models']\n",
            "['he', 'is', 'designing', 'a', 'new', 'logo', 'for', 'his', 'company']\n",
            "['the', 'weather', 'forecast', 'predicts', 'snow', 'for', 'the', 'weekend']\n",
            "['the', 'library', 'is', 'quiet', 'a', 'perfect', 'place', 'to', 'study']\n",
            "['she', 'is', 'taking', 'dance', 'lessons', 'to', 'prepare', 'for', 'her', 'wedding']\n",
            "['the', 'construction', 'of', 'the', 'new', 'bridge', 'is', 'ahead', 'of', 'schedule']\n",
            "['the', 'movie', 'premiere', 'was', 'attended', 'by', 'many', 'famous', 'celebrities']\n",
            "['he', 'is', 'training', 'for', 'a', 'triathlon', 'that', 'includes', 'swimming', 'cycling', 'and', 'running']\n",
            "['the', 'art', 'gallery', 'is', 'displaying', 'a', 'collection', 'of', 'abstract', 'paintings']\n",
            "['she', 'is', 'planting', 'flowers', 'in', 'her', 'garden', 'to', 'attract', 'butterflies']\n",
            "['the', 'airplane', 'is', 'flying', 'over', 'the', 'mountains', 'on', 'its', 'way', 'to', 'the', 'destination']\n",
            "['he', 'is', 'learning', 'to', 'play', 'the', 'guitar', 'by', 'watching', 'online', 'tutorials']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: One-Hot Encoding for a Given Text\n",
        " Write a program that converts a sentence into one-hot encoded vectors.\n"
      ],
      "metadata": {
        "id": "mffwwItQxHly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "corpus =  dataset\n",
        "\n",
        "# Create a set of unique words in the corpus\n",
        "unique_words = set()\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "        unique_words.add(word.lower())\n",
        "\n",
        "# Create a dictionary to map each\n",
        "# unique word to an index\n",
        "word_to_index = {}\n",
        "for i, word in enumerate(unique_words):\n",
        "    word_to_index[word] = i\n",
        "\n",
        "# Create one-hot encoded vectors for\n",
        "# each word in the corpus\n",
        "one_hot_vectors = []\n",
        "for sentence in corpus:\n",
        "    sentence_vectors = []\n",
        "    for word in sentence.split():\n",
        "        vector = np.zeros(len(unique_words))\n",
        "        vector[word_to_index[word.lower()]] = 1\n",
        "        sentence_vectors.append(vector)\n",
        "    one_hot_vectors.append(sentence_vectors)\n",
        "\n",
        "# Print the one-hot encoded vectors\n",
        "# for the first sentence\n",
        "print(\"One-hot encoded vectors for the first sentence:\")\n",
        "for vector in one_hot_vectors[0]:\n",
        "    print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPPMWnHTHgnv",
        "outputId": "603f9f94-a769-4026-f9be-631a79d51ccb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoded vectors for the first sentence:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: TF-IDF Calculation\n",
        "Implement a function to calculate Term Frequency (TF) and Inverse Document Frequency (IDF) for a given corpus of documents."
      ],
      "metadata": {
        "id": "XqRN6TmNzpYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def compute_tf(text):\n",
        "    # Tokenize the text by splitting on whitespace\n",
        "    words = text.split()\n",
        "    # Count the occurrences of each word in the document\n",
        "    word_counts = Counter(words)\n",
        "    # Compute the term frequency for each word\n",
        "    tf = {word: count / len(words) for word, count in word_counts.items()}\n",
        "    return tf\n",
        "\n",
        "def compute_idf(corpus):\n",
        "    # Total number of documents\n",
        "    num_docs = len(corpus)\n",
        "    # Count the number of documents that contain each word\n",
        "    word_doc_counts = Counter()\n",
        "    for text in corpus:\n",
        "        words = set(text.split())\n",
        "        for word in words:\n",
        "            word_doc_counts[word] += 1\n",
        "    # Compute the inverse document frequency for each word\n",
        "    idf = {word: math.log(num_docs / count) for word, count in word_doc_counts.items()}\n",
        "    return idf\n",
        "\n",
        "def compute_tf_idf(text, corpus):\n",
        "    tf = compute_tf(text)\n",
        "    idf = compute_idf(corpus)\n",
        "    # Compute the TF-IDF for each word in the document\n",
        "    tf_idf = {word: tf[word] * idf[word] for word in tf}\n",
        "    return tf_idf\n",
        "\n",
        "# Example usage\n",
        "corpus = dataset\n",
        "text = dataset[0]\n",
        "tf_idf = compute_tf_idf(text, corpus)\n",
        "print(tf_idf)"
      ],
      "metadata": {
        "id": "rAwTk1W6zyhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bcdeb8c-dc20-44ab-e983-66ed987db6e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'The': 0.06585054196083108, 'car': 0.5087208689434358, 'is': 0.05955625770454101, 'driven': 0.7386405707197359, 'on': 0.3180064308388159, 'the': 0.10397610550683362, 'road.': 0.7386405707197359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Word2Vec Model Implementation\n",
        "Build a simple Word2Vec model using Gensim for a given corpus\n",
        "\n",
        "use gensi m.models\n",
        "### from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "2cvBRib40Nxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = dataset\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# Create and train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "# Load the model\n",
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"airplane\")\n",
        "print(similar_words)"
      ],
      "metadata": {
        "id": "24FO7cw30SR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaeb261-1619-4a7e-a8c7-7a5f8a8d7194"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('footage', 0.3249693214893341), ('cabin', 0.29437825083732605), ('exams.', 0.28312361240386963), ('run', 0.2572650909423828), ('roses.', 0.25433534383773804), ('changing', 0.25385114550590515), ('vacation.', 0.2497791349887848), (\"company's\", 0.24248816072940826), ('for', 0.24247483909130096), ('car.', 0.24011121690273285)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: Sentence Matching Based on Given Dataset\n",
        "In this task, you will be given a large dataset of sentences (provided below). Your goal is to match a query with sentences from this dataset. You will implement a function to find sentences that contain specific words or phrases from a user query.\n",
        "## **Steps:**\n",
        "- **Data Acquisition:**Use the provided dataset of sentences.\n",
        "- **Text Preparation:** Clean the dataset (remove punctuation, convert to lowercase, etc.).\n",
        "- **Feature Engineering:** Use TF-IDF (Term Frequency-Inverse Document Frequency) to create numerical vectors for each sentence.\n",
        "- **Search:** Match the query against the sentence vectors using cosine similarity.\n",
        "**Return Matched Sentences:** Display the top matching sentences.\n",
        "\n",
        "## **Required Libraries:**\n",
        "- **NLTK** for text cleaning and preprocessing.\n",
        "- **TfidfVectorizer** from **scikit-learn** for converting text to vectors.\n",
        "- **Cosine Similarity** for finding similar sentences.\n"
      ],
      "metadata": {
        "id": "Ec5JKBzz0kGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install required libraries**"
      ],
      "metadata": {
        "id": "9rvBWeGQEoF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn"
      ],
      "metadata": {
        "id": "COsgOnNd3h4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21abd509-3814-40db-e833-4e8d1a3774aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "YkrkWvfCEv0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "gi2u4JZJEuxO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**:\n",
        "- **Clean the dataset**: Remove punctuation, convert to lowercase, etc.\n",
        "- **Tokenize the text**: Split the text into words."
      ],
      "metadata": {
        "id": "TvRbYGDpE7T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Clean the dataset\n",
        "cleaned_dataset = [clean_text(sentence) for sentence in dataset]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-caqNhLFFYK",
        "outputId": "ba752a9b-4116-4ffd-ea47-672c93e555af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**:\n",
        "- Use TF-IDF to create numerical vectors for each sentence."
      ],
      "metadata": {
        "id": "nsIdc339FLZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_dataset)"
      ],
      "metadata": {
        "id": "j6g_kNb6FRq8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search**:\n",
        "- Match the query against the sentence vectors using cosine similarity."
      ],
      "metadata": {
        "id": "juLOjBUnFVkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_sentences(query, tfidf_matrix, vectorizer, top_n=3):\n",
        "    # Clean the query\n",
        "    cleaned_query = clean_text(query)\n",
        "    # Transform the query to TF-IDF vector\n",
        "    query_vector = vectorizer.transform([cleaned_query])\n",
        "    # Compute cosine similarity\n",
        "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    # Get the top n similar sentences\n",
        "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "    return [(dataset[i], cosine_similarities[i]) for i in top_indices]\n",
        "\n",
        "# Example query\n",
        "query = \"She is learning\"\n",
        "matched_sentences = find_similar_sentences(query, tfidf_matrix, vectorizer)\n",
        "for sentence, score in matched_sentences:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ojmenPFeVM",
        "outputId": "26bc01a6-5a3c-4ec7-8f49-6fefaacc34d4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: She is learning French in her spare time.\n",
            ", Similarity Score: 0.45701242718209695\n",
            "Sentence: She is learning to play the guitar in her free time.\n",
            ", Similarity Score: 0.42180067088279133\n",
            "Sentence: She is learning to play the piano with the help of a tutor.\n",
            ", Similarity Score: 0.405846214556852\n"
          ]
        }
      ]
    }
  ]
}