{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pXK0FWC1Lv7"
      },
      "source": [
        "# RNN (Recurrent Neural Network)\n",
        "\n",
        "**RNN form scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56R_Nxtm1Lv8"
      },
      "source": [
        "Importing the libraries &amp; load data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lx-SeKC1Lv8",
        "outputId": "783ddc11-03aa-42f2-afa7-6d44d546f571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed 12393 sentences.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"I joined a new league this year and they have different scoring rules than I'm used to. It's a slight PPR league- .2 PPR. Standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per TD thrown, and some bonuses for rec/rush/pass yardage. My question is, is it wildly clear that QB has the highest potential for points? I put in the rules at a ranking site and noticed that top QBs had 300 points more than the top RB/WR. Would it be dumb not to grab a QB in the first round?\"],\n",
              " ['In your scenario, a person could just not run the mandatory background check on the buyer and still sell the gun to the felon. There\\'s no way to enforce it. An honest seller is going to not sell the gun to them when they see they\\'re a felon on the background check. A dishonest seller isn\\'t going to run the check in the first place. No one is going to be honest enough to run the check, see they\\'re a felon, and then all of a sudden immediately turn dishonest and say \"nah, you know what, here\\'s your gun anyway.\" They wouldn\\'t run the fucking check in the first place, genius. Your bullshit proposal is **NOT ENFORCEABLE**. This is why people without \\n\\n&gt;Here\\'s an idea, why not make a background check system where it would be illegal to sell guns to felons? That doesn\\'t convince you? IDGAF.\\n\\nWe already fucking have that. What aren\\'t you understanding about this?! It\\'s just currently not available to private sellers. I\\'d like to make it available, but there\\'s no point in making it mandatory.\\n\\n&gt;You\\'re just supporting it to make a gesture towards background check reform in bad faith.\\n\\nAs a gun owner who has harmed no one and has constitutionally protected rights, I actually don\\'t own the anti-gun side a damn thing. It\\'s people like you that convince me to not give an inch at all. I\\'m wiling to extend NICS to private sellers as voluntary. Take it or leave it, because it isn\\'t going to be made mandatory. Not gonna happen. Period.'],\n",
              " [\"They don't get paid for how much time you spend building your army. They get paid when you gem it. Having a premade layout saves what, 30 seconds? During that time they've made $0 and still have the ability to make $X if you decide to gem it. \"],\n",
              " ['I dunno, back before the August update in an A+ lobby for Tower Control in Saltspray Rig, I remember the enemy team barraging us with bombs in a suspiciously coordinated fashion.  And my teammates were also tossing bombs left and right.  Non-stop splat/suction/burst bombs.  Bombs, bombs fucking everywhere.  It was terrifying having to dodge one after the other.  It never ended, and some of them even had bomb rush to toss even more goddamn bombs.\\n\\nI think I still have PTSD of that horrifying event.  That match really felt like a Michael Bay movie.'],\n",
              " ['No, but Toriyama sometimes would draw himself as a little robot. Shen was a funny character for a few episodes(hitting Yamcha in the junk) before you find out his true identity. Then he has an awesome fight with Piccolo.'],\n",
              " [\"Implement some form of tenure for mods and require a majority vote to remove any tenured mod. Only allow mods to trigger a removal vote on mods lower than them in the hierarchy however.\\n\\nThis removes the possibility of a single mod (such as an absentee top mod) swooping in and blowing up the sub but doesn't create opportunities for hijacking. Importantly, it also doesn't require rejiggering the existing mod hierarchies or ongoing admin involvement in dispute resolution.\"],\n",
              " [\"I hate reading shit like this because you sound like such a catch before or after you started working out, and your husband takes you for granted.  What I would do to be with someone like you.  I'm not going to tell you to stay or go but whichever you decide, just don't lose sight of how you deserve to be treated and don't put up with anything less.\"],\n",
              " ['&gt;Baltimore County prosecutors will speak to the victim\\'s family before deciding whether to charge Shattuck, reports the station.\\n\\nEarlier the parents said: \\n\\n&gt;\"What she did to my son is heinous. The fact that she paraded her pedophilia in front of her own son is even more disturbing,\" the mother said. \"Any adult who rapes a child deserves to be in prison. Please hold her accountable.\"\\n\\nIt seems like they probably will ask that they send her to prison. I hope they do. Honestly that judge basically gave the green light to any adult woman to molest little boys free of consequence in the future and that\\'s simply not acceptable. \\n'],\n",
              " [\"I think you might be missing my point. A gun is a tool which allows you to kill very efficiently. Not having those around means there's less chance for people to make dumb decisions which end up in deaths. I'm not concerned with spree killers or other people who plan to kill, I'm concerned with people who wouldn't otherwise put other's lives at risk when they get angry, but there's a gun at hand. The latter is vastly more common than the former.\"],\n",
              " [\"Thank you for the Giveaway OP! I would like to enter for The Elder Scrolls V: Skyrim.\\n\\nI played it at my friends house but haven't been able to get it yet! I thank you again for this giveaway. Good luck everyone!\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "with open(\"reddit-comments-2015-08.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f, skipinitialspace=True)\n",
        "    next(reader)\n",
        "    # split full comments to sentences\n",
        "    data = [x for x in reader]\n",
        "\n",
        "print(\"Parsed %d sentences.\" % (len(data)))\n",
        "data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVrZUVwS1Lv9"
      },
      "source": [
        "### Data Preprocessing &amp; Training\n",
        "- Tokenize Text\n",
        "  - convert in sentences\n",
        "  - convert in words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BF5DGs91eMV",
        "outputId": "645c7402-2308-479e-c15c-3a47aec744f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9m77G_w1Lv9",
        "outputId": "a663e5fa-f323-481c-bf9d-305fe4d77c2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([\"I joined a new league this year and they have different scoring rules than I'm used to.\",\n",
              "  \"It's a slight PPR league- .2 PPR.\",\n",
              "  'Standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per TD thrown, and some bonuses for rec/rush/pass yardage.',\n",
              "  'My question is, is it wildly clear that QB has the highest potential for points?',\n",
              "  'I put in the rules at a ranking site and noticed that top QBs had 300 points more than the top RB/WR.',\n",
              "  'Would it be dumb not to grab a QB in the first round?',\n",
              "  'In your scenario, a person could just not run the mandatory background check on the buyer and still sell the gun to the felon.',\n",
              "  \"There's no way to enforce it.\",\n",
              "  \"An honest seller is going to not sell the gun to them when they see they're a felon on the background check.\",\n",
              "  \"A dishonest seller isn't going to run the check in the first place.\"],\n",
              " [['i',\n",
              "   'joined',\n",
              "   'a',\n",
              "   'new',\n",
              "   'league',\n",
              "   'this',\n",
              "   'year',\n",
              "   'and',\n",
              "   'they',\n",
              "   'have',\n",
              "   'different',\n",
              "   'scoring',\n",
              "   'rules',\n",
              "   'than',\n",
              "   'i',\n",
              "   \"'m\",\n",
              "   'used',\n",
              "   'to',\n",
              "   '.'],\n",
              "  ['it', \"'s\", 'a', 'slight', 'ppr', 'league-', '.2', 'ppr', '.'],\n",
              "  ['standard',\n",
              "   'besides',\n",
              "   '1',\n",
              "   'points',\n",
              "   'for',\n",
              "   '15',\n",
              "   'yards',\n",
              "   'receiving',\n",
              "   ',',\n",
              "   '.2',\n",
              "   'points',\n",
              "   'per',\n",
              "   'completion',\n",
              "   ',',\n",
              "   '6',\n",
              "   'points',\n",
              "   'per',\n",
              "   'td',\n",
              "   'thrown',\n",
              "   ',',\n",
              "   'and',\n",
              "   'some',\n",
              "   'bonuses',\n",
              "   'for',\n",
              "   'rec/rush/pass',\n",
              "   'yardage',\n",
              "   '.'],\n",
              "  ['my',\n",
              "   'question',\n",
              "   'is',\n",
              "   ',',\n",
              "   'is',\n",
              "   'it',\n",
              "   'wildly',\n",
              "   'clear',\n",
              "   'that',\n",
              "   'qb',\n",
              "   'has',\n",
              "   'the',\n",
              "   'highest',\n",
              "   'potential',\n",
              "   'for',\n",
              "   'points',\n",
              "   '?'],\n",
              "  ['i',\n",
              "   'put',\n",
              "   'in',\n",
              "   'the',\n",
              "   'rules',\n",
              "   'at',\n",
              "   'a',\n",
              "   'ranking',\n",
              "   'site',\n",
              "   'and',\n",
              "   'noticed',\n",
              "   'that',\n",
              "   'top',\n",
              "   'qbs',\n",
              "   'had',\n",
              "   '300',\n",
              "   'points',\n",
              "   'more',\n",
              "   'than',\n",
              "   'the',\n",
              "   'top',\n",
              "   'rb/wr',\n",
              "   '.'],\n",
              "  ['would',\n",
              "   'it',\n",
              "   'be',\n",
              "   'dumb',\n",
              "   'not',\n",
              "   'to',\n",
              "   'grab',\n",
              "   'a',\n",
              "   'qb',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'round',\n",
              "   '?'],\n",
              "  ['in',\n",
              "   'your',\n",
              "   'scenario',\n",
              "   ',',\n",
              "   'a',\n",
              "   'person',\n",
              "   'could',\n",
              "   'just',\n",
              "   'not',\n",
              "   'run',\n",
              "   'the',\n",
              "   'mandatory',\n",
              "   'background',\n",
              "   'check',\n",
              "   'on',\n",
              "   'the',\n",
              "   'buyer',\n",
              "   'and',\n",
              "   'still',\n",
              "   'sell',\n",
              "   'the',\n",
              "   'gun',\n",
              "   'to',\n",
              "   'the',\n",
              "   'felon',\n",
              "   '.'],\n",
              "  ['there', \"'s\", 'no', 'way', 'to', 'enforce', 'it', '.'],\n",
              "  ['an',\n",
              "   'honest',\n",
              "   'seller',\n",
              "   'is',\n",
              "   'going',\n",
              "   'to',\n",
              "   'not',\n",
              "   'sell',\n",
              "   'the',\n",
              "   'gun',\n",
              "   'to',\n",
              "   'them',\n",
              "   'when',\n",
              "   'they',\n",
              "   'see',\n",
              "   'they',\n",
              "   \"'re\",\n",
              "   'a',\n",
              "   'felon',\n",
              "   'on',\n",
              "   'the',\n",
              "   'background',\n",
              "   'check',\n",
              "   '.'],\n",
              "  ['a',\n",
              "   'dishonest',\n",
              "   'seller',\n",
              "   'is',\n",
              "   \"n't\",\n",
              "   'going',\n",
              "   'to',\n",
              "   'run',\n",
              "   'the',\n",
              "   'check',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'place',\n",
              "   '.']])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# flatten data\n",
        "sentences = []\n",
        "word_tokens = []\n",
        "\n",
        "for row in data:\n",
        "    # grab the first element which contains the text\n",
        "    text = row[0] if row else \"\"\n",
        "\n",
        "    # convert into sentences\n",
        "    sents = sent_tokenize(text)\n",
        "    sentences.extend(sents)\n",
        "\n",
        "    # convert into words\n",
        "    for sent in sents:\n",
        "        word_tokens.append(word_tokenize(sent.lower()))\n",
        "\n",
        "sentences[:10], word_tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEEPXst01Lv9"
      },
      "source": [
        "- Remove infrequent words\n",
        "  - remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoJKp03W1Lv-",
        "outputId": "4631c247-2931-42fa-b166-fafe92d2061f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['joined', 'new', 'league', 'year', 'different', 'scoring', 'rules', 'used'],\n",
              " ['slight', 'ppr', 'ppr'],\n",
              " ['standard',\n",
              "  'besides',\n",
              "  'points',\n",
              "  'yards',\n",
              "  'receiving',\n",
              "  'points',\n",
              "  'per',\n",
              "  'completion',\n",
              "  'points',\n",
              "  'per',\n",
              "  'td',\n",
              "  'thrown',\n",
              "  'bonuses',\n",
              "  'yardage'],\n",
              " ['question', 'wildly', 'clear', 'qb', 'highest', 'potential', 'points'],\n",
              " ['put', 'rules', 'ranking', 'site', 'noticed', 'top', 'qbs', 'points', 'top'],\n",
              " ['would', 'dumb', 'grab', 'qb', 'first', 'round'],\n",
              " ['scenario',\n",
              "  'person',\n",
              "  'could',\n",
              "  'run',\n",
              "  'mandatory',\n",
              "  'background',\n",
              "  'check',\n",
              "  'buyer',\n",
              "  'still',\n",
              "  'sell',\n",
              "  'gun',\n",
              "  'felon'],\n",
              " ['way', 'enforce'],\n",
              " ['honest',\n",
              "  'seller',\n",
              "  'going',\n",
              "  'sell',\n",
              "  'gun',\n",
              "  'see',\n",
              "  'felon',\n",
              "  'background',\n",
              "  'check'],\n",
              " ['dishonest', 'seller', 'going', 'run', 'check', 'first', 'place']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "filtered_words = [[word for word in sentence if word.isalpha() and word not in stopwords] for sentence in word_tokens]\n",
        "filtered_words[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjXFCdec1Lv-"
      },
      "source": [
        "- Build training &amp; test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpm99fGQ1Lv-",
        "outputId": "01e930b1-4d75-44cf-bfae-304bac9bf40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test:\n",
            "scene bobby hit hr save son wrath gil\n",
            "[4804, 2612, 1890, 9295, 895, 255, 2451, 17555]\n",
            "13309\n",
            "train:\n",
            "slight ppr ppr\n",
            "[8, 9, 9]\n",
            "53234\n"
          ]
        }
      ],
      "source": [
        "# create vocabulary from occuring words more than once\n",
        "word_freq = nltk.FreqDist([word for sentence in filtered_words for word in sentence])\n",
        "vocab = [word for word, freq in word_freq.items() if freq > 1]\n",
        "# map each word to an integer index\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "# map each integer index to a word\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "# convert sentences into sequences of indices\n",
        "dataset = [[word_to_index[word] for word in sentence if word in word_to_index] for sentence in filtered_words]\n",
        "\n",
        "# Split into train and test\n",
        "split_ratio = 0.8\n",
        "train_size = int(split_ratio * len(dataset))\n",
        "train_data = dataset[:train_size]\n",
        "test_data = dataset[train_size:]\n",
        "\n",
        "print (\"test:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in test_data[1]]), test_data[1]))\n",
        "print(len(test_data))\n",
        "print (\"train:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in train_data[1]]), train_data[1]))\n",
        "print(len(train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY0cPCCG1Lv-"
      },
      "source": [
        "### Building the RNN\n",
        "- Initialize Assistance parameters\n",
        "  - word_dim, hidden_dim, output_dim, bptt_truncate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LI4tjfkG1Lv-"
      },
      "outputs": [],
      "source": [
        "word_dim = len(vocab)       # Vocabulary size\n",
        "hidden_dim = 100            # Hidden layer size\n",
        "output_dim = word_dim       # Output size is the same as word_dim for predicting the next word\n",
        "bptt_truncate = 4           # Truncate for backpropagation through time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCl4ezrK1Lv-"
      },
      "source": [
        "- Initiaize Network parameters\n",
        "  - U, V, W\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f1xm3BEY1Lv-"
      },
      "outputs": [],
      "source": [
        "# Initialize weights (U, V, W)\n",
        "\n",
        "U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srfNvJ-T1Lv-"
      },
      "source": [
        "- Activate Function\n",
        "  - Sigmod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tPJwpmd81Lv-"
      },
      "outputs": [],
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_SHu6_21Lv-"
      },
      "source": [
        "\n",
        "- Forward_Propagation\n",
        "  - do forward pass to get prediction\n",
        "  - Prodict the highest score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZv2tRRl1Lv-",
        "outputId": "bfa67b07-8165-410a-89ca-17b92061531f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 22236)\n",
            "[[0.49894081 0.50011028 0.50086139 ... 0.50103027 0.49904232 0.50018804]\n",
            " [0.50018994 0.49913737 0.50013146 ... 0.50044165 0.49991939 0.50011482]\n",
            " [0.50041449 0.49971303 0.49961411 ... 0.50022544 0.50009295 0.50018931]\n",
            " ...\n",
            " [0.49981266 0.49996052 0.50074565 ... 0.4999209  0.50010474 0.50174146]\n",
            " [0.50178623 0.50096816 0.5001411  ... 0.4991828  0.50062831 0.50002625]\n",
            " [0.49985975 0.49967907 0.50021887 ... 0.49992721 0.50117586 0.49995804]]\n",
            "(8,)\n",
            "[ 9396  3510   422 12520 16337  1959  2115  6247]\n",
            "['evict', 'manufacturing', 'wrote', 'activators', 'sg', 'combination', 'price', 'insight']\n"
          ]
        }
      ],
      "source": [
        "def forward_propagation(x):\n",
        "    T = len(x) # time steps\n",
        "    s = np.zeros((T, hidden_dim)) # hidden states\n",
        "    o = np.zeros((T, output_dim)) # output states\n",
        "    for t in range(T): # for each time step\n",
        "        # st = tanh(Uxt + Wst-1)\n",
        "        s[t] = np.tanh(U[:, x[t]] + W.dot(s[t-1] if t > 0 else np.zeros(hidden_dim)))\n",
        "        # ot = softmax(Vst)\n",
        "        o[t] = sigmoid(V.dot(s[t]))\n",
        "    return o, s\n",
        "\n",
        "# predict highest score\n",
        "def predict(o):\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "np.random.seed(10)\n",
        "# Perform forward propagation\n",
        "o, s = forward_propagation(train_data[0])\n",
        "print(o.shape)\n",
        "print(o)\n",
        "predictions = predict(o)\n",
        "print(predictions.shape)\n",
        "print(predictions)\n",
        "# map predictions to words\n",
        "print([index_to_word[x] for x in predictions])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmKGeSEO1Lv_"
      },
      "source": [
        "- Calculate loss\n",
        "  - create loss function to measure the errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvMXmtJe1Lv_",
        "outputId": "e94a833e-f8b8-4323-c9e0-830084a87d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Loss for random predictions: 10.009468\n",
            "Actual loss: 5.234607\n"
          ]
        }
      ],
      "source": [
        "def calculate_total_loss(x, y):\n",
        "    L = 0\n",
        "    for i in range(len(y)):\n",
        "        o, s = forward_propagation(x[i])\n",
        "        # get correct word predictions only, using the correct word indices\n",
        "        correct_word_predictions = o[range(len(y[i])), y[i]]\n",
        "        # cross-entropy loss\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(x, y):\n",
        "    # divide loss by training data size\n",
        "    N = len(y)\n",
        "    return calculate_total_loss(x, y) / N\n",
        "\n",
        "print('Expected Loss for random predictions: %f' % np.log(word_dim))\n",
        "print('Actual loss: %f' % calculate_loss(train_data[:1000], train_data[:1000]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}