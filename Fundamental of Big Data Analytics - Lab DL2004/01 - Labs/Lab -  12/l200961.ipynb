{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epzYnlTodmat"
      },
      "source": [
        "**20L-0961**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SubTWG3Admav"
      },
      "source": [
        "# Introduction to Machine Learning in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njlIVM5Admav",
        "outputId": "2974df90-e387-43bd-b3bb-a424499009a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pyspark findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nTlBb_r0dmaw",
        "outputId": "b2fd71ef-cd20-4b1d-e0e3-2f19531bfaca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W_RU5RHdmaw"
      },
      "source": [
        "Basic Statistics\n",
        "\n",
        "1. Correlation\n",
        "---\n",
        "Calculating the correlation between two series of data is a common operation in Statistics. In\n",
        "spark.ml we provide the flexibility to calculate pairwise correlations among many series. The\n",
        "supported correlation methods are currently Pearson’s and Spearman’s correlation.\n",
        "\n",
        "Correlation computes the correlation matrix for the input Dataset of Vectors using the specified\n",
        "method. The output will be a DataFrame that contains the correlation matrix of the column of\n",
        "vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rl_LmAjdmax",
        "outputId": "b5417366-b44d-4bc0-eae9-23d649b7e8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"CorrelationExample\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # $example on$\n",
        "    data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "            (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "            (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "            (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "    df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "    r1 = Correlation.corr(df, \"features\").head()\n",
        "\n",
        "    # $example off$\n",
        "    assert r1 is not None\n",
        "    # $example on$\n",
        "    print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "    r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "\n",
        "    # $example off$\n",
        "    assert r2 is not None\n",
        "    # $example on$\n",
        "    print(\"Spearman correlation matrix:\\n\" + str(r2[0]))\n",
        "    # $example off$\n",
        "\n",
        "    spark.stop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYG-Ptoldmax",
        "outputId": "47434209-7526-45b9-db98-8088c054b465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseMatrix([[1.        , 0.05564149,        NaN, 0.40047142],\n",
            "             [0.05564149, 1.        ,        NaN, 0.91359586],\n",
            "             [       NaN,        NaN, 1.        ,        NaN],\n",
            "             [0.40047142, 0.91359586,        NaN, 1.        ]])\n",
            "DenseMatrix([[1.        , 0.10540926,        NaN, 0.4       ],\n",
            "             [0.10540926, 1.        ,        NaN, 0.9486833 ],\n",
            "             [       NaN,        NaN, 1.        ,        NaN],\n",
            "             [0.4       , 0.9486833 ,        NaN, 1.        ]])\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "spark = SparkSession.builder.appName('correlationLab').getOrCreate()\n",
        "dataset = [[Vectors.dense([1, 0, 0, -2])],\n",
        "           [Vectors.dense([4, 5, 0, 3])],\n",
        "           [Vectors.dense([6, 7, 0, 8])],\n",
        "           [Vectors.dense([9, 0, 0, 1])]]\n",
        "dataset = spark.createDataFrame(dataset, ['features'])\n",
        "pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]\n",
        "print(str(pearsonCorr).replace('nan', 'NaN'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]\n",
        "print(str(spearmanCorr).replace('nan', 'NaN'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rtivMdrdmax"
      },
      "source": [
        "2. Hypothesis testing\n",
        "---\n",
        "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically\n",
        "\n",
        "significant, whether this result occurred by chance or not. spark.ml currently supports Pearson’s Chi-\n",
        "squared ( χ2) tests for independence.\n",
        "\n",
        "ChiSquareTest\n",
        "---\n",
        "ChiSquareTest conducts Pearson’s independence test for every feature against the label. For each\n",
        "feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared\n",
        "statistic is computed. All label and feature values must be categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-SxG2KIdmax",
        "outputId": "81ec4d0f-659f-4381-95e9-fd9d897d050e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"ChiSquareTestExample\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # $example on$\n",
        "    data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "            (0.0, Vectors.dense(1.5, 20.0)),\n",
        "            (1.0, Vectors.dense(1.5, 30.0)),\n",
        "            (0.0, Vectors.dense(3.5, 30.0)),\n",
        "            (0.0, Vectors.dense(3.5, 40.0)),\n",
        "            (1.0, Vectors.dense(3.5, 40.0))]\n",
        "    df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "    r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "\n",
        "    # $example off$\n",
        "    assert r is not None\n",
        "    # $example on$\n",
        "\n",
        "    print(\"pValues: \" + str(r.pValues))\n",
        "    print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "    print(\"statistics: \" + str(r.statistics))\n",
        "    # $example off$\n",
        "\n",
        "    spark.stop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOFpOVZEdmay",
        "outputId": "49d79f6c-d61a-487a-f2a0-26c120bd92f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(degreesOfFreedom=[3, 1, 0])\n",
            "statistics:  4.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "spark = SparkSession.builder.appName(\"ChiSquareTestLab\").getOrCreate()\n",
        "dataset = [[0, Vectors.dense([0, 0, 1])],\n",
        "           [0, Vectors.dense([1, 0, 1])],\n",
        "           [1, Vectors.dense([2, 1, 1])],\n",
        "           [1, Vectors.dense([3, 1, 1])]]\n",
        "dataset = spark.createDataFrame(dataset, [\"label\", \"features\"])\n",
        "chiSqResult = ChiSquareTest.test(dataset, 'features', 'label')\n",
        "print(chiSqResult.select(\"degreesOfFreedom\").collect()[0])\n",
        "\n",
        "chiSqResult = ChiSquareTest.test(dataset, 'features', 'label', True)\n",
        "row = chiSqResult.orderBy(\"featureIndex\").collect()\n",
        "assert row is not None\n",
        "print(\"statistics: \",row[0].statistic)\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_9byRDjdmay"
      },
      "source": [
        "3. Summarizer\n",
        "---\n",
        "We provide vector column summary statistics for Dataframe through Summarizer. Available metrics are\n",
        "the column-wise max, min, mean, sum, variance, std, and number of nonzeros, as well as the total count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhLpWKEvdmay",
        "outputId": "a966ec54-332c-4d32-9599-b786efcd2c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"SummarizerExample\") \\\n",
        "        .getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # $example on$\n",
        "    df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                         Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "\n",
        "    # create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "    summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "\n",
        "    # compute statistics for multiple metrics with weight\n",
        "    df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "    # compute statistics for multiple metrics without weight\n",
        "    df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "\n",
        "    # compute statistics for single metric \"mean\" with weight\n",
        "    df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "    # compute statistics for single metric \"mean\" without weight\n",
        "    df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
        "    # $example off$\n",
        "\n",
        "    spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QqdcvIodmaz",
        "outputId": "ec2f4f70-5011-41d0-a683-04aff6c9c6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], [0.0,0.0,0.0]}     |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SummarizerLab\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# another way of using summarizer\n",
        "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                         Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "\n",
        "summarizer = Summarizer.metrics (\"sum\", \"variance\")\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
        "\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}