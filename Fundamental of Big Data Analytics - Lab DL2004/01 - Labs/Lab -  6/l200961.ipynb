{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: findspark in c:\\users\\ammar\\anaconda3\\lib\\site-packages (2.0.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: findspark in c:\\users\\ammar\\anaconda3\\lib\\site-packages (2.0.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install', 'findspark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ammar\\\\anaconda3\\\\Lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc=SparkContext.getOrCreate()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyList: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Squared List: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "#Task1\n",
    "my_list = list(range(1, 11))\n",
    "\n",
    "print(\"MyList:\", my_list)\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(my_list)\n",
    "\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "\n",
    "squared_list = squared_rdd.collect()\n",
    "print(\"Squared List:\", squared_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyList 2: [53, 35, 33, 30, 55, 11, 90, 3, 40, 84, 78, 35, 84, 43, 11, 16, 69, 47, 17, 38]\n",
      "Numbers Divisible by 5: [35, 30, 55, 90, 40, 35]\n"
     ]
    }
   ],
   "source": [
    "#Task2\n",
    "import random\n",
    "my_list_2 = [random.randint(1, 100) for _ in range(20)]\n",
    "print(\"MyList 2:\", my_list_2)\n",
    "\n",
    "rdd_2 = spark.sparkContext.parallelize(my_list_2)\n",
    "\n",
    "divisible_by_5_rdd = rdd_2.filter(lambda x: x % 5 == 0)\n",
    "\n",
    "\n",
    "divisible_by_5_list = divisible_by_5_rdd.collect()\n",
    "print(\"Numbers Divisible by 5:\", divisible_by_5_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkContext.getOrCreate()\n",
    "spark = SparkSession(spark)\n",
    "\n",
    "# Load the user comments file\n",
    "comments = spark.read.csv(\"user_comment.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV file is made from provided manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------+\n",
      "|UserName|Comments                                 |\n",
      "+--------+-----------------------------------------+\n",
      "|Ali45   |[Good !!!, I will definitely visit again]|\n",
      "|Aliya153|[Your website is superb]                 |\n",
      "|Sara2   |[You need to work on your website design]|\n",
      "+--------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find comments given by each user\n",
    "from pyspark.sql.functions import collect_list\n",
    "user_comments = comments.groupBy(\"UserName\").agg(collect_list(\"Comment\").alias(\"Comments\"))\n",
    "user_comments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------+---------------+\n",
      "|UserName|Comments                                 |NumLongComments|\n",
      "+--------+-----------------------------------------+---------------+\n",
      "|Ali45   |[Good !!!, I will definitely visit again]|1              |\n",
      "|Aliya153|[Your website is superb]                 |1              |\n",
      "|Sara2   |[You need to work on your website design]|1              |\n",
      "+--------+-----------------------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Define a UDF to count the number of long comments\n",
    "def count_long_comments(comments):\n",
    "    return len([char for char in comments if len(char) > 20])\n",
    "\n",
    "# make the UDF available in Spark\n",
    "count_long_comments_udf = udf(count_long_comments, IntegerType())\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "long_comments = user_comments.withColumn(\"NumLongComments\", count_long_comments_udf(user_comments[\"Comments\"]))\n",
    "\n",
    "# Collect the data and print each row\n",
    "long_comments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|FirstLetter|count|\n",
      "+-----------+-----+\n",
      "|A          |3    |\n",
      "|S          |1    |\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of UserNames starting with each English alphabet\n",
    "user_names_by_first_letter = comments.groupBy(substring(col(\"UserName\"), 1, 1).alias(\"FirstLetter\")).count()\n",
    "user_names_by_first_letter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with most comments: Ali45\n"
     ]
    }
   ],
   "source": [
    "# Find the user who has given the maximum number of comments\n",
    "most_comments_user = user_comments.orderBy(desc(size(\"Comments\"))).first()\n",
    "username=most_comments_user.UserName\n",
    "print (\"User with most comments:\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------+------------------------+\n",
      "|UserName|Comment                                |CleanedComment          |\n",
      "+--------+---------------------------------------+------------------------+\n",
      "|Aliya153|Your website is superb                 |website superb          |\n",
      "|Sara2   |You need to work on your website design|need work website design|\n",
      "|Ali45   |Good !!!                               |Good !!!                |\n",
      "|Ali45   |I will definitely visit again          |definitely visit        |\n",
      "+--------+---------------------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task4\n",
    "# Remove stop words from the comments of the users\n",
    "\n",
    "# get stopwords list from nltk and store in 'stopwords.txt'\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "with open('stop_words.txt', 'w') as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % word for word in stopwords_list)\n",
    "    \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveStopWords\").getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame (replace with your actual CSV file path)\n",
    "csv_file = \"user_comment.csv\"\n",
    "user_comments = spark.read.csv(csv_file, header=True)\n",
    "\n",
    "# Read the text file containing stop words\n",
    "stop_words_file = \"stop_words.txt\"\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "# Broadcast the stop words\n",
    "broadcast_stop_words = spark.sparkContext.broadcast(stop_words)\n",
    "\n",
    "# Define a function to remove stop words\n",
    "def remove_stop_words(comment):\n",
    "    stop_words = broadcast_stop_words.value\n",
    "    return \" \".join([word for word in comment.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Register the UDF\n",
    "remove_stop_words_udf = udf(remove_stop_words, StringType())\n",
    "\n",
    "# Apply the UDF to the 'Comment' column\n",
    "user_comments_without_stop_words = user_comments.withColumn(\"CleanedComment\", remove_stop_words_udf(col(\"Comment\")))\n",
    "\n",
    "# Show the result\n",
    "user_comments_without_stop_words.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
