{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "51d0YwCvcofW",
        "outputId": "27598bbe-714d-42da-8153-58e89a397d69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\ammar\\\\anaconda3\\\\Lib\\\\site-packages\\\\pyspark'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqnzoeemcofZ",
        "outputId": "1b5ed781-9bea-4257-fad8-05bcb9489391"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc=SparkContext.getOrCreate()\n",
        "spark=SparkSession(sc)\n",
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIJvP9_mcofZ",
        "outputId": "15eec188-9a87-4f16-d11b-3a42de22628a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "55\n"
          ]
        }
      ],
      "source": [
        "# write a pyspark code to calculate the sum of all xalues in an RDD) using an accumulator. Create an RDD\n",
        "# of integers from 1 to 10. Define an accutnulator vsith an initial value of 0 use the foreach\n",
        "# action on the RDD to update the accumulator with each element. Finally print the final value\n",
        "# of the accumulator after processing all elements in the RDD.\n",
        "\n",
        "integer_rdd = sc.parallelize(range(1, 11))\n",
        "accum = sc.accumulator(0)\n",
        "print(integer_rdd.collect())\n",
        "integer_rdd.foreach(lambda x: accum.add(x))\n",
        "print(accum.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGqJy1JYcofa"
      },
      "outputs": [],
      "source": [
        "# Inner Join: Perform an inner join on emp_id between rddl and rdd2 to get an RDD containing the\n",
        "# employee ID. employee name, employee department. and employee salary for employees who exist In\n",
        "# both RDDs.\n",
        "# Left Outer Join: Perform a left outer join on emp_id between rddl and rdd2 to get an RDD containing\n",
        "# the employee ID. employee name, employee department, and employee salary for all employees jn\n",
        "# rddl and matching employees in rdd2. For employees in rddl that do not have a matching employee\n",
        "# ID in rdd2, the employee salary should be set to None.\n",
        "# Calculate the total salary of employees in RDDI by joining it with RDD2 on \"emp_id\" and\n",
        "# multiplying \"emp_salary\" with \"emp_experience\".\n",
        "# Calculate the maximum salary of employees\n",
        "# Perform an inner join on RDDI and RDD2 on \"emp_id\" and \"dept_id\". and calculate the total salary\n",
        "# of employees in each department.\n",
        "# Perform a left outer join on RDDI and RDD2 on \"emp_id\" and \"dept_id\", and find the employees\n",
        "# who do not belong to anv department.\n",
        "# CODE IS CONCEPTUAL\n",
        "\n",
        "rddl = sc.parallelize([(1, 'John', 25), (2, 'Smith', 22), (3, 'Williams', 32), (4, 'Chris', 23)])\n",
        "rdd2 = sc.parallelize([(1, 'HR', 5), (2, 'Finance', 3), (3, 'IT', 4), (5, 'Admin', 2)])\n",
        "\n",
        "\n",
        "# Inner Join\n",
        "inner_join_rdd = rddl.join(rdd2, on='emp_id', how='inner')\n",
        "inner_join_rdd.select('emp_id', 'emp_name', 'emp_department', 'emp_salary').show()\n",
        "\n",
        "# Left Outer Join\n",
        "left_outer_join_rdd = rddl.join(rdd2, on='emp_id', how='left_outer')\n",
        "left_outer_join_rdd.select('emp_id', 'emp_name', 'emp_department', 'emp_salary').show()\n",
        "\n",
        "# Calculate total salary\n",
        "total_salary_rdd = rddl.join(rdd2, on='emp_id', how='inner')\n",
        "total_salary_rdd = total_salary_rdd.withColumn('total_salary', total_salary_rdd['emp_salary'] * total_salary_rdd['emp_experience'])\n",
        "total_salary = total_salary_rdd.selectExpr('sum(total_salary) as total_salary').collect()[0]['total_salary']\n",
        "print(total_salary)\n",
        "\n",
        "# Calculate maximum salary\n",
        "max_salary = rddl.selectExpr('max(emp_salary) as max_salary').collect()[0]['max_salary']\n",
        "print(max_salary)\n",
        "\n",
        "# Calculate total salary by department\n",
        "total_salary_by_dept = rddl.join(rdd2, (rddl.emp_id == rdd2.emp_id) & (rddl.dept_id == rdd2.dept_id), how='inner')\n",
        "total_salary_by_dept = total_salary_by_dept.groupBy('emp_department').agg({'emp_salary': 'sum'})\n",
        "total_salary_by_dept.show()\n",
        "\n",
        "# Employees not belonging to any department\n",
        "no_dept_employees = rddl.join(rdd2, (rddl.emp_id == rdd2.emp_id) & (rddl.dept_id == rdd2.dept_id), how='left_anti')\n",
        "no_dept_employees.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qUbOdIycofb",
        "outputId": "f9845862-dd05-4496-e626-8ed6c888da66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                data|\n",
            "+--------------------+\n",
            "| Project Gutenberg's|\n",
            "|Alice's Adventure...|\n",
            "| Project Gutenberg's|\n",
            "|Adventures in Won...|\n",
            "| Project Gutenberg's|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "data = [\"Project Gutenberg's\", \"Alice's Adventures in Wonderland\", \"Project Gutenberg's\", \"Adventures in Wonderland\", \"Project Gutenberg's\"]\n",
        "\n",
        "df = spark.createDataFrame(data, StringType()).toDF('data')\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiAAofxKcofb",
        "outputId": "f52bdae6-0caa-4333-91d4-14e025ee8e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Project', \"Gutenberg's\"],\n",
              " [\"Alice's\", 'Adventures', 'in', 'Wonderland'],\n",
              " ['Project', \"Gutenberg's\"],\n",
              " ['Adventures', 'in', 'Wonderland'],\n",
              " ['Project', \"Gutenberg's\"]]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd = sc.parallelize(data)\n",
        "rdd = rdd.map(lambda x: x.split(\" \"))\n",
        "rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Viofe2s1cofb",
        "outputId": "5e5c3ddb-119b-4629-fc66-50d12cba2360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Project',\n",
              " \"Gutenberg's\",\n",
              " \"Alice's\",\n",
              " 'Adventures',\n",
              " 'in',\n",
              " 'Wonderland',\n",
              " 'Project',\n",
              " \"Gutenberg's\",\n",
              " 'Adventures',\n",
              " 'in',\n",
              " 'Wonderland',\n",
              " 'Project',\n",
              " \"Gutenberg's\"]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply flatMap transformation and split the strings\n",
        "rdd_flatmap = sc.parallelize(data)\n",
        "rdd_flatmap = rdd_flatmap.flatMap(lambda x: x.split(\" \"))\n",
        "rdd_flatmap.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tBpNj-Rcofb",
        "outputId": "97d90d4a-be57-40cd-df14-8fad55293272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Project', 1),\n",
              " (\"Gutenberg's\", 1),\n",
              " (\"Alice's\", 1),\n",
              " ('Adventures', 1),\n",
              " ('in', 1),\n",
              " ('Wonderland', 1),\n",
              " ('Project', 1),\n",
              " (\"Gutenberg's\", 1),\n",
              " ('Adventures', 1),\n",
              " ('in', 1),\n",
              " ('Wonderland', 1),\n",
              " ('Project', 1),\n",
              " (\"Gutenberg's\", 1)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_words(line):\n",
        "    return line.split(\" \")\n",
        "def make_pairs(word):\n",
        "    return (word, 1)\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "pair_rdds = rdd.flatMap(split_words).map(make_pairs)\n",
        "pair_rdds.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g24nFb4ncofc",
        "outputId": "2997c54c-f307-4a3e-b96e-886b3902eae0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For example, Suppose RDD contains first five natural numbers (1, 2, 3, 4, and 5) and the\n",
        "# predicate is check for an even number. The resulting RDD after the filter will contain only\n",
        "# the even numbers i.e., 2 and 4.\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.filter(lambda x: x % 2 == 0).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJWW-86Lcofc",
        "outputId": "35fd1fd0-e94e-488f-80fa-91c1c032781b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing people.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile people.csv\n",
        "name;age;job\n",
        "Jorge;30;Developer\n",
        "Bob;32;Developer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2PwzWsMcofd",
        "outputId": "09cbe02c-b239-4270-ce2b-0f3d19eef45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf = spark.read.csv('people.csv', sep=';', header=True,inferSchema=True)\n",
        "peopledf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzfznC-zcofd",
        "outputId": "661e135b-4d8d-46ff-f145-f9931aa539d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P1Vyjhfcofe",
        "outputId": "fd7728f5-83e8-4845-9da7-538c99ea5983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|Jorge|\n",
            "|  Bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf.select('name').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV9-NLCFcofe",
        "outputId": "503ed604-ed31-4258-9593-e269d4e686fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------+\n",
            "| name|(age + 1)|\n",
            "+-----+---------+\n",
            "|Jorge|       31|\n",
            "|  Bob|       33|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf.select('name', peopledf.age+1).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IZVePUvcofe",
        "outputId": "37a696f4-4526-4a7f-d508-8c29784ee924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+---------+\n",
            "|name|age|      job|\n",
            "+----+---+---------+\n",
            "| Bob| 32|Developer|\n",
            "+----+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# filter on multiple conditions\n",
        "peopledf.filter((peopledf.age > 30) & (peopledf.job == 'Developer')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIR8CHXcoff",
        "outputId": "b1d3ad82-43f4-491f-ef86-e8be5c76348e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 32|    1|\n",
            "| 30|    1|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf.groupBy('age').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYHJtWoscoff"
      },
      "source": [
        "sparksql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA08kf-lcofg",
        "outputId": "6ba2c013-5025-4499-d93e-8ce1d80e345e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "peopledf.createOrReplaceTempView('people')\n",
        "spark.sql('SELECT * FROM people').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6XALVQacofg",
        "outputId": "5274f599-fbed-4850-8de4-7969c0ec02e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('great', 'great first try super boy'),\n",
              " ('first', 'first try good'),\n",
              " ('super', 'super boy won first try')]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = ['great first try super boy', 'first try good', 'super boy won first try']\n",
        "rdd = sc.parallelize(data)\n",
        "pairs = rdd.map(lambda x: (x.split(\" \")[0], x))\n",
        "# • Creating a pair RDD using the first word as the key in Python\n",
        "pairs.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBwm1A5Zcofh",
        "outputId": "21e84a06-fd01-40ec-a8a8-e870f42d0791"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('first', 'first try good')]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = pairs.filter(lambda Value: len(Value[1]) < 15)\n",
        "# • Simple filter on second element in Pytho\n",
        "result.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-p-mxc-cofh"
      },
      "source": [
        "pair rdds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWCNCWQecofi"
      },
      "source": [
        "Reduce-like operations are transformations that combine values associated with the same key across partitions. They are commonly used for aggregation tasks, such as counting, summing, or finding the minimum or maximum value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdWj4CUocofi"
      },
      "source": [
        "reduceByKey\n",
        "\n",
        "Takes an RDD of key-value pairs and returns a new RDD with the same keys, but with the values reduced to a single value using a specified reduce function.\n",
        "The reduce function takes two values of the same type and returns a single value of the same type.\n",
        "Example: To sum the values associated with each key in an RDD:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR23ZMs9cofi",
        "outputId": "0f12150d-7231-4331-f17d-36f830ecf10c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 4), ('b', 4)]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd = sc.parallelize([[\"a\", 3], [\"b\", 4], [\"a\", 1]])\n",
        "rk = rdd.reduceByKey(lambda x,y: x+y)\n",
        "rk.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdCXC0Scofi"
      },
      "source": [
        "groupByKey\n",
        "\n",
        "Takes an RDD of key-value pairs and returns a new RDD with the same keys, but with the values grouped into a single list for each key.\n",
        "Example: To group the values associated with each key in an RDD:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00OoMUFZcofi"
      },
      "source": [
        "```• rdd = sc.parallelize([[\"a\", 3], [\"b\", 4], [\"a\", 1]])\n",
        "• rk = rdd.reduceByKey(lambda x,y: x+y)\n",
        "• (‘a’,4)\n",
        "• (‘b’,4)\n",
        "• rdd.groupByKey()\n",
        "• (‘a’, (3, 1))\n",
        "• (‘b’, (4))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHLROBsFcofi"
      },
      "source": [
        "sort\n",
        "```\n",
        "• rdd = sc.parallelize([[\"a\", 3],\n",
        "[\"b\", 4], [\"a\", 1]])\n",
        "• rdd1.sortByKey()\n",
        "[‘a’ , 3 ]\n",
        "[‘a’ , 1 ]\n",
        "[‘b’ , 4 ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v_VPd11cofj",
        "outputId": "bccde9d7-e470-40b0-f167-533e710c2397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('to', 1),\n",
              " ('easy', 1),\n",
              " ('is', 2),\n",
              " ('Hadoop', 1),\n",
              " ('faster', 1),\n",
              " ('than', 1),\n",
              " ('use', 1),\n",
              " ('Spark', 2)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Data = sc.parallelize([\"Spark is faster than Hadoop\", \"Spark is easy to use\"]) #rdds\n",
        "words = Data.flatMap(lambda line: line.split(\" \"))#rdds #transformations\n",
        "wordmap = words.map(lambda word:(word,1))#rdds #transformations\n",
        "wordcount = wordmap.reduceByKey(lambda x,y: x+y)#rdds #transformations\n",
        "wordcount.collect()#action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "356URHK8cofj",
        "outputId": "e6f0eece-70e6-4fb6-a51f-a6b48193bbb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 5), ('b', 6), ('a', 3)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd = sc.parallelize([[\"a\", 3], [\"b\", 4], [\"a\", 1]])\n",
        "# mapValues\n",
        "rdd=rdd.mapValues(lambda x: x+2)\n",
        "# • Apply a function to each value of a pair RDD without\n",
        "# changing the key\n",
        "rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acx7roCVcofj",
        "outputId": "9f5f2636-838d-49f9-90ac-6d07cf2b34d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 0),\n",
              " ('a', 1),\n",
              " ('a', 2),\n",
              " ('b', 0),\n",
              " ('b', 1),\n",
              " ('b', 2),\n",
              " ('b', 3),\n",
              " ('a', 0)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd=sc.parallelize([(\"a\", 3), (\"b\", 4), (\"a\", 1)])\n",
        "rdd=rdd.flatMapValues(lambda x: range(0,x) )\n",
        "rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBxNlc3qcofj",
        "outputId": "908995e2-ebcb-40eb-a7ee-318a7e4c1134"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('S2', 7.666666666666667), ('S1', 7.666666666666667)]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You are given a file containing marks obtained by each student in different\n",
        "# assessments in the form (marks, StudentNo)\n",
        "data = sc.parallelize([(8,'S1'),(9,'S2'),(7,'S1'),(9,'S2'),(8,'S1'),(5,'S2')])\n",
        "# 1. Find the average marks obtained by each student\n",
        "rdd = data.map(lambda x: (x[1], x[0]))\n",
        "rdd2=rdd.mapValues(lambda x: (x,1))\n",
        "rdd3=rdd2.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
        "avg=rdd3.mapValues(lambda x: x[0]/x[1])\n",
        "avg.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StvNS0Jucofk"
      },
      "source": [
        "working<br>\n",
        "('S1', (8, 1))<br> ('S2', (2, 1))<br> ('S3', (7, 1))<br> ('S1', (4, 1))<br> ('S2', (5, 1))<br>\n",
        "('S1', (2, 1))<br>\n",
        "('S4', (5, 1))<br> reducebykey<br>\n",
        "('S1', (14, 3)) <br>('S2', (7, 2))<br> ('S3', (7, 1))<br> ('S4', (5, 1)) <br>avgbykey<br> ('S1', 4.666666666666<br>\n",
        "('S2', 3.5)<br>\n",
        "('S3', 7.0)<br>\n",
        "('54' 5.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEQHt2m8cofk",
        "outputId": "325da778-9244-460a-ad62-7cbfeb52097a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.714285714285714\n"
          ]
        }
      ],
      "source": [
        "# find average marks of assessments\n",
        "data = sc.parallelize([(8,'S1'),(2,'S2'),(7,'S3'),(4,'S1'),(5,'S2'),(2,'S1'),(5,'S4')])\n",
        "rddtotal=data.map(lambda x: (x[0]))\n",
        "t=rddtotal.reduce(lambda x,y: x+y)\n",
        "total=data.map(lambda x: (x[0])).sum()\n",
        "print(total/data.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INQ4GdE_cofk",
        "outputId": "40c1f91a-6316-4e95-b303-e189340142e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', (3, 5)), ('a', (1, 5))]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Join-like Operations Examples\n",
        "rdd1 = sc.parallelize([[\"a\", 3], [\"b\", 4], [\"a\", 1]])\n",
        "rdd2= sc.parallelize([[\"a\",5],[\"c\",10]])\n",
        "rdd1.join(rdd2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiFrjaIkcofk",
        "outputId": "648de95e-9c57-4163-9d34-c64a340c5ede"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('b', (4, None)), ('a', (3, 5)), ('a', (1, 5))]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1.leftOuterJoin(rdd2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAJkX_AEcofl",
        "outputId": "7795f655-8b1c-47e1-8e7c-ff416f9f2c99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('c', (None, 10)), ('a', (3, 5)), ('a', (1, 5))]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1.rightOuterJoin(rdd2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BreJb2mxcofl",
        "outputId": "33f693bf-635d-494c-f234-32a84d38725d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', ([3, 1], [5])), ('b', ([4], [])), ('c', ([], [10]))]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n",
        "# • Group data from both RDDs sharing the same key.\n",
        "# • It’s a full outer join."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbFRocjbcu6v"
      },
      "source": [
        "Finals prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fVqMu5lAcofl"
      },
      "outputs": [],
      "source": [
        "# sales_transactions table\n",
        "sales_transactions = [\n",
        "    {\"transaction_id\": 1, \"customer_id\": 1, \"product_id\": 1, \"quantity\": 2, \"transaction_date\": \"2024-01-01\"},\n",
        "    {\"transaction_id\": 2, \"customer_id\": 1, \"product_id\": 2, \"quantity\": 1, \"transaction_date\": \"2024-01-02\"},\n",
        "    {\"transaction_id\": 3, \"customer_id\": 2, \"product_id\": 3, \"quantity\": 3, \"transaction_date\": \"2024-01-01\"},\n",
        "    {\"transaction_id\": 4, \"customer_id\": 2, \"product_id\": 4, \"quantity\": 2, \"transaction_date\": \"2024-01-03\"},\n",
        "]\n",
        "\n",
        "customers = [\n",
        "    {\"customer_id\": 1, \"customer_name\": \"John\", \"customer_email\": \"johndoe@company\", \"customer_age\": 30, \"customer_city\": \"New York\"},\n",
        "    {\"customer_id\": 2, \"customer_name\": \"Jane\", \"customer_email\": \"janedoe@company\", \"customer_age\": 25, \"customer_city\": \"Los Angeles\"},\n",
        "    {\"customer_id\": 3, \"customer_name\": \"Bob\", \"customer_email\": \"bob@company\", \"customer_age\": 40, \"customer_city\": \"Chicago\"},\n",
        "    {\"customer_id\": 4, \"customer_name\": \"Alice\", \"customer_email\": \"alice@company\", \"customer_age\": 35, \"customer_city\": \"Houston\"},\n",
        "]\n",
        "\n",
        "products = [\n",
        "    {\"product_id\": 1, \"product_name\": \"Product 1\", \"product_category\": \"Category 1\", \"product_price\": 10.99},\n",
        "    {\"product_id\": 2, \"product_name\": \"Product 2\", \"product_category\": \"Category 2\", \"product_price\": 19.99},\n",
        "    {\"product_id\": 3, \"product_name\": \"Product 3\", \"product_category\": \"Category 1\", \"product_price\": 8.99},\n",
        "    {\"product_id\": 4, \"product_name\": \"Product 4\", \"product_category\": \"Category 3\", \"product_price\": 29.99},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lERdHCGacofm",
        "outputId": "04387ec5-95c9-4eaf-a065-6b7e5e957f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----------------+\n",
            "|customer_name|      total_spent|\n",
            "+-------------+-----------------+\n",
            "|         John|41.96999931335449|\n",
            "|         Jane|86.94999885559082|\n",
            "+-------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Suppose you're tasked with analyzing the sales performance of a retail company You have been provided\n",
        "# a dataset containing information about sales transactions, customer details and product information. The dataset\n",
        "# consists of multiple tables\n",
        "# 1.sales _ transactions table:\n",
        "# Columns: transaction_id, customer_id, product_id, quantity, transaction_date.\n",
        "# 2. customers table:\n",
        "# Columns: customer_id, customer name, customer_email, customer_age, customer_city.\n",
        "# 3. products table:\n",
        "# Columns: product_id, product _ name, product _ category, product _ price.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SQL on Dictionaries\").getOrCreate()\n",
        "\n",
        "# Define schema for each table\n",
        "sales_schema = StructType([\n",
        "    StructField(\"transaction_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"transaction_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"customer_email\", StringType(), True),\n",
        "    StructField(\"customer_age\", IntegerType(), True),\n",
        "    StructField(\"customer_city\", StringType(), True)\n",
        "])\n",
        "\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"product_category\", StringType(), True),\n",
        "    StructField(\"product_price\", FloatType(), True)\n",
        "])\n",
        "\n",
        "# Creating Spark DataFrames with explicit schema\n",
        "sales_sdf = spark.createDataFrame(sales_transactions, schema=sales_schema)\n",
        "customers_sdf = spark.createDataFrame(customers, schema=customer_schema)\n",
        "products_sdf = spark.createDataFrame(products, schema=product_schema)\n",
        "\n",
        "# Register DataFrames as temporary views to run SQL queries\n",
        "sales_sdf.createOrReplaceTempView(\"sales_transactions\")\n",
        "customers_sdf.createOrReplaceTempView(\"customers\")\n",
        "products_sdf.createOrReplaceTempView(\"products\")\n",
        "\n",
        "# Example SQL-like operation using Spark SQL\n",
        "query = \"\"\"\n",
        "SELECT c.customer_name, SUM(s.quantity * p.product_price) AS total_spent\n",
        "FROM sales_transactions s\n",
        "JOIN customers c ON s.customer_id = c.customer_id\n",
        "JOIN products p ON s.product_id = p.product_id\n",
        "GROUP BY c.customer_name\n",
        "\"\"\"\n",
        "\n",
        "result_sdf = spark.sql(query)\n",
        "result_sdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWNiQBX7pgrX",
        "outputId": "bcfe5f47-f99f-4e4e-e2e2-5c1d43160726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+------------------+\n",
            "|product_category|     total_revenue|\n",
            "+----------------+------------------+\n",
            "|      Category 1|19.979999542236328|\n",
            "|      Category 3|29.989999771118164|\n",
            "|      Category 2|19.989999771118164|\n",
            "+----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write a SQL query to find the total sales revenue generated by each product category for the current year.\n",
        "# query should return the product category along with the total revenue. Assume the current year is 2024\n",
        "# also assume the tables are like the tempviews you have made\n",
        "\n",
        "query =\"\"\"\n",
        "SELECT p.product_category, SUM(p.product_price) as total_revenue\n",
        "FROM products p\n",
        "JOIN sales_transactions s on p.product_id=s.product_id\n",
        "WHERE YEAR(transaction_date) = 2024\n",
        "GROUP BY p.product_category\n",
        "\"\"\"\n",
        "\n",
        "result_sdf = spark.sql(query)\n",
        "result_sdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFwrdqYlraYL",
        "outputId": "18536e27-5937-43cb-f231-2fcce1dad3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 customers by total purchase amount:\n",
            "  customer_name  total_spent\n",
            "0          John       2700.0\n",
            "1          Jane        550.0\n",
            "Average age of customers who made purchases in the Electronics category: 27.50\n"
          ]
        }
      ],
      "source": [
        "# Now, load the provided dataset into Python using Pandas DataFrames.\n",
        "# Write Python code to:\n",
        "# i. Find the top 5 customers who made the highest total purchase amount.\n",
        "# ii. Calculate the average age of customers who made purchases in the Electronics category\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the data as provided earlier\n",
        "sales_transactions = [\n",
        "    {\"transaction_id\": 1, \"customer_id\": 1, \"product_id\": 1, \"quantity\": 2, \"transaction_date\": \"2022-01-01\"},\n",
        "    {\"transaction_id\": 2, \"customer_id\": 1, \"product_id\": 2, \"quantity\": 1, \"transaction_date\": \"2022-01-02\"},\n",
        "    {\"transaction_id\": 3, \"customer_id\": 2, \"product_id\": 3, \"quantity\": 3, \"transaction_date\": \"2022-01-01\"},\n",
        "    {\"transaction_id\": 4, \"customer_id\": 2, \"product_id\": 4, \"quantity\": 2, \"transaction_date\": \"2022-01-03\"},\n",
        "]\n",
        "\n",
        "customers = [\n",
        "    {\"customer_id\": 1, \"customer_name\": \"John\", \"customer_email\": \"johndoe@company.com\", \"customer_age\": 30, \"customer_city\": \"New York\"},\n",
        "    {\"customer_id\": 2, \"customer_name\": \"Jane\", \"customer_email\": \"janedoe@company.com\", \"customer_age\": 25, \"customer_city\": \"Los Angeles\"},\n",
        "    {\"customer_id\": 3, \"customer_name\": \"Bob\", \"customer_email\": \"bob@company.com\", \"customer_age\": 40, \"customer_city\": \"Chicago\"},\n",
        "    {\"customer_id\": 4, \"customer_name\": \"Alice\", \"customer_email\": \"alice@company.com\", \"customer_age\": 35, \"customer_city\": \"Houston\"},\n",
        "]\n",
        "\n",
        "products = [\n",
        "    {\"product_id\": 1, \"product_name\": \"Laptop\", \"product_category\": \"Electronics\", \"product_price\": 1000.0},\n",
        "    {\"product_id\": 2, \"product_name\": \"Smartphone\", \"product_category\": \"Electronics\", \"product_price\": 700.0},\n",
        "    {\"product_id\": 3, \"product_name\": \"Keyboard\", \"product_category\": \"Electronics\", \"product_price\": 150.0},\n",
        "    {\"product_id\": 4, \"product_name\": \"Mouse\", \"product_category\": \"Electronics\", \"product_price\": 50.0},\n",
        "]\n",
        "\n",
        "# Convert dictionaries to Pandas DataFrames\n",
        "sales_df = pd.DataFrame(sales_transactions)\n",
        "customers_df = pd.DataFrame(customers)\n",
        "products_df = pd.DataFrame(products)\n",
        "\n",
        "# i\n",
        "# Merge the necessary DataFrames\n",
        "merged_df = sales_df.merge(customers_df, on=\"customer_id\").merge(products_df, on=\"product_id\")\n",
        "\n",
        "# Calculate the total spent per transaction\n",
        "merged_df['total_spent'] = merged_df['quantity'] * merged_df['product_price']\n",
        "\n",
        "# Aggregate to find the top 5 customers\n",
        "top_customers = merged_df.groupby('customer_name').total_spent.sum().nlargest(5).reset_index()\n",
        "\n",
        "print(\"Top 5 customers by total purchase amount:\")\n",
        "print(top_customers)\n",
        "\n",
        "# ii\n",
        "# Filter for the Electronics category\n",
        "electronics_purchases = merged_df[merged_df['product_category'] == 'Electronics']\n",
        "\n",
        "# Calculate the average age of customers who purchased electronics\n",
        "average_age_electronics = electronics_purchases['customer_age'].mean()\n",
        "\n",
        "print(f\"Average age of customers who made purchases in the Electronics category: {average_age_electronics:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY_e2Y66wJqc",
        "outputId": "22794241-43e2-41a9-90d4-0734342229b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+--------------+\n",
            "|product_category|total_quantity|\n",
            "+----------------+--------------+\n",
            "|     Electronics|             3|\n",
            "+----------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assume you need to find the total sales quantity for each product category but only for the customers residing\n",
        "# in 'New York'.\n",
        "# i. Write an SQL query to filter out the transactions made b customers from 'New York' and calculate the total\n",
        "# sales quantity for each product category\n",
        "# ii. Now, using the filtered dataset obtained from the SQL query. write Python code using DataFrames to\n",
        "# calculate the average product price for each product category\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Sales Analysis\").getOrCreate()\n",
        "\n",
        "# Create DataFrames for sales, customers, and products\n",
        "sales_df = spark.createDataFrame(sales_transactions)\n",
        "customers_df = spark.createDataFrame(customers)\n",
        "products_df = spark.createDataFrame(products)\n",
        "\n",
        "# Register DataFrames as temporary views\n",
        "sales_df.createOrReplaceTempView(\"sales_transactions\")\n",
        "customers_df.createOrReplaceTempView(\"customers\")\n",
        "products_df.createOrReplaceTempView(\"products\")\n",
        "\n",
        "# i\n",
        "# Perform the SQL query using PySpark SQL\n",
        "query = \"\"\"\n",
        "SELECT p.product_category, SUM(s.quantity) AS total_quantity\n",
        "FROM sales_transactions s\n",
        "JOIN customers c ON s.customer_id = c.customer_id\n",
        "JOIN products p ON s.product_id = p.product_id\n",
        "WHERE c.customer_city = 'New York'\n",
        "GROUP BY p.product_category\n",
        "\"\"\"\n",
        "filtered_sales = spark.sql(query)\n",
        "\n",
        "# Show the results of the SQL query\n",
        "filtered_sales.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q1f5-zGyC8I",
        "outputId": "63354f36-d9c5-44ea-bc1b-3789aecc984f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-------------+\n",
            "|product_category|average_price|\n",
            "+----------------+-------------+\n",
            "|     Electronics|        475.0|\n",
            "+----------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ii\n",
        "# Join the filtered sales with the products to get pricing information\n",
        "category_price_df = filtered_sales.join(products_df, filtered_sales.product_category == products_df.product_category)\n",
        "\n",
        "# Calculate the average product price per category\n",
        "average_price = category_price_df.groupBy(\"p.product_category\").agg(avg(\"product_price\").alias(\"average_price\"))\n",
        "\n",
        "# Show the average prices\n",
        "average_price.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We have a large dataset containing information about students' grades in different\n",
        "# subjects at a university. The dataset is structured as follows: each record consists of the student's ID,\n",
        "# subject code, and grade obtained in that subject. Consider an input file in the following format:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing grades.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile grades.txt\n",
        "S001 Math A\n",
        "S002 English B\n",
        "S003 Science C\n",
        "S001 Science B\n",
        "S002 Math A\n",
        "S003 English B\n",
        "S004 Math C\n",
        "S004 English A\n",
        "S005 Science A\n",
        "S005 Math B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------+-----+\n",
            "|StudentID|Subject|Grade|\n",
            "+---------+-------+-----+\n",
            "|     S001|   Math|    A|\n",
            "|     S002|   Math|    A|\n",
            "|     S004|English|    A|\n",
            "|     S005|Science|    A|\n",
            "+---------+-------+-----+\n",
            "\n",
            "+---------+--------+\n",
            "|StudentID|AvgGrade|\n",
            "+---------+--------+\n",
            "|     S004|     3.0|\n",
            "|     S001|     3.5|\n",
            "|     S002|     3.5|\n",
            "|     S005|     3.5|\n",
            "|     S003|     2.5|\n",
            "+---------+--------+\n",
            "\n",
            "Subject with most A's: Math, Count: 2\n",
            "+---------+-------+-----+\n",
            "|StudentID|Subject|Grade|\n",
            "+---------+-------+-----+\n",
            "|     S001|   Math|    A|\n",
            "|     S001|Science|    B|\n",
            "|     S002|English|    B|\n",
            "|     S002|   Math|    A|\n",
            "|     S003|Science|    C|\n",
            "|     S003|English|    B|\n",
            "|     S004|English|    A|\n",
            "|     S004|   Math|    C|\n",
            "|     S005|Science|    A|\n",
            "|     S005|   Math|    B|\n",
            "+---------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Grades Analysis\").getOrCreate()\n",
        "\n",
        "# Read the input file\n",
        "df = spark.read.text(\"grades.txt\")\n",
        "\n",
        "# Split the line into separate columns\n",
        "df = df.selectExpr(\"split(value, ' ')[0] as StudentID\",\n",
        "                   \"split(value, ' ')[1] as Subject\",\n",
        "                   \"split(value, ' ')[2] as Grade\")\n",
        "\n",
        "# Task i: Select and display the records of students who have obtained grade 'A' in at least one subject.\n",
        "students_with_a = df.filter(df.Grade == 'A')\n",
        "students_with_a.show()\n",
        "from pyspark.sql.functions import col, when, avg\n",
        "\n",
        "# Task ii: Calculate the average grade obtained by each student across all subjects. \n",
        "# Display the student ID along with their average grade.\n",
        "# We need to convert grades to numerical values, assuming A=4, B=3, C=2 for simplicity\n",
        "grade_to_numeric = when(col(\"Grade\") == 'A', 4).when(col(\"Grade\") == 'B', 3).when(col(\"Grade\") == 'C', 2)\n",
        "df_with_numeric_grades = df.withColumn(\"NumericGrade\", grade_to_numeric)\n",
        "\n",
        "avg_grades = df_with_numeric_grades.groupBy(\"StudentID\").agg(avg(\"NumericGrade\").alias(\"AvgGrade\"))\n",
        "avg_grades.show()\n",
        "\n",
        "# Task iii: Find the subject in which the highest number of students have obtained grade 'A'. \n",
        "# Display the subject code along with the count of students who received grade 'A' in that subject.\n",
        "top_subject_for_a = students_with_a.groupBy(\"Subject\").count().orderBy(col(\"count\").desc()).first()\n",
        "print(f\"Subject with most A's: {top_subject_for_a['Subject']}, Count: {top_subject_for_a['count']}\")\n",
        "\n",
        "# Task iv: Sort the records based on the student ID in ascending order.\n",
        "sorted_records = df.sort(\"StudentID\")\n",
        "sorted_records.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
